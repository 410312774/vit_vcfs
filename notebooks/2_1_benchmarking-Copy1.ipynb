{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a263fe4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/homes/gws/chanwkim/vit-shapley/notebooks\n",
      "/homes/gws/chanwkim/vit-shapley\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir('../')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f348f8",
   "metadata": {},
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "548e5e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from collections import OrderedDict\n",
    "import copy\n",
    "import pickle\n",
    "import time\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from vit_shapley.datamodules.ImageNette_datamodule import ImageNetteDataModule\n",
    "from vit_shapley.datamodules.MURA_datamodule import MURADataModule\n",
    "from vit_shapley.datamodules.Pet_datamodule import PetDataModule\n",
    "\n",
    "from vit_shapley.modules.classifier import Classifier\n",
    "from vit_shapley.modules.classifier_masked import ClassifierMasked\n",
    "from vit_shapley.modules.surrogate import Surrogate\n",
    "from vit_shapley.modules.explainer import Explainer\n",
    "\n",
    "from vit_shapley.config import ex\n",
    "from vit_shapley.config import config, env_chanwkim, dataset_ImageNette, dataset_MURA, dataset_Pet\n",
    "_config=config()\n",
    "\n",
    "dataset_split=\"test\"\n",
    "parallel_mode = (1, 4)\n",
    "backbone_to_use=[\"vit_large_patch16_224\"]\n",
    "_config.update(dataset_ImageNette())\n",
    "evaluation_stage=[\"1_classifier_evaluate\",\n",
    "                  \"2_surrogate_evaluate\",\n",
    "                  \"3_explanation_generate\",\n",
    "                  \"4_insert_delete\",\n",
    "                  \"5_sensitivity\",\n",
    "                  \"6_noretraining\",\n",
    "                  \"7_classifiermasked\",\n",
    "                  \"8_elapsedtime\",\n",
    "                  \"9_estimationerror\"][3]\n",
    "\n",
    "_config.update(env_chanwkim()); _config.update({'gpus_classifier':[2,],\n",
    "                                                'gpus_surrogate':[2,],\n",
    "                                                'gpus_explainer':[2,]})\n",
    "\n",
    "_config.update({'classifier_backbone_type': None,\n",
    "                'classifier_download_weight': False,\n",
    "                'classifier_load_path': None})\n",
    "_config.update({'classifier_masked_mask_location': \"pre-softmax\",\n",
    "                'classifier_enable_pos_embed': True,\n",
    "                })\n",
    "_config.update({'surrogate_mask_location': \"pre-softmax\"})\n",
    "_config.update({'surrogate_backbone_type': None,\n",
    "                'surrogate_download_weight': False,\n",
    "                'surrogate_load_path': None})\n",
    "_config.update({'explainer_num_mask_samples': 2,\n",
    "                'explainer_paired_mask_samples': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc42bddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37ml0.cs.washington.edu         \u001b[m  Tue Feb 28 17:21:58 2023  \u001b[1m\u001b[30m515.65.01\u001b[m\r\n",
      "\u001b[36m[0]\u001b[m \u001b[34mNVIDIA GeForce RTX 2080 Ti\u001b[m |\u001b[31m 30'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m  247\u001b[m / \u001b[33m11264\u001b[m MB |\r\n",
      "\u001b[36m[1]\u001b[m \u001b[34mNVIDIA GeForce RTX 2080 Ti\u001b[m |\u001b[1m\u001b[31m 78'C\u001b[m, \u001b[1m\u001b[32m100 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m 7830\u001b[m / \u001b[33m11264\u001b[m MB | \u001b[1m\u001b[30mchanwkim\u001b[m(\u001b[33m7583M\u001b[m)\r\n",
      "\u001b[36m[2]\u001b[m \u001b[34mNVIDIA GeForce RTX 2080 Ti\u001b[m |\u001b[31m 36'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m  247\u001b[m / \u001b[33m11264\u001b[m MB |\r\n",
      "\u001b[36m[3]\u001b[m \u001b[34mNVIDIA GeForce RTX 2080 Ti\u001b[m |\u001b[31m 39'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m  247\u001b[m / \u001b[33m11264\u001b[m MB |\r\n",
      "\u001b[36m[4]\u001b[m \u001b[34mNVIDIA GeForce RTX 2080 Ti\u001b[m |\u001b[31m 42'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m 1395\u001b[m / \u001b[33m11264\u001b[m MB |\r\n",
      "\u001b[36m[5]\u001b[m \u001b[34mNVIDIA GeForce RTX 2080 Ti\u001b[m |\u001b[31m 49'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m 3594\u001b[m / \u001b[33m11264\u001b[m MB |\r\n",
      "\u001b[36m[6]\u001b[m \u001b[34mNVIDIA GeForce RTX 2080 Ti\u001b[m |\u001b[31m 34'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m10788\u001b[m / \u001b[33m11264\u001b[m MB | \u001b[1m\u001b[30mchanwkim\u001b[m(\u001b[33m10541M\u001b[m)\r\n",
      "\u001b[36m[7]\u001b[m \u001b[34mNVIDIA GeForce RTX 2080 Ti\u001b[m |\u001b[1m\u001b[31m 50'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m 7162\u001b[m / \u001b[33m11264\u001b[m MB |\r\n"
     ]
    }
   ],
   "source": [
    "!gpustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6058196f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _config[\"datasets\"]==\"ImageNette\":\n",
    "    backbone_type_config_dict_=OrderedDict({\n",
    "        \"vit_small_patch16_224\":{\n",
    "            \"classifier_path\": \"results/wandb_transformer_interpretability_project/1yndrggu/checkpoints/epoch=14-step=2204.ckpt\",\n",
    "            \"classifier_masked_path\": \"results/wandb_transformer_interpretability_project/fdm70w72/checkpoints/epoch=19-step=2939.ckpt\",\n",
    "            \"surrogate_path\":{\n",
    "                \"pre-softmax\": \"results/wandb_transformer_interpretability_project/3lfv4nmn/checkpoints/epoch=39-step=5879.ckpt\"\n",
    "            },\n",
    "            \"explainer_path\":\"results/wandb_transformer_interpretability_project/3biv2s85/checkpoints/epoch=60-step=9027.ckpt\"\n",
    "\n",
    "        },\n",
    "        \"deit_small_patch16_224\":{\n",
    "        },\n",
    "        \"vit_base_patch16_224\":{\n",
    "            \"classifier_path\": \"results/wandb_transformer_interpretability_project/2rq1issn/checkpoints/epoch=16-step=2498.ckpt\",\n",
    "            \"classifier_masked_path\": \"results/wandb_transformer_interpretability_project/x59c992d/checkpoints/epoch=21-step=3233.ckpt\",\n",
    "            \"surrogate_path\":{\n",
    "                #\"original\": \"results/wandb_transformer_interpretability_project/2rq1issn/checkpoints/epoch=16-step=2498.ckpt\",\n",
    "                \"pre-softmax\": \"results/wandb_transformer_interpretability_project/3i6zzjnp/checkpoints/epoch=38-step=5732.ckpt\",\n",
    "                #\"zero-input\": \"results/wandb_transformer_interpretability_project/zyybgzcm/checkpoints/epoch=22-step=3380.ckpt\",\n",
    "                #\"zero-embedding\": \"results/wandb_transformer_interpretability_project/1gi5gmrm/checkpoints/epoch=36-step=5438.ckpt\"\n",
    "                },\n",
    "            \"explainer_path\": \"results/wandb_transformer_interpretability_project/3ty85eft/checkpoints/epoch=83-step=12431.ckpt\"\n",
    "        },\n",
    "        \"deit_base_patch16_224\":{\n",
    "\n",
    "        },\n",
    "        \"vit_large_patch16_224\":{\n",
    "            \"classifier_path\": \"results/wandb_transformer_interpretability_project/1at36lgp/checkpoints/epoch=2-step=440.ckpt\",\n",
    "            \"surrogate_path\": {\n",
    "                \"pre-softmax\":\"results/wandb_transformer_interpretability_project/284sm0on/checkpoints/epoch=37-step=5585.ckpt\"\n",
    "            },\n",
    "            \"explainer_path\":\"results/wandb_transformer_interpretability_project/34gbowsg/checkpoints/epoch=91-step=13615.ckpt\"\n",
    "        }\n",
    "    })    \n",
    "elif _config[\"datasets\"]==\"MURA\":\n",
    "    backbone_type_config_dict_=OrderedDict({\n",
    "        \"vit_small_patch16_224\":{\n",
    "\n",
    "        },\n",
    "        \"deit_small_patch16_224\":{\n",
    "        },\n",
    "        \"vit_base_patch16_224\":{\n",
    "            \"classifier_path\":\"results/wandb_transformer_interpretability_project/1u2xgwks/checkpoints/epoch=15-step=8255.ckpt\",\n",
    "            \"surrogate_path\": {\n",
    "                #\"original\": \"results/wandb_transformer_interpretability_project/1u2xgwks/checkpoints/epoch=15-step=8255.ckpt\",\n",
    "                \"pre-softmax\": \"results/wandb_transformer_interpretability_project/22ompjqu/checkpoints/epoch=47-step=24767.ckpt\",\n",
    "                #\"zero-input\": \"results/wandb_transformer_interpretability_project/2z2qs6t0/checkpoints/epoch=44-step=23219.ckpt\",\n",
    "                #\"zero-embedding\": \"results/wandb_transformer_interpretability_project/1pbmwnvb/checkpoints/epoch=45-step=23735.ckpt\"\n",
    "            },\n",
    "            \"explainer_path\":\"results/wandb_transformer_interpretability_project/1dmhcwej/checkpoints/epoch=93-step=48597.ckpt\"\n",
    "        },\n",
    "        \"deit_base_patch16_224\":{\n",
    "\n",
    "        }\n",
    "    })\n",
    "    \n",
    "elif _config[\"datasets\"]==\"Pet\":\n",
    "    backbone_type_config_dict_=OrderedDict({\n",
    "        \"vit_small_patch16_224\":{\n",
    "\n",
    "        },\n",
    "        \"deit_small_patch16_224\":{\n",
    "        },\n",
    "        \"vit_base_patch16_224\":{\n",
    "            \"classifier_path\":\"results/wandb_transformer_interpretability_project/3g01rci7/checkpoints/epoch=9-step=909.ckpt\",\n",
    "            \"surrogate_path\": {\n",
    "                \"original\": \"results/wandb_transformer_interpretability_project/3g01rci7/checkpoints/epoch=9-step=909.ckpt\",\n",
    "                \"pre-softmax\": \"results/wandb_transformer_interpretability_project/146vf465/checkpoints/epoch=40-step=3730.ckpt\",\n",
    "                #\"zero-input\": \"results/wandb_transformer_interpretability_project/2z2qs6t0/checkpoints/epoch=44-step=23219.ckpt\",\n",
    "                #\"zero-embedding\": \"results/wandb_transformer_interpretability_project/1pbmwnvb/checkpoints/epoch=45-step=23735.ckpt\"\n",
    "            },\n",
    "            \"explainer_path\":\"results/wandb_transformer_interpretability_project/2oq7lhr7/checkpoints/epoch=85-step=7911.ckpt\"\n",
    "        },\n",
    "        \"deit_base_patch16_224\":{\n",
    "\n",
    "        }\n",
    "    })    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8426be8",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54d6d39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(num_players: int, num_mask_samples: int or None = None, paired_mask_samples: bool = True,\n",
    "                  mode: str = 'uniform', random_state: np.random.RandomState or None = None) -> np.array:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        num_players: the number of players in the coalitional game\n",
    "        num_mask_samples: the number of masks to generate\n",
    "        paired_mask_samples: if True, the generated masks are pairs of x and 1-x.\n",
    "        mode: the distribution that the number of masked features follows. ('uniform' or 'shapley')\n",
    "        random_state: random generator\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor of shape\n",
    "        (num_masks, num_players) if num_masks is int\n",
    "        (num_players) if num_masks is None\n",
    "\n",
    "    \"\"\"\n",
    "    random_state = random_state or np.random\n",
    "\n",
    "    num_samples_ = num_mask_samples or 1\n",
    "\n",
    "    if paired_mask_samples:\n",
    "        assert num_samples_ % 2 == 0, \"'num_samples' must be a multiple of 2 if 'paired' is True\"\n",
    "        num_samples_ = num_samples_ // 2\n",
    "    else:\n",
    "        num_samples_ = num_samples_\n",
    "\n",
    "    if mode == 'uniform':\n",
    "        masks = (random_state.rand(num_samples_, num_players) > random_state.rand(num_samples_, 1)).astype('int')\n",
    "    elif mode == 'shapley':\n",
    "        probs = 1 / (np.arange(1, num_players) * (num_players - np.arange(1, num_players)))\n",
    "        probs = probs / probs.sum()\n",
    "        masks = (random_state.rand(num_samples_, num_players) > 1 / num_players * random_state.choice(\n",
    "            np.arange(num_players - 1), p=probs, size=[num_samples_, 1])).astype('int')\n",
    "    else:\n",
    "        raise ValueError(\"'mode' must be 'random' or 'shapley'\")\n",
    "\n",
    "    if paired_mask_samples:\n",
    "        masks = np.stack([masks, 1 - masks], axis=1).reshape(num_samples_ * 2, num_players)\n",
    "\n",
    "    if num_mask_samples is None:\n",
    "        masks = masks.squeeze(0)\n",
    "        return masks  # (num_masks)\n",
    "    else:\n",
    "        return masks  # (num_samples, num_masks)\n",
    "\n",
    "def set_datamodule(datasets,\n",
    "                   dataset_location,\n",
    "                   explanation_location_train,\n",
    "                   explanation_mask_amount_train,\n",
    "                   explanation_mask_ascending_train,\n",
    "                   \n",
    "                   explanation_location_val,\n",
    "                   explanation_mask_amount_val,\n",
    "                   explanation_mask_ascending_val,                   \n",
    "                   \n",
    "                   explanation_location_test,\n",
    "                   explanation_mask_amount_test,\n",
    "                   explanation_mask_ascending_test,                   \n",
    "                   \n",
    "                   transforms_train,\n",
    "                   transforms_val,\n",
    "                   transforms_test,\n",
    "                   num_workers,\n",
    "                   per_gpu_batch_size,\n",
    "                   test_data_split):\n",
    "    dataset_parameters = {\n",
    "        \"dataset_location\": dataset_location,\n",
    "        \"explanation_location_train\": explanation_location_train,\n",
    "        \"explanation_mask_amount_train\": explanation_mask_amount_train,\n",
    "        \"explanation_mask_ascending_train\": explanation_mask_ascending_train,\n",
    "        \n",
    "        \"explanation_location_val\": explanation_location_val,\n",
    "        \"explanation_mask_amount_val\": explanation_mask_amount_val,\n",
    "        \"explanation_mask_ascending_val\": explanation_mask_ascending_val,\n",
    "        \n",
    "        \"explanation_location_test\": explanation_location_test,\n",
    "        \"explanation_mask_amount_test\": explanation_mask_amount_test,\n",
    "        \"explanation_mask_ascending_test\": explanation_mask_ascending_test,        \n",
    "        \n",
    "        \"transforms_train\": transforms_train,\n",
    "        \"transforms_val\": transforms_val,\n",
    "        \"transforms_test\": transforms_test,\n",
    "        \"num_workers\": num_workers,\n",
    "        \"per_gpu_batch_size\": per_gpu_batch_size,\n",
    "        \"test_data_split\": test_data_split\n",
    "    }\n",
    "\n",
    "    if datasets == \"CheXpert\":\n",
    "        datamodule = CheXpertDataModule(**dataset_parameters)\n",
    "    elif datasets == \"MIMIC\":\n",
    "        datamodule = MIMICDataModule(**dataset_parameters)\n",
    "    elif datasets == \"MURA\":\n",
    "        datamodule = MURADataModule(**dataset_parameters)\n",
    "    elif datasets == \"ImageNette\":\n",
    "        datamodule = ImageNetteDataModule(**dataset_parameters)\n",
    "    else:\n",
    "        ValueError(\"Invalid 'datasets' configuration\")\n",
    "    return datamodule\n",
    "\n",
    "datamodule = set_datamodule(datasets=_config[\"datasets\"],\n",
    "                            dataset_location=_config[\"dataset_location\"],\n",
    "\n",
    "                            explanation_location_train=_config[\"explanation_location_train\"],\n",
    "                            explanation_mask_amount_train=_config[\"explanation_mask_amount_train\"],\n",
    "                            explanation_mask_ascending_train=_config[\"explanation_mask_ascending_train\"],\n",
    "\n",
    "                            explanation_location_val=_config[\"explanation_location_val\"],\n",
    "                            explanation_mask_amount_val=_config[\"explanation_mask_amount_val\"],\n",
    "                            explanation_mask_ascending_val=_config[\"explanation_mask_ascending_val\"],\n",
    "\n",
    "                            explanation_location_test=_config[\"explanation_location_test\"],\n",
    "                            explanation_mask_amount_test=_config[\"explanation_mask_amount_test\"],\n",
    "                            explanation_mask_ascending_test=_config[\"explanation_mask_ascending_test\"],                            \n",
    "\n",
    "                            transforms_train=_config[\"transforms_train\"],\n",
    "                            transforms_val=_config[\"transforms_val\"],\n",
    "                            transforms_test=_config[\"transforms_test\"],\n",
    "                            num_workers=_config[\"num_workers\"],\n",
    "                            per_gpu_batch_size=_config[\"per_gpu_batch_size\"],\n",
    "                            test_data_split=_config[\"test_data_split\"])\n",
    "\n",
    "# The batch for training classifier consists of images and labels, but the batch for training explainer consists of images and masks.\n",
    "# The masks are generated to follow the Shapley distribution.\n",
    "\"\"\"\n",
    "original_getitem = copy.deepcopy(datamodule.dataset_cls.__getitem__)\n",
    "def __getitem__(self, idx):\n",
    "    if self.split == 'train':\n",
    "        masks = generate_mask(num_players=surrogate.num_players,\n",
    "                              num_mask_samples=_config[\"explainer_num_mask_samples\"],\n",
    "                              paired_mask_samples=_config[\"explainer_paired_mask_samples\"], mode='shapley')\n",
    "    elif self.split == 'val' or self.split == 'test':\n",
    "        # get cached if available\n",
    "        if not hasattr(self, \"masks_cached\"):\n",
    "            self.masks_cached = {}\n",
    "        masks = self.masks_cached.setdefault(idx, generate_mask(num_players=surrogate.num_players,\n",
    "                                                                num_mask_samples=_config[\n",
    "                                                                    \"explainer_num_mask_samples\"],\n",
    "                                                                paired_mask_samples=_config[\n",
    "                                                                    \"explainer_paired_mask_samples\"],\n",
    "                                                                mode='shapley'))\n",
    "    else:\n",
    "        raise ValueError(\"'split' variable must be train, val or test.\")\n",
    "    return {\"images\": original_getitem(self, idx)[\"images\"],\n",
    "            \"labels\": original_getitem(self, idx)[\"labels\"],\n",
    "            \"masks\": masks}\n",
    "datamodule.dataset_cls.__getitem__ = __getitem__\n",
    "\"\"\"\n",
    "\n",
    "datamodule.set_train_dataset()\n",
    "datamodule.set_val_dataset()\n",
    "datamodule.set_test_dataset()\n",
    "\n",
    "train_dataset=datamodule.train_dataset\n",
    "val_dataset=datamodule.val_dataset\n",
    "test_dataset=datamodule.test_dataset\n",
    "\n",
    "dset=test_dataset\n",
    "\n",
    "if dataset_split==\"train\":\n",
    "    dset.data = train_dataset.data\n",
    "elif dataset_split==\"val\":\n",
    "    dset.data = val_dataset.data     \n",
    "elif dataset_split==\"test\": \n",
    "    dset.data = test_dataset.data\n",
    "else:\n",
    "    raise\n",
    "\n",
    "labels = np.array([i['label'] for i in dset.data])\n",
    "num_classes = labels.max() + 1\n",
    "\n",
    "images_idx_list = [np.where(labels == category)[0] for category in range(num_classes)]\n",
    "\n",
    "images_idx=[]\n",
    "for classidx in range(4,4+int(10/len(images_idx_list))):\n",
    "    images_idx+=[category_idx[classidx] for category_idx in images_idx_list]\n",
    "\n",
    "xy=[dset[idx] for idx in images_idx]\n",
    "x, y = zip(*[(i['images'], i['labels']) for i in xy])\n",
    "x = torch.stack(x)\n",
    "y_labels=[dset.labels[i] for i in y]\n",
    "\n",
    "\n",
    "if _config[\"datasets\"]==\"ImageNette\":\n",
    "    label_name_list=['Cassette player', \n",
    "                      'Garbage truck', \n",
    "                      'Tench', \n",
    "                      'English springer', \n",
    "                      'Church', \n",
    "                      'Parachute', \n",
    "                      'French horn', \n",
    "                      'Chain saw', \n",
    "                      'Golf ball', \n",
    "                      'Gas pump']\n",
    "    \n",
    "elif _config[\"datasets\"]==\"MURA\":\n",
    "    label_name_list=[\"Normal\", \"Abnormal\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7380660e",
   "metadata": {},
   "source": [
    "# Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbc25f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vit_large_patch16_224\n"
     ]
    }
   ],
   "source": [
    "backbone_type_config_dict = OrderedDict()\n",
    "for idx, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict_.items()):\n",
    "    if backbone_type in backbone_to_use:\n",
    "        print(backbone_type)\n",
    "        backbone_type_config_dict[backbone_type]=backbone_type_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13aafbd0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "classifier_dict = OrderedDict()\n",
    "\n",
    "for idx, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    classifier_dict[backbone_type] = Classifier(backbone_type=backbone_type,\n",
    "                                               download_weight=_config['classifier_download_weight'],\n",
    "                                               load_path=backbone_type_config[\"classifier_path\"],\n",
    "                                               target_type=_config[\"target_type\"],\n",
    "                                               output_dim=_config[\"output_dim\"],\n",
    "                                               enable_pos_embed=_config[\"classifier_enable_pos_embed\"],\n",
    "\n",
    "                                               checkpoint_metric=None,\n",
    "                                               loss_weight=None,\n",
    "                                               optim_type=None,\n",
    "                                               learning_rate=None,\n",
    "                                               weight_decay=None,\n",
    "                                               decay_power=None,\n",
    "                                               warmup_steps=None).to(_config[\"gpus_classifier\"][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3016a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_dict_ = OrderedDict()\n",
    "\n",
    "for idx, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    classifier_dict_[backbone_type] = Surrogate(mask_location=_config[\"surrogate_mask_location\"],\n",
    "                                                   backbone_type=backbone_type,\n",
    "                                                   download_weight=_config['classifier_download_weight'],\n",
    "                                                   load_path=backbone_type_config[\"classifier_path\"],\n",
    "                                                   target_type=_config[\"target_type\"],\n",
    "                                                   output_dim=_config[\"output_dim\"],\n",
    "\n",
    "                                                   target_model=None,\n",
    "                                                   checkpoint_metric=None,\n",
    "                                                   optim_type=None,\n",
    "                                                   learning_rate=None,\n",
    "                                                   weight_decay=None,\n",
    "                                                   decay_power=None,\n",
    "                                                   warmup_steps=None).to(_config[\"gpus_classifier\"][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05bf9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluation_stage==\"7_classifiermasked\":\n",
    "    classifier_masked_dict = OrderedDict()\n",
    "\n",
    "    for idx, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "        classifier_masked_dict[backbone_type] = ClassifierMasked(mask_location=_config[\"classifier_masked_mask_location\"],\n",
    "                                                               backbone_type=backbone_type,\n",
    "                                                               download_weight=_config['classifier_download_weight'],\n",
    "                                                               load_path=backbone_type_config[\"classifier_masked_path\"],\n",
    "                                                               target_type=_config[\"target_type\"],\n",
    "                                                               output_dim=_config[\"output_dim\"],\n",
    "\n",
    "                                                               checkpoint_metric=None,\n",
    "                                                               loss_weight=None,                                                             \n",
    "                                                               optim_type=None,\n",
    "                                                               learning_rate=None,\n",
    "                                                               weight_decay=None,\n",
    "                                                               decay_power=None,\n",
    "                                                               warmup_steps=None).to(_config[\"gpus_classifier\"][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1a294aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate_dict = OrderedDict()\n",
    "\n",
    "for idx, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    mask_method_dict = OrderedDict()\n",
    "    for mask_location in backbone_type_config[\"surrogate_path\"].keys():\n",
    "        mask_method_dict[mask_location] = Surrogate(mask_location=mask_location if mask_location!=\"original\" else \"pre-softmax\",\n",
    "                                          backbone_type=backbone_type,\n",
    "                                          download_weight=_config['surrogate_download_weight'],\n",
    "                                          load_path=backbone_type_config[\"surrogate_path\"][mask_location],\n",
    "                                          target_type=_config[\"target_type\"],\n",
    "                                          output_dim=_config[\"output_dim\"],\n",
    "\n",
    "                                          target_model=None,\n",
    "                                          checkpoint_metric=None,\n",
    "                                          optim_type=None,\n",
    "                                          learning_rate=None,\n",
    "                                          weight_decay=None,\n",
    "                                          decay_power=None,\n",
    "                                          warmup_steps=None).to(_config[\"gpus_surrogate\"][idx])\n",
    "    surrogate_dict[backbone_type]=mask_method_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33775c46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77a2343",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8414a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vitmedical.modules.explainer import Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0c7ce1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9276fe1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_config.update({'explainer_normalization': \"additive\",\n",
    "                'explainer_activation': \"tanh\",\n",
    "                'explainer_link': 'sigmoid' if _config[\"output_dim\"]==1 else 'softmax',\n",
    "                'explainer_head_num_attention_blocks': 1,\n",
    "                'explainer_head_include_cls': True,\n",
    "                'explainer_head_num_mlp_layers': 3,\n",
    "                'explainer_head_mlp_layer_ratio': 4,\n",
    "                'explainer_residual': [],\n",
    "                'explainer_freeze_backbone': \"all\"})\n",
    "\n",
    "explainer_dict = OrderedDict()\n",
    "for idx, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    explainer_dict[backbone_type] = Explainer(normalization=_config[\"explainer_normalization\"],\n",
    "                                              normalization_class=_config[\"explainer_normalization_class\"],\n",
    "                                              activation=_config[\"explainer_activation\"],\n",
    "                                              surrogate=surrogate_dict[backbone_type][\"pre-softmax\"],\n",
    "                                              link=_config[\"explainer_link\"],\n",
    "                                              backbone_type=backbone_type,\n",
    "                                              download_weight=False,\n",
    "                                              residual=_config['explainer_residual'],\n",
    "                                              load_path=backbone_type_config[\"explainer_path\"],\n",
    "                                              target_type=_config[\"target_type\"],\n",
    "                                              output_dim=_config[\"output_dim\"],\n",
    "\n",
    "                                              explainer_head_num_attention_blocks=_config[\"explainer_head_num_attention_blocks\"],\n",
    "                                              explainer_head_include_cls=_config[\"explainer_head_include_cls\"],\n",
    "                                              explainer_head_num_mlp_layers=_config[\"explainer_head_num_mlp_layers\"],\n",
    "                                              explainer_head_mlp_layer_ratio=_config[\"explainer_head_mlp_layer_ratio\"],\n",
    "                                              explainer_norm=_config[\"explainer_norm\"],\n",
    "\n",
    "                                              efficiency_lambda=_config[\"explainer_efficiency_lambda\"],\n",
    "                                              efficiency_class_lambda=_config[\"explainer_efficiency_class_lambda\"],\n",
    "                                              freeze_backbone=_config[\"explainer_freeze_backbone\"],\n",
    "\n",
    "                                              checkpoint_metric=_config[\"checkpoint_metric\"],\n",
    "                                              optim_type=_config[\"optim_type\"],\n",
    "                                              learning_rate=_config[\"learning_rate\"],\n",
    "                                              weight_decay=_config[\"weight_decay\"],\n",
    "                                              decay_power=_config[\"decay_power\"],\n",
    "                                              warmup_steps=_config[\"warmup_steps\"]).to(_config[\"gpus_explainer\"][idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e7f6fb",
   "metadata": {},
   "source": [
    "# explanation methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02048ea8",
   "metadata": {},
   "source": [
    "## attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12d90ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_joint_attention(attentions, add_residual=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        attentions: (num_batches, num_layers, num_players, num_players)\n",
    "        add_residual: bool\n",
    "    Returns:\n",
    "        joint_attentions: (num_batches, num_layers, num_players, num_players)\n",
    "    \"\"\"\n",
    "    assert len(attentions.shape)==4\n",
    "    if add_residual:\n",
    "        residual_att = np.eye(attentions.shape[2])[np.newaxis, np.newaxis, ...]\n",
    "        aug_attentions = attentions + residual_att\n",
    "        aug_attentions = aug_attentions / aug_attentions.sum(axis=-1)[..., np.newaxis]\n",
    "    else:\n",
    "        aug_attentions =  attentions\n",
    "    \n",
    "    joint_attentions = np.zeros(aug_attentions.shape) # (num_batches, num_layers, num_players, num_players)\n",
    "\n",
    "    for i in np.arange(joint_attentions.shape[1]):\n",
    "        if i==0:\n",
    "            joint_attentions[:,i] = aug_attentions[:,0]\n",
    "        else:\n",
    "            joint_attentions[:,i] = (aug_attentions[:,i] @ joint_attentions[:,i-1])\n",
    "    return joint_attentions\n",
    "\n",
    "\n",
    "def attentions_to_explanation(attentions, mode='rollout'):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        attentions: (num_batches, num_layers, num_heads, num_players, num_players)\n",
    "    \"\"\"\n",
    "    assert len(attentions.shape)==5 and attentions.shape[-1]==attentions.shape[-2]\n",
    "    attentions_nohead = attentions.sum(axis=2)/attentions.shape[2] # (num_batch, num_layers, num_players, num_players)\n",
    "    attentions_nohead_residual = attentions_nohead + np.eye(attentions_nohead.shape[2])[np.newaxis, np.newaxis, ...] # (num_batch, num_layers, num_players, num_players)\n",
    "    attentions_nohead_residual_normalized = attentions_nohead_residual / attentions_nohead_residual.sum(axis=-1)[..., np.newaxis] # (num_batch, num_layers, num_players, num_players)\n",
    "    \n",
    "    if isinstance(mode, int):\n",
    "        return attentions_nohead_residual_normalized[:, mode, 0, 1:]\n",
    "    elif mode=='raw':\n",
    "        return attentions_nohead_residual_normalized[:, -1, 0, 1:]\n",
    "    elif mode=='rollout':\n",
    "        attentions_nohead_residual_normalized_rollout = compute_joint_attention(attentions_nohead_residual_normalized,\n",
    "                                                                                add_residual=False)\n",
    "        return attentions_nohead_residual_normalized_rollout[:, -1, 0, 1:]\n",
    "#explanation_to_mask(attention_rollout).argmin(axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220f301b",
   "metadata": {},
   "source": [
    "## lrp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8045007",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.transformer_explainability.baselines.ViT.ViT_new as ViT_new\n",
    "import utils.transformer_explainability.baselines.ViT.ViT_LRP as ViT_LRP\n",
    "import utils.transformer_explainability.baselines.ViT.ViT_orig_LRP as ViT_orig_LRP\n",
    "\n",
    "from utils.transformer_explainability.baselines.ViT.ViT_explanation_generator import Baselines, LRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e055b095",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "baselines_dict = OrderedDict()\n",
    "lrp_dict = OrderedDict()\n",
    "orig_lrp_dict = OrderedDict()\n",
    "\n",
    "\n",
    "for idx, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    checkpoint = torch.load(backbone_type_config[\"classifier_path\"], map_location=\"cpu\")\n",
    "    checkpoint[\"state_dict\"]=OrderedDict([(k.replace('backbone.',''), v) for k, v in checkpoint[\"state_dict\"].items()])\n",
    "    state_dict = checkpoint[\"state_dict\"]\n",
    "    \n",
    "    model = getattr(ViT_new, backbone_type)(num_classes=_config[\"output_dim\"]).to(_config[\"gpus_classifier\"][idx])\n",
    "    ret = model.load_state_dict(state_dict, strict=False)\n",
    "    print(f\"Model parameters were updated from a checkpoint file {backbone_type_config['classifier_path']}\")\n",
    "    print(f\"Unmatched parameters - missing_keys:    {ret.missing_keys}\")\n",
    "    print(f\"Unmatched parameters - unexpected_keys: {ret.unexpected_keys}\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output1=model(x.to(next(model.parameters()).device))\n",
    "        output2=classifier_dict[backbone_type](x.to(next(model.parameters()).device))['logits']\n",
    "        assert torch.allclose(output1,output2,atol=1e-03)\n",
    "    baselines = Baselines(model)\n",
    "    baselines_dict[backbone_type]=baselines        \n",
    "    \n",
    "    model_LRP=getattr(ViT_LRP, backbone_type)(num_classes=_config[\"output_dim\"]).to(_config[\"gpus_classifier\"][idx])\n",
    "    ret = model_LRP.load_state_dict(state_dict, strict=False)\n",
    "    print(f\"Model parameters were updated from a checkpoint file {backbone_type_config['classifier_path']}\")\n",
    "    print(f\"Unmatched parameters - missing_keys:    {ret.missing_keys}\")\n",
    "    print(f\"Unmatched parameters - unexpected_keys: {ret.unexpected_keys}\")\n",
    "    model_LRP.eval()      \n",
    "    lrp = LRP(model_LRP)\n",
    "    lrp_dict[backbone_type]=lrp\n",
    "    \n",
    "#     model_orig_LRP=getattr(ViT_orig_LRP, backbone_type)(num_classes=_config[\"output_dim\"]).to(_config[\"gpus_classifier\"][idx])\n",
    "#     ret = model_orig_LRP.load_state_dict(state_dict, strict=False)\n",
    "#     print(f\"Model parameters were updated from a checkpoint file {backbone_type_config['classifier_path']}\")\n",
    "#     print(f\"Unmatched parameters - missing_keys:    {ret.missing_keys}\")\n",
    "#     print(f\"Unmatched parameters - unexpected_keys: {ret.unexpected_keys}\")\n",
    "#     model_orig_LRP.eval()    \n",
    "#     orig_lrp = LRP(model_orig_LRP)  \n",
    "#     orig_lrp_dict[backbone_type]=orig_lrp\n",
    "    \n",
    "    \n",
    "def get_lrp_module_explanation(backbone_type, original_image, class_index=None, mode='transformer_attribution'):\n",
    "    if mode==\"transformer_attribution\": # ours\n",
    "        transformer_attribution = lrp_dict[backbone_type].generate_LRP(original_image.unsqueeze(0).to(next(lrp_dict[backbone_type].model.parameters()).device), method=\"transformer_attribution\", index=class_index).detach()\n",
    "    elif mode==\"rollout\": # rollout\n",
    "        transformer_attribution = baselines_dict[backbone_type].generate_rollout(original_image.unsqueeze(0).to(next(baselines_dict[backbone_type].model.parameters()).device), start_layer=1).detach()\n",
    "    elif mode==\"attn_last_layer\": # raw-attention\n",
    "        transformer_attribution = lrp_dict[backbone_type].generate_LRP(original_image.unsqueeze(0).to(next(lrp_dict[backbone_type].model.parameters()).device), method=\"last_layer_attn\", index=class_index).detach()\n",
    "    elif mode == 'attn_gradcam': # GradCAM\n",
    "        transformer_attribution = baselines_dict[backbone_type].generate_cam_attn(original_image.unsqueeze(0).to(next(baselines_dict[backbone_type].model.parameters()).device), index=class_index).detach()\n",
    "        transformer_attribution = transformer_attribution.reshape(1,-1)\n",
    "        #transformer_attribution=torch.nan_to_num(transformer_attribution,nan=0)\n",
    "        #transformer_attribution+=torch.rand(size=transformer_attribution.shape, device=transformer_attribution.device)*1e-20        \n",
    "        #transformer_attribution = (transformer_attribution - transformer_attribution.min()) / (transformer_attribution.max() - transformer_attribution.min())\n",
    "    elif mode == 'full_lrp':\n",
    "        transformer_attribution = orig_lrp_dict[backbone_type].generate_LRP(original_image.unsqueeze(0).to(next(orig_lrp_dict[backbone_type].model.parameters()).device), method=\"full\", index=class_index).detach()\n",
    "        #transformer_attribution = (transformer_attribution - transformer_attribution.min()) / (transformer_attribution.max() - transformer_attribution.min())\n",
    "    elif mode == 'lrp_last_layer':\n",
    "        transformer_attribution = orig_lrp_dict[backbone_type].generate_LRP(original_image.unsqueeze(0).to(next(orig_lrp_dict[backbone_type].model.parameters()).device), method=\"last_layer\", index=class_index).detach()\n",
    "        #transformer_attribution = (transformer_attribution - transformer_attribution.min()) / (transformer_attribution.max() - transformer_attribution.min())\n",
    "    #print(transformer_attribution.max(), transformer_attribution.min())\n",
    "    #print(transformer_attribution.shape)\n",
    "    return transformer_attribution    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40064fa3",
   "metadata": {},
   "source": [
    "## CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb8e73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.pytorch_grad_cam import GradCAM\n",
    "from utils.pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "\n",
    "def reshape_transform(tensor, height=14, width=14):\n",
    "    result = tensor[:, 1 :  , :].reshape(tensor.size(0),\n",
    "        height, width, tensor.size(2))\n",
    "\n",
    "    # Bring the channels to the first dimension,\n",
    "    # like in CNNs.\n",
    "    result = result.transpose(2, 3).transpose(1, 2)\n",
    "    return result\n",
    "\n",
    "class WrapperLogits(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model=model\n",
    "\n",
    "    def forward(self, images):\n",
    "        x = self.model(images)\n",
    "        return x['logits']\n",
    "\n",
    "cam_dict = OrderedDict()\n",
    "for backbone_type, backbone_type_config in backbone_type_config_dict.items():\n",
    "    cam_dict[backbone_type] = GradCAM(model=WrapperLogits(classifier_dict[backbone_type]),\n",
    "                                      target_layers=[classifier_dict[backbone_type].backbone.blocks[-1].norm1],\n",
    "                                      reshape_transform=reshape_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e2b912",
   "metadata": {},
   "source": [
    "## Gradient-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bc9f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import IntegratedGradients, InputXGradient, Saliency, NoiseTunnel\n",
    "import torch.nn as nn\n",
    "\n",
    "class FromPixel(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model=model\n",
    "\n",
    "    def forward(self, images):\n",
    "        x = self.model.backbone.patch_embed(images)\n",
    "        x = self.model.backbone.forward_features(x)['x']\n",
    "        logits = self.model.head(x)\n",
    "        \n",
    "        if _config[\"output_dim\"]==1:\n",
    "            return logits.sigmoid()\n",
    "        else:\n",
    "            return logits.softmax(dim=-1)\n",
    "    \n",
    "class FromEmbedding(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model=model\n",
    "\n",
    "    def forward(self, embedding):\n",
    "        x = self.model.backbone.forward_features(embedding)['x']\n",
    "        logits = self.model.head(x)\n",
    "        \n",
    "        if _config[\"output_dim\"]==1:\n",
    "            return logits.sigmoid()\n",
    "        else:\n",
    "            return logits.softmax(dim=-1)\n",
    "\n",
    "#Classifier Wrapping    \n",
    "classifier_pixel_dict = OrderedDict()\n",
    "classifier_embedding_dict = OrderedDict()\n",
    "\n",
    "for idx, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    classifier_pixel_dict[backbone_type]=FromPixel(classifier_dict_[backbone_type])\n",
    "    classifier_embedding_dict[backbone_type]=FromEmbedding(classifier_dict_[backbone_type])\n",
    "\n",
    "#Vanilla\n",
    "saliency_pixel_dict = OrderedDict()\n",
    "saliency_embedding_dict = OrderedDict()\n",
    "\n",
    "for idx, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    saliency_pixel_dict[backbone_type] = Saliency(classifier_pixel_dict[backbone_type])\n",
    "    saliency_embedding_dict[backbone_type] = Saliency(classifier_embedding_dict[backbone_type])      \n",
    "\n",
    "#NoiseTunnel\n",
    "noisetunnel_pixel_dict = OrderedDict()\n",
    "noisetunnel_embedding_dict = OrderedDict()\n",
    "\n",
    "for idx, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    noisetunnel_pixel_dict[backbone_type] = NoiseTunnel(saliency_pixel_dict[backbone_type])\n",
    "    noisetunnel_embedding_dict[backbone_type] = NoiseTunnel(saliency_embedding_dict[backbone_type])      \n",
    "\n",
    "#IntegratedGradients    \n",
    "ig_pixel_dict = OrderedDict()\n",
    "ig_embedding_dict = OrderedDict()\n",
    "\n",
    "for idx, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    ig_pixel_dict[backbone_type] = IntegratedGradients(classifier_pixel_dict[backbone_type])\n",
    "    ig_embedding_dict[backbone_type] = IntegratedGradients(classifier_embedding_dict[backbone_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb4e500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attributions_pixel_process(attributions_pixel):\n",
    "    attributions_pixel_sum = attributions_pixel.sum(axis=-3)\n",
    "    attributions_pixel_abssum = attributions_pixel.abs().sum(axis=-3)\n",
    "    attributions_pixel_patchsum = F.conv2d(attributions_pixel,\n",
    "                                           weight=torch.ones(size=(1, 3, 16, 16),\n",
    "                                                             dtype=attributions_pixel.dtype,\n",
    "                                                             device=attributions_pixel.device),\n",
    "                                           stride=16).squeeze(axis=1)#.flatten(1, 2)\n",
    "    attributions_pixel_pathabssum = F.conv2d(attributions_pixel.abs(),\n",
    "                                             weight=torch.ones(size=(1, 3, 16, 16),\n",
    "                                                               dtype=attributions_pixel.dtype,\n",
    "                                                               device=attributions_pixel.device),\n",
    "                                             stride=16).squeeze(axis=1)#.flatten(1, 2) \n",
    "    \n",
    "    return {'attributions_pixel_sum': attributions_pixel_sum.detach().cpu(),# makes sense? (but cannot used for benchmarking)\n",
    "            'attributions_pixel_abssum': attributions_pixel_abssum.detach().cpu(),# makes sense (but cannot used for benchmarking)\n",
    "            'attributions_pixel_patchsum': attributions_pixel_patchsum.detach().cpu(),  # makes sense?\n",
    "            'attributions_pixel_patchabssum': attributions_pixel_pathabssum.detach().cpu()  # makes sense    \n",
    "           }\n",
    "    \n",
    "    \n",
    "def attributions_embedding_process(attributions_embedding):\n",
    "    attributions_embedding_sum = attributions_embedding.sum(axis=-1)\n",
    "    attributions_embedding_abssum = attributions_embedding.abs().sum(axis=-1)\n",
    "    return {'attributions_embedding_sum': attributions_embedding_sum.detach().cpu(), # makes sense?\n",
    "            'attributions_embedding_abssum': attributions_embedding_abssum.detach().cpu() # makes sense\n",
    "           }  \n",
    "\n",
    "def get_vanilla(image, saliency_pixel=None, saliency_embedding=None):\n",
    "    result={}\n",
    "    with torch.no_grad():\n",
    "        if saliency_pixel is not None:\n",
    "            attributions_pixel = [saliency_pixel.attribute(inputs=image.unsqueeze(0).to(next(saliency_pixel.forward_func.parameters()).device), \n",
    "                                                           target=i) for i in range(_config[\"output_dim\"])]\n",
    "\n",
    "            attributions_pixel = torch.concat(attributions_pixel)\n",
    "            result.update(attributions_pixel_process(attributions_pixel))\n",
    "            \n",
    "        if saliency_embedding is not None:\n",
    "            attributions_embedding = [saliency_embedding.attribute(inputs=saliency_embedding.forward_func.model.backbone.patch_embed(image.unsqueeze(0).to(next(saliency_embedding.forward_func.parameters()).device)).detach(),\n",
    "                                                                   target=i) for i in range(_config[\"output_dim\"])]\n",
    "\n",
    "            attributions_embedding = torch.concat(attributions_embedding)\n",
    "            result.update(attributions_embedding_process(attributions_embedding))\n",
    "        \n",
    "    return result\n",
    "\n",
    "def get_sg(image, noisetunnel_pixel=None, noisetunnel_embedding=None):\n",
    "    result={}    \n",
    "    with torch.no_grad():\n",
    "        if noisetunnel_pixel is not None:\n",
    "            attributions_pixel = [noisetunnel_pixel.attribute(inputs=image.unsqueeze(0).to(next(noisetunnel_pixel.forward_func.parameters()).device),\n",
    "                                                              nt_type='smoothgrad',\n",
    "                                                              nt_samples=10,\n",
    "                                                              target=i) for i in range(_config[\"output_dim\"])]\n",
    "\n",
    "            attributions_pixel = torch.concat(attributions_pixel)\n",
    "            result.update(attributions_pixel_process(attributions_pixel))\n",
    "\n",
    "        if noisetunnel_embedding is not None:\n",
    "            attributions_embedding  = [noisetunnel_embedding.attribute(inputs=noisetunnel_embedding.forward_func.model.backbone.patch_embed(image.unsqueeze(0).to(next(noisetunnel_embedding.forward_func.parameters()).device)).detach(),\n",
    "                                                                       nt_type='smoothgrad',\n",
    "                                                                       nt_samples=10,            \n",
    "                                                                       target=i) for i in range(_config[\"output_dim\"])]\n",
    "\n",
    "            attributions_embedding = torch.concat(attributions_embedding)\n",
    "            result.update(attributions_embedding_process(attributions_embedding))\n",
    "\n",
    "        \n",
    "    return result\n",
    "\n",
    "def get_vargrad(image, noisetunnel_pixel=None, noisetunnel_embedding=None):\n",
    "    result={}    \n",
    "    with torch.no_grad():\n",
    "        if noisetunnel_pixel is not None:\n",
    "            attributions_pixel = [noisetunnel_pixel.attribute(inputs=image.unsqueeze(0).to(next(noisetunnel_pixel.forward_func.parameters()).device),\n",
    "                                                              nt_type='vargrad',\n",
    "                                                              nt_samples=10,\n",
    "                                                              target=i) for i in range(_config[\"output_dim\"])]\n",
    "\n",
    "            attributions_pixel = torch.concat(attributions_pixel)\n",
    "            result.update(attributions_pixel_process(attributions_pixel))   \n",
    "\n",
    "        if noisetunnel_embedding is not None:\n",
    "            attributions_embedding  = [noisetunnel_embedding.attribute(inputs=noisetunnel_embedding.forward_func.model.backbone.patch_embed(image.unsqueeze(0).to(next(noisetunnel_embedding.forward_func.parameters()).device)).detach(),\n",
    "                                                                       nt_type='vargrad',\n",
    "                                                                       nt_samples=10,            \n",
    "                                                                       target=i) for i in range(_config[\"output_dim\"])]\n",
    "\n",
    "            attributions_embedding = torch.concat(attributions_embedding)\n",
    "            result.update(attributions_embedding_process(attributions_embedding))            \n",
    "\n",
    "        \n",
    "    return result\n",
    "\n",
    "def get_ig(image, ig_pixel=None, ig_embedding=None):\n",
    "    result={}    \n",
    "    with torch.no_grad():\n",
    "        if ig_pixel is not None:\n",
    "            attributions_pixel = [ig_pixel.attribute(inputs=image.unsqueeze(0).to(next(ig_pixel.forward_func.parameters()).device),\n",
    "                                                                            baselines=torch.zeros_like(image).unsqueeze(0).to(next(ig_pixel.forward_func.parameters()).device),\n",
    "                                                                            target=i,\n",
    "                                                                            n_steps=10) for i in range(_config[\"output_dim\"])]\n",
    "\n",
    "            attributions_pixel = torch.concat(attributions_pixel)\n",
    "            result.update(attributions_pixel_process(attributions_pixel))           \n",
    "\n",
    "        if ig_embedding is not None:\n",
    "            attributions_embedding = [ig_embedding.attribute(inputs=ig_embedding.forward_func.model.backbone.patch_embed(image.unsqueeze(0).to(next(ig_embedding.forward_func.parameters()).device)).detach(),\n",
    "                                                                                           baselines=ig_embedding.forward_func.model.backbone.patch_embed(torch.zeros_like(image).unsqueeze(0).to(next(ig_embedding.forward_func.parameters()).device)).detach(),\n",
    "                                                                                           target=i,\n",
    "                                                                                           n_steps=10) for i in range(_config[\"output_dim\"])]\n",
    "\n",
    "            attributions_embedding = torch.concat(attributions_embedding)\n",
    "            result.update(attributions_embedding_process(attributions_embedding))          \n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24d0766",
   "metadata": {},
   "source": [
    "## leave-one-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232428ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_one_out(image, surrogate=None, classifier=None):\n",
    "    with torch.no_grad():\n",
    "        mask=torch.cat([torch.ones(1, 196) ,1-torch.eye(196)])\n",
    "        if surrogate is not None:\n",
    "            out=surrogate(image.unsqueeze(0).repeat(196+1, 1, 1, 1).to(surrogate.device), \n",
    "                          masks=mask.to(surrogate.device))\n",
    "        elif classifier is not None:\n",
    "            mask_scaled = torch.repeat_interleave(torch.repeat_interleave(mask.reshape(-1, 14, 14), 16, dim=2), 16, dim=1)\n",
    "            image_masked=image * mask_scaled.unsqueeze(1)\n",
    "            \n",
    "            if classifier.__class__==Classifier:\n",
    "                out=classifier(image_masked.to(classifier.device))\n",
    "            elif classifier.__class__==Surrogate:\n",
    "                out=classifier(image_masked.to(classifier.device),\n",
    "                              masks=torch.ones((len(image_masked),196)))\n",
    "            else:\n",
    "                raise\n",
    "            \n",
    "        if _config[\"output_dim\"]==1:\n",
    "            prob=out['logits'].sigmoid().detach().cpu().numpy()\n",
    "        else:\n",
    "            prob=out['logits'].softmax(dim=-1).detach().cpu().numpy()    \n",
    "        \n",
    "        result=prob[0:1]-prob[1:]\n",
    "\n",
    "    return result.transpose(1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef518bd",
   "metadata": {},
   "source": [
    "# RISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f988c172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rise(image, surrogate=None, classifier=None, include_prob=0.5, N=2000):\n",
    "    assert (surrogate is None) != (classifier is None)\n",
    "    \n",
    "    prob_list=[]\n",
    "    mask_list=[]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(N//100):\n",
    "            mask=torch.rand(100, 196)<include_prob\n",
    "            if surrogate is not None:\n",
    "                out=surrogate(image.unsqueeze(0).repeat(100, 1, 1, 1).to(surrogate.device), \n",
    "                              masks=(mask).to(surrogate.device))\n",
    "            elif classifier is not None:\n",
    "                mask_scaled = torch.repeat_interleave(torch.repeat_interleave(mask.reshape(-1, 14, 14), 16, dim=2), 16, dim=1)\n",
    "                image_masked = image * mask_scaled.unsqueeze(1)\n",
    "                del mask_scaled\n",
    "                if classifier.__class__==Classifier:\n",
    "                    out=classifier(image_masked.to(classifier.device))\n",
    "                elif classifier.__class__==Surrogate:\n",
    "                    out=classifier(image_masked.to(classifier.device),\n",
    "                                  masks=torch.ones_like(mask))\n",
    "                else:\n",
    "                    raise\n",
    "                #out=surrogate_dict[backbone_type](image_masked.to(surrogate_dict[backbone_type].device), \n",
    "                #             masks=torch.ones((100,196)).to(surrogate_dict[backbone_type].device))                \n",
    "            else:\n",
    "                raise\n",
    "            \n",
    "            if _config[\"output_dim\"]==1:\n",
    "                prob=out['logits'].sigmoid().detach().cpu().numpy()\n",
    "            else:\n",
    "                prob=out['logits'].softmax(dim=-1).detach().cpu().numpy()    \n",
    "            \n",
    "            del out\n",
    "            prob_list.append(prob)\n",
    "            mask_list.append(mask.numpy())\n",
    "            del mask\n",
    "            \n",
    "            \n",
    "    prob_list_array=np.concatenate(prob_list) # (num_trials, num_classes)\n",
    "    mask_list_array=np.concatenate(mask_list) # (num_trials, num_players)\n",
    "\n",
    "    result = (prob_list_array.T @ mask_list_array) # (num_classes, num_players)\n",
    "    result = result/mask_list_array.sum(axis=0)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e1e499",
   "metadata": {},
   "source": [
    "# KernelSHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fb0a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.shapreg import removal, games, shapley\n",
    "\n",
    "class SurrogateSHAPWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model=model\n",
    "        if _config[\"output_dim\"]==1:\n",
    "            self.activation=nn.Sigmoid()\n",
    "        else:\n",
    "            self.activation=nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        images, mask = x\n",
    "        mask = mask.squeeze(1).flatten(1)\n",
    "        out=self.model(images, mask)['logits']\n",
    "        out=self.activation(out)\n",
    "        return out\n",
    "\n",
    "surrogate_SHAP_wrapped_dict = OrderedDict()\n",
    "\n",
    "for idx, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    surrogate_SHAP_wrapped_dict[backbone_type]=SurrogateSHAPWrapper(surrogate_dict[backbone_type][\"pre-softmax\"])    \n",
    "\n",
    "def get_shap(surrogate_SHAP_wrapped, x, batch_size=64, thresh=0.2, variance_batches=60):\n",
    "    game = games.PredictionGame_torchimagetensor(surrogate_SHAP_wrapped, x)\n",
    "    explanation = shapley.ShapleyRegression(game, batch_size=batch_size, thresh=thresh, variance_batches=variance_batches)\n",
    "    return explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3c4f5f",
   "metadata": {},
   "source": [
    "# save_dict_setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9cee727",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation_save_dict={}\n",
    "for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    explanation_save_dict_backbone={\"random\":{},\n",
    "                                    \"attention_rollout\":{},\n",
    "                                    \"attention_last\":{},\n",
    "                                    \"LRP\":{},\n",
    "                                    \"gradcam\":{},\n",
    "                                    \"gradcamgithub\": {},\n",
    "                                    \"vanillapixel\": {},\n",
    "                                    \"vanillaembedding\": {},\n",
    "                                    \"sgpixel\": {},\n",
    "                                    \"sgembedding\": {},\n",
    "                                    \"vargradpixel\": {},\n",
    "                                    \"vargradembedding\": {},               \n",
    "                                    \"igpixel\": {},\n",
    "                                    \"igembedding\": {},\n",
    "                                    \"leaveoneoutclassifier\": {},\n",
    "                                    \"leaveoneoutsurrogate\": {},\n",
    "                                    \"riseclassifier\": {},\n",
    "                                    \"risesurrogate\": {},\n",
    "                                    \"ours\": {},\n",
    "                                    \"kernelshap\": {}\n",
    "                                    }\n",
    "    explanation_save_dict[backbone_type]=explanation_save_dict_backbone\n",
    "    \n",
    "def explanation_save_dict_update(backbone_type, explanation_method,\n",
    "                                 path_list, explanation_list, elapsed_time_list, \n",
    "                                 shape=None):\n",
    "    explanation_save_dict_backbone_method=explanation_save_dict[backbone_type][explanation_method]\n",
    "        \n",
    "    assert len(path_list) == len(explanation_list) == len(elapsed_time_list)\n",
    "    \n",
    "    for explanation, path, elapsed_time in zip(explanation_list, path_list, elapsed_time_list):\n",
    "        assert type(explanation)==np.ndarray\n",
    "        assert type(path)==str\n",
    "        assert type(elapsed_time)==float\n",
    "        if shape is not None:\n",
    "            assert explanation.shape==shape\n",
    "        explanation_save_dict_backbone_method[path]={\"explanation\": explanation.astype(float),\n",
    "                                                     \"elapsed_time\": elapsed_time}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79d38051",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random                         0   +    924   ->    924\n",
      "attention_rollout              0   +    924   ->    924\n",
      "attention_last                 0   +    923   ->    923\n",
      "LRP                            0   +    924   ->    924\n",
      "gradcam                        0   +    924   ->    924\n",
      "gradcamgithub                  0   +    924   ->    924\n",
      "vanillapixel                   0   +    924   ->    924\n",
      "vanillaembedding               0   +    924   ->    924\n",
      "sgpixel                        0   +    924   ->    924\n",
      "sgembedding                    0   +    924   ->    924\n",
      "vargradpixel                   0   +    924   ->    924\n",
      "vargradembedding               0   +    924   ->    924\n",
      "igpixel                        0   +    924   ->    924\n",
      "igembedding                    0   +    924   ->    924\n",
      "leaveoneoutclassifier          0   +    924   ->    924\n",
      "leaveoneoutsurrogate           0   +      0   ->      0\n",
      "riseclassifier                 0   +    924   ->    924\n",
      "risesurrogate                  0   +      0   ->      0\n",
      "ours                           0   +    924   ->    924\n",
      "kernelshap                     0   +      0   ->      0\n"
     ]
    }
   ],
   "source": [
    "for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    for explanation_method, explanation_save_dict_backbone_method in explanation_save_dict[backbone_type].items():\n",
    "        try:\n",
    "            explanation_save_dict_path=f'results/3_explanation_generate/{_config[\"datasets\"]}/{backbone_type}_{explanation_method}_{dataset_split}.pickle'\n",
    "\n",
    "            if os.path.isfile(explanation_save_dict_path):\n",
    "                with open(explanation_save_dict_path, 'rb') as f:\n",
    "                    explanation_save_dict_loaded=pickle.load(f)\n",
    "            else:\n",
    "                explanation_save_dict_loaded={}\n",
    "\n",
    "            len_original=len(explanation_save_dict_backbone_method)            \n",
    "            len_loaded=len(explanation_save_dict_loaded)\n",
    "            explanation_save_dict_backbone_method.update(explanation_save_dict_loaded)\n",
    "            len_updated=len(explanation_save_dict_backbone_method)\n",
    "\n",
    "            print(f'{explanation_method:24}  {len_original:6}   + {len_loaded:6}   -> {len_updated:6}') \n",
    "        except:\n",
    "            print('aa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a028c2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "insertdelete_save_dict={}\n",
    "for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    insertdelete_save_dict_backbone={\"random\":{},\n",
    "                                     \"attention_rollout\":{},\n",
    "                                     \"attention_last\":{},\n",
    "                                     \"LRP\":{},\n",
    "                                     \"gradcam\":{},\n",
    "                                     \"gradcamgithub\": {},\n",
    "                                     \"vanillapixel\": {},\n",
    "                                     \"vanillaembedding\": {},\n",
    "                                     \"sgpixel\": {},\n",
    "                                     \"sgembedding\": {},\n",
    "                                     \"vargradpixel\": {},\n",
    "                                     \"vargradembedding\": {},               \n",
    "                                     \"igpixel\": {},\n",
    "                                     \"igembedding\": {},\n",
    "                                     \"leaveoneoutclassifier\": {},\n",
    "                                     \"leaveoneoutsurrogate\": {},\n",
    "                                     \"riseclassifier\": {},\n",
    "                                     \"risesurrogate\": {},\n",
    "                                     \"ours\": {},\n",
    "                                     \"kernelshap\":{}\n",
    "                                    }\n",
    "    insertdelete_save_dict[backbone_type]=insertdelete_save_dict_backbone\n",
    "    \n",
    "def insertdelete_save_dict_update(backbone_type, explanation_method,\n",
    "                                 path_list, insert_list, delete_list,\n",
    "                                 shape=None):\n",
    "    insertdelete_save_dict_backbone_method=insertdelete_save_dict[backbone_type][explanation_method]\n",
    "        \n",
    "    assert len(path_list) == len(insert_list) == len(delete_list)\n",
    "    \n",
    "    for insert, delete, path in zip(insert_list, delete_list, path_list):\n",
    "        assert type(insert)==np.ndarray\n",
    "        assert type(delete)==np.ndarray\n",
    "        assert type(path)==str\n",
    "        if shape is not None:\n",
    "            assert insert.shape==shape\n",
    "            assert delete.shape==shape\n",
    "        insertdelete_save_dict_backbone_method[path]={\"insert\": insert.astype(float),\n",
    "                                                      \"delete\": delete.astype(float)\n",
    "                                                      }    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34817dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random                         0   +    677   ->    677\n",
      "attention_rollout              0   +    677   ->    677\n",
      "attention_last                 0   +    677   ->    677\n",
      "LRP                            0   +    677   ->    677\n",
      "gradcam                        0   +    677   ->    677\n",
      "gradcamgithub                  0   +    677   ->    677\n",
      "vanillapixel                   0   +    677   ->    677\n",
      "vanillaembedding               0   +    677   ->    677\n",
      "sgpixel                        0   +    677   ->    677\n",
      "sgembedding                    0   +    677   ->    677\n",
      "vargradpixel                   0   +    677   ->    677\n",
      "vargradembedding               0   +    677   ->    677\n",
      "igpixel                        0   +    677   ->    677\n",
      "igembedding                    0   +    677   ->    677\n",
      "leaveoneoutclassifier          0   +    677   ->    677\n",
      "leaveoneoutsurrogate           0   +      0   ->      0\n",
      "riseclassifier                 0   +    677   ->    677\n",
      "risesurrogate                  0   +      0   ->      0\n",
      "ours                           0   +    677   ->    677\n",
      "kernelshap                     0   +      0   ->      0\n"
     ]
    }
   ],
   "source": [
    "for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    for explanation_method, insertdelete_save_dict_backbone_method in insertdelete_save_dict[backbone_type].items():\n",
    "        insertdelete_save_dict_path=f'results/4_insert_delete/{_config[\"datasets\"]}/{backbone_type}_{explanation_method}_{dataset_split}.pickle'\n",
    "\n",
    "        if os.path.isfile(insertdelete_save_dict_path):\n",
    "            with open(insertdelete_save_dict_path, 'rb') as f:\n",
    "                insertdelete_save_dict_loaded=pickle.load(f)\n",
    "        else:\n",
    "            insertdelete_save_dict_loaded={}\n",
    "            \n",
    "        len_original=len(insertdelete_save_dict_backbone_method)            \n",
    "        len_loaded=len(insertdelete_save_dict_loaded)\n",
    "        insertdelete_save_dict_backbone_method.update(insertdelete_save_dict_loaded)\n",
    "        len_updated=len(insertdelete_save_dict_backbone_method)\n",
    "            \n",
    "        print(f'{explanation_method:24}  {len_original:6}   + {len_loaded:6}   -> {len_updated:6}')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7a38937",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity_save_dit={}\n",
    "for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    sensitivity_save_dit_backbone={\"attention_rollout\":{},\n",
    "                                   \"attention_last\":{},\n",
    "                                   \"LRP\":{},\n",
    "                                   \"gradcam\":{},\n",
    "                                   \"gradcamgithub\": {},\n",
    "                                   \"vanillapixel\": {},\n",
    "                                   \"vanillaembedding\": {},\n",
    "                                   \"sgpixel\": {},\n",
    "                                   \"sgembedding\": {},\n",
    "                                   \"vargradpixel\": {},\n",
    "                                   \"vargradembedding\": {},               \n",
    "                                   \"igpixel\": {},\n",
    "                                   \"igembedding\": {},\n",
    "                                   \"leaveoneoutclassifier\": {},\n",
    "                                   \"leaveoneoutsurrogate\": {},\n",
    "                                   \"riseclassifier\": {},\n",
    "                                   \"risesurrogate\": {},\n",
    "                                   \"ours\": {},\n",
    "                                   }\n",
    "    sensitivity_save_dit[backbone_type]=sensitivity_save_dit_backbone\n",
    "    \n",
    "def sensitivity_save_dit_update(backbone_type, explanation_method, num_included_players,\n",
    "                                path_list, sensitivity_list,\n",
    "                                shape=None):\n",
    "    \n",
    "    sensitivity_save_dit_backbone_method=sensitivity_save_dit[backbone_type][explanation_method]\n",
    "        \n",
    "    assert len(path_list) == len(sensitivity_list)\n",
    "    \n",
    "    for sensitivity, path in zip(sensitivity_list, path_list):\n",
    "        assert type(sensitivity)==np.ndarray\n",
    "        assert type(path)==str\n",
    "        if shape is not None:\n",
    "            assert sensitivity.shape==shape\n",
    "        sensitivity_save_dit_backbone_method.setdefault(path, {})\n",
    "        sensitivity_save_dit_backbone_method[path][num_included_players]=sensitivity.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df2a81b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_rollout              0   +    816   ->    816\n",
      "attention_last                 0   +    816   ->    816\n",
      "LRP                            0   +    816   ->    816\n",
      "gradcam                        0   +    816   ->    816\n",
      "gradcamgithub                  0   +    816   ->    816\n",
      "vanillapixel                   0   +    816   ->    816\n",
      "vanillaembedding               0   +    816   ->    816\n",
      "sgpixel                        0   +    816   ->    816\n",
      "sgembedding                    0   +    816   ->    816\n",
      "vargradpixel                   0   +    816   ->    816\n",
      "vargradembedding               0   +    815   ->    815\n",
      "igpixel                        0   +    815   ->    815\n",
      "igembedding                    0   +    815   ->    815\n",
      "leaveoneoutclassifier          0   +    815   ->    815\n",
      "leaveoneoutsurrogate           0   +      0   ->      0\n",
      "riseclassifier                 0   +    815   ->    815\n",
      "risesurrogate                  0   +      0   ->      0\n",
      "ours                           0   +    814   ->    814\n"
     ]
    }
   ],
   "source": [
    "for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    for explanation_method, sensitivity_save_dit_backbone_method in sensitivity_save_dit[backbone_type].items():\n",
    "        sensitivity_save_dit_path=f'results/5_sensitivity/{_config[\"datasets\"]}/{backbone_type}_{explanation_method}_{dataset_split}.pickle'\n",
    "\n",
    "        if os.path.isfile(sensitivity_save_dit_path):\n",
    "            with open(sensitivity_save_dit_path, 'rb') as f:\n",
    "                sensitivity_save_dit_loaded=pickle.load(f)\n",
    "        else:\n",
    "            sensitivity_save_dit_loaded={}\n",
    "\n",
    "        len_original=len(sensitivity_save_dit_backbone_method)            \n",
    "        len_loaded=len(sensitivity_save_dit_loaded)\n",
    "        sensitivity_save_dit_backbone_method.update(sensitivity_save_dit_loaded)\n",
    "        len_updated=len(sensitivity_save_dit_backbone_method)\n",
    "\n",
    "        print(f'{explanation_method:24}  {len_original:6}   + {len_loaded:6}   -> {len_updated:6}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c313913",
   "metadata": {},
   "outputs": [],
   "source": [
    "noretraining_save_dict={}\n",
    "for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    noretraining_save_dict_backbone={\"random\":{},\n",
    "                                     \"attention_rollout\":{},\n",
    "                                     \"attention_last\":{},\n",
    "                                     \"LRP\":{},\n",
    "                                     \"gradcam\":{},\n",
    "                                     \"gradcamgithub\": {},\n",
    "                                     \"vanillapixel\": {},\n",
    "                                     \"vanillaembedding\": {},\n",
    "                                     \"sgpixel\": {},\n",
    "                                     \"sgembedding\": {},\n",
    "                                     \"vargradpixel\": {},\n",
    "                                     \"vargradembedding\": {},               \n",
    "                                     \"igpixel\": {},\n",
    "                                     \"igembedding\": {},\n",
    "                                     \"leaveoneoutclassifier\": {},\n",
    "                                     \"leaveoneoutsurrogate\": {},\n",
    "                                     \"riseclassifier\": {},\n",
    "                                     \"risesurrogate\": {},\n",
    "                                     \"ours\": {},\n",
    "                                    }\n",
    "    noretraining_save_dict[backbone_type]=noretraining_save_dict_backbone\n",
    "    \n",
    "def noretraining_save_dict_update(backbone_type, explanation_method,\n",
    "                                 path_list, insert_list, delete_list,\n",
    "                                 shape=None):\n",
    "    noretraining_save_dict_backbone_method=noretraining_save_dict[backbone_type][explanation_method]\n",
    "        \n",
    "    assert len(path_list) == len(insert_list) == len(delete_list)\n",
    "    \n",
    "    for insert, delete, path in zip(insert_list, delete_list, path_list):\n",
    "        assert type(insert)==np.ndarray\n",
    "        assert type(delete)==np.ndarray\n",
    "        assert type(path)==str\n",
    "        if shape is not None:\n",
    "            assert insert.shape==shape\n",
    "            assert delete.shape==shape\n",
    "        noretraining_save_dict_backbone_method[path]={\"insert\": insert.astype(float),\n",
    "                                                      \"delete\": delete.astype(float)\n",
    "                                                      }    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644df8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    for explanation_method, noretraining_save_dict_backbone_method in noretraining_save_dict[backbone_type].items():\n",
    "        noretraining_save_dict_path=f'results/6_noretraining/{_config[\"datasets\"]}/{backbone_type}_{explanation_method}_{dataset_split}.pickle'\n",
    "\n",
    "        if os.path.isfile(noretraining_save_dict_path):\n",
    "            with open(noretraining_save_dict_path, 'rb') as f:\n",
    "                noretraining_save_dict_loaded=pickle.load(f)\n",
    "        else:\n",
    "            noretraining_save_dict_loaded={}\n",
    "            \n",
    "        len_original=len(noretraining_save_dict_backbone_method)            \n",
    "        len_loaded=len(noretraining_save_dict_loaded)\n",
    "        noretraining_save_dict_backbone_method.update(noretraining_save_dict_loaded)\n",
    "        len_updated=len(noretraining_save_dict_backbone_method)\n",
    "            \n",
    "        print(f'{explanation_method:24}  {len_original:6}   + {len_loaded:6}   -> {len_updated:6}')                                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cc56a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiermasked_save_dict={}\n",
    "for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    classifiermasked_save_dict_backbone={\"random\":{},\n",
    "                                     \"attention_rollout\":{},\n",
    "                                     \"attention_last\":{},\n",
    "                                     \"LRP\":{},\n",
    "                                     \"gradcam\":{},\n",
    "                                     \"gradcamgithub\": {},\n",
    "                                     \"vanillapixel\": {},\n",
    "                                     \"vanillaembedding\": {},\n",
    "                                     \"sgpixel\": {},\n",
    "                                     \"sgembedding\": {},\n",
    "                                     \"vargradpixel\": {},\n",
    "                                     \"vargradembedding\": {},               \n",
    "                                     \"igpixel\": {},\n",
    "                                     \"igembedding\": {},\n",
    "                                     \"leaveoneoutclassifier\": {},\n",
    "                                     \"leaveoneoutsurrogate\": {},\n",
    "                                     \"riseclassifier\": {},\n",
    "                                     \"risesurrogate\": {},\n",
    "                                     \"ours\": {},\n",
    "                                    }\n",
    "    classifiermasked_save_dict[backbone_type]=classifiermasked_save_dict_backbone\n",
    "    \n",
    "def classifiermasked_save_dict_update(backbone_type, explanation_method,\n",
    "                                 path_list, insert_list, delete_list,\n",
    "                                 shape=None):\n",
    "    classifiermasked_save_dict_backbone_method=classifiermasked_save_dict[backbone_type][explanation_method]\n",
    "        \n",
    "    assert len(path_list) == len(insert_list) == len(delete_list)\n",
    "    \n",
    "    for insert, delete, path in zip(insert_list, delete_list, path_list):\n",
    "        assert type(insert)==np.ndarray\n",
    "        assert type(delete)==np.ndarray\n",
    "        assert type(path)==str\n",
    "        if shape is not None:\n",
    "            assert insert.shape==shape\n",
    "            assert delete.shape==shape\n",
    "        classifiermasked_save_dict_backbone_method[path]={\"insert\": insert.astype(float),\n",
    "                                                      \"delete\": delete.astype(float)\n",
    "                                                      }    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfd4cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    for explanation_method, classifiermasked_save_dict_backbone_method in classifiermasked_save_dict[backbone_type].items():\n",
    "        classifiermasked_save_dict_path=f'results/7_classifiermasked/{_config[\"datasets\"]}/{backbone_type}_{explanation_method}_{dataset_split}.pickle'\n",
    "\n",
    "        if os.path.isfile(classifiermasked_save_dict_path):\n",
    "            with open(classifiermasked_save_dict_path, 'rb') as f:\n",
    "                classifiermasked_save_dict_loaded=pickle.load(f)\n",
    "        else:\n",
    "            classifiermasked_save_dict_loaded={}\n",
    "            \n",
    "        len_original=len(classifiermasked_save_dict_backbone_method)            \n",
    "        len_loaded=len(classifiermasked_save_dict_loaded)\n",
    "        classifiermasked_save_dict_backbone_method.update(classifiermasked_save_dict_loaded)\n",
    "        len_updated=len(classifiermasked_save_dict_backbone_method)\n",
    "            \n",
    "        print(f'{explanation_method:24}  {len_original:6}   + {len_loaded:6}   -> {len_updated:6}')                                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882fc10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsedtime_save_dict={}\n",
    "for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    elapsedtime_save_dict_backbone={\"random\":{},\n",
    "                                     \"attention_rollout\":{},\n",
    "                                     \"attention_last\":{},\n",
    "                                     \"LRP\":{},\n",
    "                                     \"gradcam\":{},\n",
    "                                     \"gradcamgithub\": {},\n",
    "                                     \"vanillapixel\": {},\n",
    "                                     \"vanillaembedding\": {},\n",
    "                                     \"sgpixel\": {},\n",
    "                                     \"sgembedding\": {},\n",
    "                                     \"vargradpixel\": {},\n",
    "                                     \"vargradembedding\": {},               \n",
    "                                     \"igpixel\": {},\n",
    "                                     \"igembedding\": {},\n",
    "                                     \"leaveoneoutclassifier\": {},\n",
    "                                     \"leaveoneoutsurrogate\": {},\n",
    "                                     \"riseclassifier\": {},\n",
    "                                     \"risesurrogate\": {},\n",
    "                                     \"ours\": {},\n",
    "                                    }\n",
    "    elapsedtime_save_dict[backbone_type]=elapsedtime_save_dict_backbone\n",
    "    \n",
    "def elapsedtime_save_dict_update(backbone_type, explanation_method,\n",
    "                                 path_list, elapsed_time_list,\n",
    "                                 shape=None):\n",
    "    elapsedtime_save_dict_backbone_method=elapsedtime_save_dict[backbone_type][explanation_method]\n",
    "        \n",
    "    assert len(path_list) == len(elapsed_time_list)\n",
    "    \n",
    "    for elapsed_time, path in zip(elapsed_time_list, path_list):\n",
    "        assert type(elapsed_time)==float\n",
    "        assert type(path)==str\n",
    "        elapsedtime_save_dict_backbone_method[path]={\"time\": elapsed_time}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50926815",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    for explanation_method, elapsedtime_save_dict_backbone_method in elapsedtime_save_dict[backbone_type].items():\n",
    "        elapsedtime_save_dict_path=f'results/8_elapsedtime/{_config[\"datasets\"]}/{backbone_type}_{explanation_method}_{dataset_split}.pickle'\n",
    "\n",
    "        if os.path.isfile(elapsedtime_save_dict_path):\n",
    "            with open(elapsedtime_save_dict_path, 'rb') as f:\n",
    "                elapsedtime_save_dict_loaded=pickle.load(f)\n",
    "        else:\n",
    "            elapsedtime_save_dict_loaded={}\n",
    "            \n",
    "        len_original=len(elapsedtime_save_dict_backbone_method)            \n",
    "        len_loaded=len(elapsedtime_save_dict_loaded)\n",
    "        elapsedtime_save_dict_backbone_method.update(elapsedtime_save_dict_loaded)\n",
    "        len_updated=len(elapsedtime_save_dict_backbone_method)\n",
    "            \n",
    "        print(f'{explanation_method:24}  {len_original:6}   + {len_loaded:6}   -> {len_updated:6}')                                                                                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf838f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimationerror_save_dict={}\n",
    "for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    estimationerror_save_dict_backbone={\"kernelshap\":{},\n",
    "                                        \"kernelshapnopair\":{},\n",
    "                                        \"ours\": {}\n",
    "                                        }\n",
    "    estimationerror_save_dict[backbone_type]=estimationerror_save_dict_backbone\n",
    "    \n",
    "def estimationerror_save_dict_update(backbone_type, explanation_method,\n",
    "                                     path_list, estimation_list, label_list,\n",
    "                                     shape=None):\n",
    "    estimationerror_save_dict_backbone_method=estimationerror_save_dict[backbone_type][explanation_method]\n",
    "        \n",
    "    assert len(path_list) == len(estimation_list) == len(label_list)\n",
    "    \n",
    "    for path, estimation, label in zip(path_list, estimation_list, label_list):\n",
    "        assert type(path)==str\n",
    "        #assert type(estimation)==np.ndarray\n",
    "        assert type(label)==int\n",
    "        \n",
    "        if shape is not None:\n",
    "            assert estimation.shape==shape        \n",
    "        \n",
    "        estimationerror_save_dict_backbone_method[path]={\"estimation\": estimation,\n",
    "                                                         \"label\": label}        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cfd7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    for explanation_method, estimationerror_save_dict_backbone_method in estimationerror_save_dict[backbone_type].items():\n",
    "        estimationerror_save_dict_path=f'results/9_estimationerror/{_config[\"datasets\"]}/{backbone_type}_{explanation_method}_{dataset_split}.pickle'\n",
    "\n",
    "        if os.path.isfile(estimationerror_save_dict_path):\n",
    "            with open(estimationerror_save_dict_path, 'rb') as f:\n",
    "                estimationerror_save_dict_loaded=pickle.load(f)\n",
    "        else:\n",
    "            estimationerror_save_dict_loaded={}\n",
    "            \n",
    "        len_original=len(estimationerror_save_dict_backbone_method)            \n",
    "        len_loaded=len(estimationerror_save_dict_loaded)\n",
    "        estimationerror_save_dict_backbone_method.update(estimationerror_save_dict_loaded)\n",
    "        len_updated=len(estimationerror_save_dict_backbone_method)\n",
    "            \n",
    "        print(f'{explanation_method:24}  {len_original:6}   + {len_loaded:6}   -> {len_updated:6}')                                                                                                                            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7c1111",
   "metadata": {},
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16405739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relative_value(x, random_seed=None):\n",
    "    assert len(x.shape)==1\n",
    "    \n",
    "    if isinstance(random_seed, int):\n",
    "        rng = np.random.default_rng(random_seed)\n",
    "        perm = rng.permutation(np.arange(len(x)))\n",
    "    else:\n",
    "        perm = np.random.permutation(np.arange(len(x)))    \n",
    "\n",
    "    argsorted=np.arange(len(x))[perm][np.argsort(x[perm])]\n",
    "    relative_value=np.argsort(argsorted)\n",
    "\n",
    "    return relative_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "252a017d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt_path(path_original, dict_keys):\n",
    "    path_list = ['l0.cs.washington.edu', 'l1lambda.cs.washington.edu', 'l2lambda.cs.washington.edu',\n",
    "                 'l3.cs.washington.edu', 'deeper.cs.washington.edu', 'sync', '/homes/gws/chanwkim/', '/mmfs1/home/chanwkim/']\n",
    "    dict_keys=list(dict_keys)\n",
    "\n",
    "\n",
    "    for path1 in path_list:\n",
    "        if path1 in path_original:\n",
    "            for path2 in path_list:\n",
    "                path_replaced=path_original.replace(path1, path2)\n",
    "                if path_replaced in dict_keys:\n",
    "                    return path_replaced\n",
    "    return path_original\n",
    "    #raise ValueError(f\"not found {path_original}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aef1c66",
   "metadata": {},
   "source": [
    "# Methods to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b71c18f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['random', 'attention_rollout', 'attention_last', 'LRP', 'gradcam', 'gradcamgithub', 'vanillapixel', 'vanillaembedding', 'sgpixel', 'sgembedding', 'vargradpixel', 'vargradembedding', 'igpixel', 'igembedding', 'leaveoneoutclassifier', 'riseclassifier', 'ours']\n"
     ]
    }
   ],
   "source": [
    "explanation_method_to_run_=[\"random\", \"attention_rollout\", \"attention_last\", \n",
    "                            \"LRP\", \"gradcam\", \"gradcamgithub\",\n",
    "                            \"vanillapixel\", \"vanillaembedding\",\n",
    "                            \"sgpixel\", \"sgembedding\",\n",
    "                            \"vargradpixel\", \"vargradembedding\",\n",
    "                            \"igpixel\", \"igembedding\",                           \n",
    "                            \"leaveoneoutclassifier\",\n",
    "                            \"riseclassifier\", \n",
    "                            \"ours\"]\n",
    "#explanation_method_to_run_=[\"kernelshap\"]\n",
    "# explanation_method_to_run_=[\"random\", \"attention_rollout\", \"attention_last\", \n",
    "#                             \"LRP\", \"gradcam\", \n",
    "#                             \"vanillaembedding\",\n",
    "#                             \"sgembedding\",\n",
    "#                             \"vargradembedding\",\n",
    "#                             \"igembedding\",                           \n",
    "#                             \"leaveoneoutclassifier\",\n",
    "#                             \"riseclassifier\", \n",
    "#                             \"ours\"]\n",
    "explanation_method_to_run=[]\n",
    "explanation_method_to_run+=explanation_method_to_run_[:]\n",
    "\n",
    "\n",
    "print(explanation_method_to_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "213d97c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1963\n",
      "1963\n"
     ]
    }
   ],
   "source": [
    "data_loader=DataLoader(dset, batch_size=1, shuffle=False, drop_last=False, num_workers=4) #16\n",
    "print(len(dset))\n",
    "print(len(data_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045b09d1",
   "metadata": {},
   "source": [
    "# 1_classifier_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4328d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluation_stage==\"1_classifier_evaluate\":    \n",
    "    classifier_result_list_all={}\n",
    "    for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "        classifier_result_list_all[backbone_type]={}\n",
    "           \n",
    "    \n",
    "    for idx, batch in enumerate(tqdm(data_loader)):\n",
    "\n",
    "        images=batch['images']\n",
    "        labels=batch['labels']\n",
    "        paths=batch['path']\n",
    "\n",
    "        for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "            # Get classifier output\n",
    "            classifier_dict[backbone_type].eval()\n",
    "            with torch.no_grad():\n",
    "                classifier_output=classifier_dict[backbone_type](images.to(classifier_dict[backbone_type].device),\n",
    "                                                                 output_attentions=True)\n",
    "            if _config[\"output_dim\"]==1:\n",
    "                prob=classifier_output['logits'].sigmoid().cpu().numpy()\n",
    "            else:\n",
    "                prob=classifier_output['logits'].softmax(dim=-1).cpu().numpy()          \n",
    "                \n",
    "                \n",
    "            for path, label, prob in zip(paths, labels, prob):\n",
    "                classifier_result_list_all[backbone_type][path]={'label':label.item(), 'prob':prob.astype(float)}\n",
    "                \n",
    "    for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "        classifier_result_list_path=f'results/1_classifier_evaluate/{_config[\"datasets\"]}/{backbone_type}_{dataset_split}.pickle'\n",
    "        with open(classifier_result_list_path, \"wb\") as f:\n",
    "            pickle.dump(classifier_result_list_all[backbone_type], f)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d237daa",
   "metadata": {},
   "source": [
    "# 2_surrogate_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e7849b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluation_stage==\"2_surrogate_evaluate\":\n",
    "    result_list_all={}\n",
    "    for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "        result_list_all[backbone_type]=[]\n",
    "\n",
    "    dset_loader=DataLoader(dset, batch_size=64, num_workers=4, shuffle=False, drop_last=True)\n",
    "\n",
    "    for batch_idx, batch in enumerate(tqdm(dset_loader, unit='batch')):  \n",
    "        for num_mask in range(0,196+1,14):\n",
    "            mask=torch.zeros((len(batch[\"images\"]), 196))\n",
    "            mask[:,:num_mask]=1\n",
    "            for i in range(len(mask)):\n",
    "                mask[i]=mask[i][torch.randperm(len(mask[i]))]\n",
    "            for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "                surrogate_dict[backbone_type][\"original\"].eval()\n",
    "                with torch.no_grad():\n",
    "                    out_original=surrogate_dict[backbone_type][\"original\"](batch[\"images\"].to(surrogate_dict[backbone_type][\"original\"].device),\n",
    "                                                                          torch.ones((len(batch[\"images\"]), 196)).to(surrogate_dict[backbone_type][\"original\"].device))\n",
    "\n",
    "                for mask_location_model in [\"original\" , \"pre-softmax\", \"zero-input\", \"zero-embedding\"]:\n",
    "                    if mask_location_model==\"original\":\n",
    "                        kl_divergence=0\n",
    "\n",
    "                        if _config[\"output_dim\"]==1:\n",
    "                            accuracy=((out_original[\"logits\"].sigmoid()>0.5).cpu().int()==batch['labels']).float().mean().item()\n",
    "                        else:\n",
    "                            accuracy=(torch.argmax(out_original[\"logits\"], dim=1).cpu()==batch['labels']).float().mean().item()\n",
    "\n",
    "                        result_list_all[backbone_type].append({\"batch_idx\": batch_idx,\n",
    "                                            \"backbone_type\": backbone_type,\n",
    "                                            \"num_mask\": num_mask,\n",
    "                                            \"mask_location_model\": mask_location_model,\n",
    "                                            \"mask_location_parameter\": \"original\",\n",
    "                                            \"kl_divergence\": kl_divergence,\n",
    "                                            \"accuracy\": accuracy})\n",
    "\n",
    "                    for mask_location_parameter in [\"pre-softmax\", \"post-softmax\", \"zero-input\", \"zero-embedding\", \"random-sampling\"]:\n",
    "                        surrogate_dict[backbone_type][mask_location_model].eval()\n",
    "                        with torch.no_grad():\n",
    "                            out_surrogate=surrogate_dict[backbone_type][mask_location_model](batch[\"images\"].to(surrogate_dict[backbone_type][mask_location_model].device), \n",
    "                                                                                             mask.to(surrogate_dict[backbone_type][mask_location_model].device),\n",
    "                                                                                             mask_location_parameter)\n",
    "\n",
    "\n",
    "                        if _config[\"output_dim\"]==1:\n",
    "                            kl_divergence = F.kl_div(input=torch.concat([F.logsigmoid(out_surrogate[\"logits\"]), F.logsigmoid(-out_surrogate[\"logits\"])], dim=1),\n",
    "                                                    target=torch.concat([torch.sigmoid(out_original[\"logits\"]), torch.sigmoid(-out_original[\"logits\"])], dim=1),\n",
    "                                                    reduction=\"batchmean\",\n",
    "                                                    log_target=False)                        \n",
    "\n",
    "                        else:\n",
    "                            kl_divergence=F.kl_div(input=torch.log_softmax(out_surrogate[\"logits\"], dim=1),\n",
    "                                                   target=torch.softmax(out_original[\"logits\"], dim=1),\n",
    "                                                   log_target=False,\n",
    "                                                   reduction='batchmean').item()                           \n",
    "\n",
    "                        if _config[\"output_dim\"]==1:\n",
    "                            accuracy=((out_surrogate[\"logits\"].sigmoid()>0.5).cpu().int()==batch['labels']).float().mean().item()\n",
    "                        else:\n",
    "                            accuracy=(torch.argmax(out_surrogate[\"logits\"], dim=1).cpu()==batch['labels']).float().mean().item()\n",
    "\n",
    "\n",
    "                        result_list_all[backbone_type].append({\"batch_idx\": batch_idx,\n",
    "                                            \"backbone_type\": backbone_type,\n",
    "                                            \"num_mask\": num_mask,\n",
    "                                            \"mask_location_model\": mask_location_model,\n",
    "                                            \"mask_location_parameter\": mask_location_parameter,\n",
    "                                            \"kl_divergence\": kl_divergence,\n",
    "                                            \"accuracy\": accuracy})\n",
    "                        \n",
    "                        \n",
    "    for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "        result_df=pd.DataFrame(result_list_all[backbone_type])\n",
    "\n",
    "        result_df.to_csv(f'results/4_0_surrogate_evaluate/{_config[\"datasets\"]}/{backbone_type}.csv')                            \n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d352d73b",
   "metadata": {},
   "source": [
    "# 3_explanation_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb324d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def adapt_path(path_original, path_format):\n",
    "#     path_list=['l0.cs.washington.edu', 'l1lambda.cs.washington.edu', 'l2lambda.cs.washington.edu', 'l3.cs.washington.edu', 'deeper.cs.washington.edu']\n",
    "    \n",
    "#     for path1 in path_list:\n",
    "#         if path1 in path_original:\n",
    "#             for path2 in path_list:\n",
    "#                 if path2 in path_format:\n",
    "#                     return path_original.replace(path1, path2)\n",
    "#             raise\n",
    "#     return path_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe91774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_explanation(num_players, num_samples=None):\n",
    "    if num_samples is None:\n",
    "        return np.random.RandomState(42).uniform(low=0, high=1e-40, size=(num_players,))\n",
    "    else:\n",
    "        return np.random.RandomState(42).uniform(low=0, high=1e-40, size=(num_samples, num_players))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439314e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_use=['Garbage truck', \n",
    "              'Tench', \n",
    "              'English springer', \n",
    "              'Parachute',  \n",
    "              'Golf ball', \n",
    "              'Gas pump']\n",
    "kernelshap_sample_idx_list_all=[]\n",
    "for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    for random_seed in [2, 3, 4, 5]:\n",
    "        label_data_list=np.array([i['label'] for i in dset.data])\n",
    "        kernelshap_sample_idx_list=[np.random.RandomState(random_seed).choice(np.arange(len(label_data_list))[(label_data_list==label_idx)]) for label_idx in [label_name_list.index(label) for label in label_to_use]]\n",
    "        kernelshap_sample_idx_list_all+=kernelshap_sample_idx_list\n",
    "kernelshap_sample_path_list_all=[dset[i]['path'] for i in kernelshap_sample_idx_list_all]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1478944c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if evaluation_stage==\"3_explanation_generate\":\n",
    "    for idx, batch in enumerate(tqdm(data_loader)):#, total = int(1000/data_loader.batch_size+0.5) if dataset_split==\"test\" else None)):\n",
    "#         if dataset_split==\"test\":\n",
    "#             if idx>int(1000/data_loader.batch_size+0.5):\n",
    "#                 break\n",
    "        if (idx%parallel_mode[1])!=parallel_mode[0]:\n",
    "            continue\n",
    "            \n",
    "        images=batch['images']\n",
    "        labels=batch['labels']\n",
    "        paths=batch['path']\n",
    "        updated_signal_list=[]\n",
    "        \n",
    "        for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "            # Get classifier output\n",
    "            classifier_dict[backbone_type].eval()\n",
    "            with torch.no_grad():\n",
    "                classifier_output=classifier_dict[backbone_type](images.to(classifier_dict[backbone_type].device),\n",
    "                                                                 output_attentions=True)        \n",
    "            for explanation_method in explanation_method_to_run:\n",
    "                data_keys=explanation_save_dict[backbone_type][explanation_method].keys()\n",
    "                data_keys=[adapt_path(data_keys_path, paths) for data_keys_path in data_keys]\n",
    "                if all([path in data_keys for path in paths]):\n",
    "                    continue\n",
    "                else:\n",
    "                    print(explanation_method,'not exist')\n",
    "                    updated_signal_list.append(explanation_method)\n",
    "                if explanation_method==\"random\":\n",
    "                    start_time=time.time()\n",
    "                    explanation_random_list=[get_random_explanation(num_players=196) for path in paths]\n",
    "                    elapsed_time=time.time()-start_time\n",
    "                    explanation_save_dict_update(backbone_type, 'random', path_list=paths, explanation_list=explanation_random_list, elapsed_time_list=[elapsed_time/len(paths) for i in range(len(paths))], shape=(196, ))\n",
    "                \n",
    "                elif explanation_method==\"attention_rollout\":\n",
    "                    attentions = np.asarray([att.cpu().detach().numpy() for att in classifier_output['self_attentions']]).transpose(1,0,2,3,4)\n",
    "                    start_time=time.time()\n",
    "                    explanation_attention_rollout_list=attentions_to_explanation(attentions, mode='rollout')\n",
    "                    elapsed_time=time.time()-start_time\n",
    "                    explanation_save_dict_update(backbone_type, 'attention_rollout', path_list=paths, explanation_list=explanation_attention_rollout_list, elapsed_time_list=[elapsed_time/len(paths) for i in range(len(paths))], shape=(196,))\n",
    "\n",
    "                elif explanation_method==\"attention_last\":\n",
    "                    attentions = np.asarray([att.cpu().detach().numpy() for att in classifier_output['self_attentions']]).transpose(1,0,2,3,4)\n",
    "                    start_time=time.time()\n",
    "                    explanation_attention_last_list=attentions_to_explanation(attentions, mode=-1)\n",
    "                    elapsed_time=time.time()-start_time\n",
    "                    explanation_save_dict_update(backbone_type, 'attention_last', path_list=paths, explanation_list=explanation_attention_last_list, elapsed_time_list=[elapsed_time/len(paths) for i in range(len(paths))], shape=(196,))            \n",
    "\n",
    "                elif explanation_method==\"LRP\":\n",
    "                    explanation_lrp_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        explanation_lrp_list.append(np.concatenate([get_lrp_module_explanation(backbone_type=backbone_type,\n",
    "                                                                              original_image=image.squeeze(0),\n",
    "                                                                              class_index=i,\n",
    "                                                                              mode='transformer_attribution').cpu().numpy() for i in range(_config[\"output_dim\"])], axis=0))\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    explanation_save_dict_update(backbone_type, 'LRP', path_list=paths, explanation_list=explanation_lrp_list, elapsed_time_list=elapsed_time_list, shape=(_config[\"output_dim\"], 196))\n",
    "\n",
    "                elif explanation_method==\"gradcam\":\n",
    "                    explanation_gradcam_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        explanation_gradcam = np.concatenate([get_lrp_module_explanation(backbone_type=backbone_type,\n",
    "                                                                                              original_image=image.squeeze(0),\n",
    "                                                                                              class_index=i,\n",
    "                                                                                              mode='attn_gradcam').cpu().numpy() for i in range(_config[\"output_dim\"])], axis=0)\n",
    "                        explanation_gradcam = np.nan_to_num(explanation_gradcam,nan=0)+np.random.uniform(low=0, high=1e-20, size=explanation_gradcam.shape)                    \n",
    "                        explanation_gradcam_list.append(explanation_gradcam)\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    explanation_save_dict_update(backbone_type, 'gradcam', path_list=paths, explanation_list=explanation_gradcam_list, elapsed_time_list=elapsed_time_list, shape=(_config[\"output_dim\"], 196))\n",
    "\n",
    "\n",
    "                elif explanation_method==\"gradcamgithub\":\n",
    "                    explanation_gradcamgithub_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        explanation_gradcamgithub = np.concatenate([cam_dict[backbone_type](input_tensor=image.unsqueeze(0).to(next(cam_dict[backbone_type].model.parameters()).device),\n",
    "                                                                                            targets=[ClassifierOutputTarget(i)], resize=False).flatten()[np.newaxis,:] for i in range(_config[\"output_dim\"])], axis=0)\n",
    "                        explanation_gradcamgithub_list.append(explanation_gradcamgithub)\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    explanation_save_dict_update(backbone_type, 'gradcamgithub', path_list=paths, explanation_list=explanation_gradcamgithub_list, elapsed_time_list=elapsed_time_list, shape=(_config[\"output_dim\"], 196))\n",
    "\n",
    "                elif explanation_method==\"vanillapixel\":\n",
    "                    explanation_vanillapixel_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        grad=get_vanilla(image, saliency_pixel=saliency_pixel_dict[backbone_type])\n",
    "                        explanation_vanillapixel_list.append(grad[\"attributions_pixel_patchabssum\"].reshape(_config[\"output_dim\"],196).numpy())\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    explanation_save_dict_update(backbone_type, 'vanillapixel', path_list=paths, explanation_list=explanation_vanillapixel_list, elapsed_time_list=elapsed_time_list, shape=(_config[\"output_dim\"], 196))\n",
    "\n",
    "                elif explanation_method==\"vanillaembedding\":\n",
    "                    explanation_vanillaembedding_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:\n",
    "                        start_time=time.time()\n",
    "                        grad=get_vanilla(image, saliency_embedding=saliency_embedding_dict[backbone_type])\n",
    "                        explanation_vanillaembedding_list.append(grad[\"attributions_embedding_abssum\"].reshape(_config[\"output_dim\"],196).numpy())\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    explanation_save_dict_update(backbone_type, 'vanillaembedding', path_list=paths, explanation_list=explanation_vanillaembedding_list, elapsed_time_list=elapsed_time_list, shape=(_config[\"output_dim\"], 196))\n",
    "\n",
    "                elif explanation_method==\"sgpixel\":\n",
    "                    explanation_sgpixel_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        grad=get_sg(image, noisetunnel_pixel=noisetunnel_pixel_dict[backbone_type])\n",
    "                        explanation_sgpixel_list.append(grad[\"attributions_pixel_patchabssum\"].reshape(_config[\"output_dim\"],196).numpy())\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    explanation_save_dict_update(backbone_type, 'sgpixel', path_list=paths, explanation_list=explanation_sgpixel_list, elapsed_time_list=elapsed_time_list, shape=(_config[\"output_dim\"], 196))\n",
    "\n",
    "\n",
    "                elif explanation_method==\"sgembedding\":\n",
    "                    explanation_sgembedding_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        grad=get_sg(image, noisetunnel_embedding=noisetunnel_embedding_dict[backbone_type])\n",
    "                        explanation_sgembedding_list.append(grad[\"attributions_embedding_abssum\"].reshape(_config[\"output_dim\"],196).numpy())\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    explanation_save_dict_update(backbone_type, 'sgembedding', path_list=paths, explanation_list=explanation_sgembedding_list, elapsed_time_list=elapsed_time_list, shape=(_config[\"output_dim\"], 196))                                \n",
    "\n",
    "                elif explanation_method==\"vargradpixel\":\n",
    "                    explanation_vargradpixel_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        grad=get_vargrad(image, noisetunnel_pixel=noisetunnel_pixel_dict[backbone_type])\n",
    "                        explanation_vargradpixel_list.append(grad[\"attributions_pixel_patchabssum\"].reshape(_config[\"output_dim\"],196).numpy())\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    explanation_save_dict_update(backbone_type, 'vargradpixel', path_list=paths, explanation_list=explanation_vargradpixel_list, elapsed_time_list=elapsed_time_list, shape=(_config[\"output_dim\"], 196))\n",
    "\n",
    "                elif explanation_method==\"vargradembedding\":\n",
    "                    explanation_vargradembedding_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        grad=get_vargrad(image, noisetunnel_embedding=noisetunnel_embedding_dict[backbone_type])\n",
    "                        explanation_vargradembedding_list.append(grad[\"attributions_embedding_abssum\"].reshape(_config[\"output_dim\"],196).numpy())\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    explanation_save_dict_update(backbone_type, 'vargradembedding', path_list=paths, explanation_list=explanation_vargradembedding_list, elapsed_time_list=elapsed_time_list, shape=(_config[\"output_dim\"], 196))                                \n",
    "\n",
    "                elif explanation_method==\"igpixel\":\n",
    "                    explanation_igpixel_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        grad=get_ig(image, ig_pixel=ig_pixel_dict[backbone_type])\n",
    "                        explanation_igpixel_list.append(grad[\"attributions_pixel_patchsum\"].reshape(_config[\"output_dim\"],196).numpy())\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    explanation_save_dict_update(backbone_type, 'igpixel', path_list=paths, explanation_list=explanation_igpixel_list, elapsed_time_list=elapsed_time_list, shape=(_config[\"output_dim\"], 196))\n",
    "\n",
    "                elif explanation_method==\"igembedding\":\n",
    "                    explanation_igembedding_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        grad=get_ig(image, ig_embedding=ig_embedding_dict[backbone_type])\n",
    "                        explanation_igembedding_list.append(grad[\"attributions_embedding_sum\"].reshape(_config[\"output_dim\"],196).numpy())\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    explanation_save_dict_update(backbone_type, 'igembedding', path_list=paths, explanation_list=explanation_igembedding_list, elapsed_time_list=elapsed_time_list, shape=(_config[\"output_dim\"], 196))                \n",
    "\n",
    "                elif explanation_method==\"leaveoneoutclassifier\":\n",
    "                    explanation_leaveoneoutclassifier_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        explanation_leaveoneoutclassifier = leave_one_out(classifier=classifier_dict_[backbone_type], image=image).reshape(_config[\"output_dim\"], 196)\n",
    "                        explanation_leaveoneoutclassifier_list.append(explanation_leaveoneoutclassifier)\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    explanation_save_dict_update(backbone_type, 'leaveoneoutclassifier', path_list=paths, explanation_list=explanation_leaveoneoutclassifier_list, elapsed_time_list=elapsed_time_list, shape=(_config[\"output_dim\"], 196))\n",
    "\n",
    "                elif explanation_method==\"leaveoneoutsurrogate\":\n",
    "                    explanation_leaveoneoutsurrogate_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        explanation_leaveoneoutsurrogate = leave_one_out(surrogate=surrogate_dict[backbone_type][\"pre-softmax\"], image=image).reshape(_config[\"output_dim\"], 196)\n",
    "                        explanation_leaveoneoutsurrogate_list.append(explanation_leaveoneoutsurrogate)\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    explanation_save_dict_update(backbone_type, 'leaveoneoutsurrogate', path_list=paths, explanation_list=explanation_leaveoneoutsurrogate_list, elapsed_time_list=elapsed_time_list, shape=(_config[\"output_dim\"], 196))                \n",
    "\n",
    "                elif explanation_method==\"riseclassifier\":\n",
    "                    explanation_riseclassifier_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        explanation_riseclassifier = rise(classifier=classifier_dict_[backbone_type], image=image, N=2000, include_prob=0.5).reshape(_config[\"output_dim\"], 196)\n",
    "                        explanation_riseclassifier_list.append(explanation_riseclassifier)\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    explanation_save_dict_update(backbone_type, 'riseclassifier', path_list=paths, explanation_list=explanation_riseclassifier_list, elapsed_time_list=elapsed_time_list, shape=(_config[\"output_dim\"], 196))\n",
    "\n",
    "                elif explanation_method==\"risesurrogate\":\n",
    "                    explanation_risesurrogate_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        explanation_risesurrogate = rise(surrogate=surrogate_dict[backbone_type][\"pre-softmax\"], image=image, N=2000, include_prob=0.5).reshape(_config[\"output_dim\"], 196)\n",
    "                        explanation_risesurrogate_list.append(explanation_risesurrogate)\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    explanation_save_dict_update(backbone_type, 'risesurrogate', path_list=paths, explanation_list=explanation_risesurrogate_list, elapsed_time_list=elapsed_time_list, shape=(_config[\"output_dim\"], 196))                \n",
    "                    \n",
    "                elif explanation_method==\"ours\":\n",
    "                    start_time=time.time()\n",
    "                    explainer_dict[backbone_type].eval()\n",
    "                    with torch.no_grad():\n",
    "                        explanation_ours = explainer_dict[backbone_type](images.to(explainer_dict[backbone_type].device))[0].detach().cpu().numpy().transpose(0, 2, 1)\n",
    "                    elapsed_time=time.time()-start_time\n",
    "                    explanation_save_dict_update(backbone_type, 'ours', path_list=paths, explanation_list=explanation_ours, elapsed_time_list=[elapsed_time/len(paths) for i in range(len(paths))], shape=(_config[\"output_dim\"], 196))                                \n",
    "                    \n",
    "                elif explanation_method==\"kernelshap\":                    \n",
    "                    explanation_kernelshap_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    path_list=[]\n",
    "                    surrogate_SHAP_wrapped_dict[backbone_type].eval()\n",
    "                    for path,image in zip(paths, images):\n",
    "                        if path not in kernelshap_sample_path_list_all:\n",
    "                            continue\n",
    "                        print(path)\n",
    "                        start_time=time.time()                        \n",
    "                        explanation_kernelshap_ret = get_shap(surrogate_SHAP_wrapped_dict[backbone_type], image, thresh=0.2)\n",
    "                        explanation_kernelshap = explanation_kernelshap_ret.values.T\n",
    "                        explanation_kernelshap_list.append(explanation_kernelshap)\n",
    "                        path_list.append(path)\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    explanation_save_dict_update(backbone_type, 'kernelshap', path_list=path_list, explanation_list=explanation_kernelshap_list, elapsed_time_list=elapsed_time_list, shape=(_config[\"output_dim\"], 196))                    \n",
    "                else:\n",
    "                    raise\n",
    "\n",
    "        try:\n",
    "            for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "                for explanation_method, explanation_save_dict_backbone_method in explanation_save_dict[backbone_type].items():\n",
    "                    if explanation_method not in updated_signal_list:\n",
    "                        continue\n",
    "                    explanation_save_dict_path=f'results/3_explanation_generate/{_config[\"datasets\"]}/{backbone_type}_{explanation_method}_{dataset_split}.pickle'\n",
    "\n",
    "                    if os.path.isfile(explanation_save_dict_path):\n",
    "                        try:\n",
    "                            with open(explanation_save_dict_path, 'rb') as f:\n",
    "                                explanation_save_dict_loaded=pickle.load(f)\n",
    "                        except:\n",
    "                            explanation_save_dict_loaded={}\n",
    "                    else:\n",
    "                        explanation_save_dict_loaded={}\n",
    "\n",
    "                    len_original=len(explanation_save_dict_backbone_method)            \n",
    "                    len_loaded=len(explanation_save_dict_loaded)\n",
    "                    explanation_save_dict_backbone_method.update(explanation_save_dict_loaded)\n",
    "                    len_updated=len(explanation_save_dict_backbone_method)\n",
    "\n",
    "                    print(f'{explanation_method:24}  {len_original:6}   + {len_loaded:6}   -> {len_updated:6}')\n",
    "\n",
    "                    with open(explanation_save_dict_path, \"wb\") as f:\n",
    "                        pickle.dump(explanation_save_dict_backbone_method, f)\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "            \n",
    "            \n",
    "    #         # Get ours\n",
    "    #         start_time=time.time();explainer_dict[backbone_type].eval()\n",
    "    #         with torch.no_grad():\n",
    "    #             values, _, _=explainer_dict[backbone_type](torch.Tensor(image).unsqueeze(0).to(explainer_dict[backbone_type].device))\n",
    "    #             values=values.cpu().numpy().transpose(0,2,1).squeeze(0)\n",
    "    #         values_ours=(values, time.time()-start_time)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41b8d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluation_stage==\"3_explanation_generate\":\n",
    "    for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "        for explanation_method, explanation_save_dict_backbone_method in explanation_save_dict[backbone_type].items():\n",
    "            explanation_save_dict_path=f'results/3_explanation_generate/{_config[\"datasets\"]}/{backbone_type}_{explanation_method}_{dataset_split}.pickle'\n",
    "\n",
    "            if os.path.isfile(explanation_save_dict_path):\n",
    "                with open(explanation_save_dict_path, 'rb') as f:\n",
    "                    explanation_save_dict_loaded=pickle.load(f)\n",
    "            else:\n",
    "                explanation_save_dict_loaded={}\n",
    "\n",
    "            len_original=len(explanation_save_dict_backbone_method)            \n",
    "            len_loaded=len(explanation_save_dict_loaded)\n",
    "            explanation_save_dict_backbone_method.update(explanation_save_dict_loaded)\n",
    "            len_updated=len(explanation_save_dict_backbone_method)\n",
    "\n",
    "            print(f'{explanation_method:24}  {len_original:6}   + {len_loaded:6}   -> {len_updated:6}')\n",
    "\n",
    "            with open(explanation_save_dict_path, \"wb\") as f:\n",
    "                pickle.dump(explanation_save_dict_backbone_method, f)       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcab1c5",
   "metadata": {},
   "source": [
    "# 4_insert_delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1c78578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explanation_to_mask(explanation, mode='insertion'):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        explanation: (num_batches, num_players)\n",
    "    Returns:\n",
    "        explanation_expaned_bool: (num_batches, num_players+1, num_players)\n",
    "    \"\"\"\n",
    "    \n",
    "    explanation_expaned=np.repeat(explanation[:,np.newaxis,:], explanation.shape[-1], axis=1) # (num_batches, num_players, num_players)\n",
    "    \n",
    "    if mode=='insertion':\n",
    "        explanation_expaned_bool = explanation_expaned > ((np.sort(explanation, axis=-1)[:, : :-1])[:, :, np.newaxis]) # (num_batches, num_players, num_players)\n",
    "        explanation_expaned_bool = np.concatenate([explanation_expaned_bool,\n",
    "                                                   np.ones(shape=(explanation_expaned_bool.shape[0], 1, explanation_expaned_bool.shape[2]))==1], axis=1) # (num_batches, num_players+1, num_players)\n",
    "        #print(explanation_expaned_bool.shape)\n",
    "    elif mode=='deletion':\n",
    "        explanation_expaned_bool = explanation_expaned < ((np.sort(explanation, axis=-1)[:, : :-1])[:, :, np.newaxis]) # (num_batches, num_players, num_players)\n",
    "        explanation_expaned_bool = np.concatenate([np.ones(shape=(explanation_expaned_bool.shape[0], 1, explanation_expaned_bool.shape[2]))==1,\n",
    "                                                   explanation_expaned_bool],axis=1) # (num_batches, num_players+1, num_players)        \n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f'{mode} should be insertion or deletion.')\n",
    "    \n",
    "    \n",
    "    return explanation_expaned_bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "835e5d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_stage=\"4_insert_delete\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161dc89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation_method_to_run=[\"kernelshap\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e30e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimationerror_sample_path_list=pd.DataFrame(data_loader.dataset.data).groupby(\"label\").apply(lambda x: x.sample(n=10, random_state=42))[\"img_path\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37cb8db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a1e097",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_mode=(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c495b86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "[path in data_keys for path in paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c028ed84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_4351.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_2252.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_19792.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_4290.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_4062.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/ILSVRC2012_val_00017020.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_10200.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_12260.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_3260.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_6190.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_8971.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_3190.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_18040.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_10241.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_930.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_9011.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_11871.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_8312.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_13871.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_3651.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_37161.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_24352.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/ILSVRC2012_val_00022252.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_11481.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_16220.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_9940.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_23321.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_4220.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_121.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_7982.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/ILSVRC2012_val_00026451.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_8522.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_48060.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_7790.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_29712.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_3241.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_16300.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_17551.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_30001.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_18981.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_6241.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_7841.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_12972.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_9642.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_3311.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_6992.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_11210.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_4900.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_20360.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_11091.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_30231.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_49041.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_8992.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_521.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_4352.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_20301.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_14910.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_28352.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_4511.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_10451.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_2511.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_5820.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_31592.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_1072.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_931.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_3281.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/ILSVRC2012_val_00025761.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_9311.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_12291.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_19162.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_15571.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_3532.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_15262.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/ILSVRC2012_val_00024560.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_30072.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_2390.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_8552.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_2122.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_672.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_5712.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_15441.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_24332.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_17460.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_14582.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_24552.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_1222.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_32350.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_5781.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_1132.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_4470.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_13600.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/ILSVRC2012_val_00030740.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_2782.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_16861.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_39880.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_41101.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_5641.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_8081.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_4131.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_27102.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_11081.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_8330.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_6622.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_15810.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_11871.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_782.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_28600.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_15701.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_35172.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_6421.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_7090.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_4980.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_29062.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_11701.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_3932.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_3582.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_8260.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_23510.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_71550.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_11401.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_29580.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_261.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_1002.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_15152.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_10230.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_10592.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_18622.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_9821.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_72470.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_8240.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_6882.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_4691.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_37571.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_7372.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_562.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_48881.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_26102.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_6180.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_51440.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_4260.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_9770.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_16321.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_12831.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_19842.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_11000.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_13480.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_13231.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_6391.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_911.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_20281.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_20061.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_10251.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_4162.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_3940.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_4020.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_4310.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_11452.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_2391.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_6752.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_16920.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_16051.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_29231.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_19501.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_29462.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_8542.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_1100.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_38841.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_16090.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_7292.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_230.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_11092.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_5362.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_10271.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_12632.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_6811.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_4492.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_4241.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_14351.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_5312.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_2102.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_38201.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_38680.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_10491.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_2730.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_3471.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_4320.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_2822.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_41871.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_10692.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_7772.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_19020.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_17192.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_39102.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_19472.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_17782.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_19542.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_102.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_3922.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_5231.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_27662.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_2342.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_18871.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_390.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_2290.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/ILSVRC2012_val_00038942.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_2270.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_46700.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_30141.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_5852.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/ILSVRC2012_val_00004301.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_13541.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_8610.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_7792.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_7311.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_5690.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_73490.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_14112.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_13281.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_192.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_23440.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_12802.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/ILSVRC2012_val_00035211.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_4411.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_3530.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_13250.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_15291.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_8891.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_4731.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_52232.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_1792.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_24941.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_2490.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/ILSVRC2012_val_00027110.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_9311.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_8681.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_532.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_28350.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_22681.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_22390.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_11011.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_10531.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_24391.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_28400.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_8552.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_5641.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_4852.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_42671.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_3061.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_2572.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_24681.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_7011.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_3780.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_2402.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_2321.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_6162.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_3792.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/ILSVRC2012_val_00008162.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_18152.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_3011.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_1630.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_17690.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_11470.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_1300.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_56022.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/ILSVRC2012_val_00023440.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_43251.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_9260.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_14682.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_5731.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_4972.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_7582.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_31961.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_5370.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_38560.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_16782.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_9300.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_16280.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/ILSVRC2012_val_00035160.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_16551.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_362.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_16370.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_1660.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_482.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_2340.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_501.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_1621.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_20500.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_320.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_19260.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_18590.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_12170.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_6160.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_11190.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_20572.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_5280.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_2170.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_6662.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_30671.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_31710.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_7092.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_10992.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_14870.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_6531.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_8661.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_17851.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_5120.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_26541.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/ILSVRC2012_val_00022172.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_67480.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_880.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_10141.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_20382.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_6251.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_35890.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_5090.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_3890.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_23911.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_21350.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_20742.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_15312.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_17001.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_3030.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_16072.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_1822.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_15820.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_18252.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_2382.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_23312.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_20052.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_13831.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_18430.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_3171.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_62551.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_13770.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_13542.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_7860.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_30881.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_14531.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_50380.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_10210.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_63471.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_3290.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_8320.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_41342.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_19060.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_10151.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_42422.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_21161.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_8611.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/ILSVRC2012_val_00009651.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_3142.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_19390.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_8911.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_21730.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_18450.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_26260.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_6490.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_10121.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/ILSVRC2012_val_00021740.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_20620.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_73320.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_8161.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_21301.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_9811.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_16080.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_1980.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_9592.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_8801.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_4262.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_13582.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_8241.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_3242.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_1960.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_23571.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_15731.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_3722.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_26631.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_14181.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_38212.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_24542.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_6881.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_21032.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/ILSVRC2012_val_00017801.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_7160.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_2841.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_23971.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_19570.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_9220.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_16912.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_7620.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_28830.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_560.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_3062.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_14362.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_681.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_17272.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_18592.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_20121.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_22302.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_3441.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_1502.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_1350.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_18272.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_13702.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_32661.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_14992.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_1121.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_10790.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_19661.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_15130.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_6971.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_32422.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_22661.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_4341.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_7822.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_13672.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_1230.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_34492.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_49281.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_8112.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_4980.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_1292.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_4752.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_901.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_791.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_7002.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_5871.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_5501.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_17872.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_23272.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_15162.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_10230.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_3272.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_562.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_4382.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_17031.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_61581.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_1821.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_12122.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_931.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_33182.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_1401.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_7931.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_9662.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_6710.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_35320.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_1530.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_1262.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/ILSVRC2012_val_00047060.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_6031.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_36541.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_1100.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_10410.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_2170.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_72982.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_34632.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_29910.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_33221.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_27231.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_6081.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_7022.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_6042.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_60232.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_17862.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_3932.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_8220.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_1962.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_36380.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_72272.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_27252.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_18042.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_5680.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_621.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_35271.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_11642.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_761.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_8620.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_14502.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_34280.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_1770.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_1881.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_1842.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_7400.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_29410.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_19261.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_1300.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_731.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_19401.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_65922.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_19272.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/ILSVRC2012_val_00020502.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_1951.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_10762.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_1332.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_1850.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_58270.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_5311.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_10751.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_44580.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_9301.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_18581.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_3260.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_24502.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_14860.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_20751.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_7310.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_9440.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_27010.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_12301.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_12732.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_6882.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_2481.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_1790.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_8172.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_2930.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_1450.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_38320.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_3492.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_1271.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_13751.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_23421.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_76121.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_16081.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_4622.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_6400.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_26802.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_5592.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_910.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_11241.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/ILSVRC2012_val_00043731.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_22761.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_7160.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_11602.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_76721.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_2941.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_12370.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_11802.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_9441.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_13280.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_5402.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_40411.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_821.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_5222.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_1790.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_34132.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_16822.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_32980.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_20651.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_31181.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_1561.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_661.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_15511.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_7320.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_5861.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_11441.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_14900.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_20312.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_3972.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_9431.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_20572.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_5851.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_6552.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_1172.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_1372.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_6761.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_8052.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_731.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_13681.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_6201.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_26852.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_12140.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_12330.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_59361.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_5852.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_6770.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_33021.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_10612.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_10782.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_47472.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_5410.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_17922.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_410.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_7580.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_2241.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_13672.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_3601.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_16952.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_5890.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_10760.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_6912.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_1122.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_11120.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_652.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_17521.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_43260.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_38050.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_3200.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_31790.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_14002.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_8420.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_13601.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_5551.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_7960.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_12051.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_1781.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_27320.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_14600.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_422.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_3311.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_2221.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_1791.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_8831.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_8630.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_3540.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_8160.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_12401.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_12681.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_3161.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_39220.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_802.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_10342.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_1840.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_70.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_17060.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_108321.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_3321.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_2920.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_2162.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_3161.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_17280.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_3540.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_20232.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_7951.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_32580.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_151.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_142.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_16392.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_11561.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_1401.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_28352.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_12430.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_1631.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_6392.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_12861.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_6520.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_3371.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_5732.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_4342.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_10852.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_7581.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_4790.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_9981.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_8901.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_1022.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_7730.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_26270.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_11372.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_19282.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_630.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_7360.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_13442.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_400.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_6540.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_9481.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_48491.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_251.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_19580.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_1000.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_531.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_26892.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_1922.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_9740.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_11331.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_8641.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_9371.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_760.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_350.JPEG']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74d5ceef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_4351.JPEG']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49ff78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "/homes/gws/chanwkim/, /mmfs1/home/chanwkim/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2b8727f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|█████████████████████████████████████████████████████████████████████████████████████▎                                       | 682/1000 [00:06<00:02, 113.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|█████████████████████████████████████████████████████████████████████████████████████▎                                       | 682/1000 [00:19<00:02, 113.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       678   +    679   ->    680\n",
      "attention_rollout            678   +    678   ->    679\n",
      "attention_last               678   +    678   ->    679\n",
      "LRP                          678   +    678   ->    679\n",
      "gradcam                      678   +    678   ->    679\n",
      "gradcamgithub                678   +    678   ->    679\n",
      "vanillapixel                 678   +    678   ->    679\n",
      "vanillaembedding             678   +    678   ->    679\n",
      "sgpixel                      678   +    678   ->    679\n",
      "sgembedding                  678   +    678   ->    679\n",
      "vargradpixel                 678   +    678   ->    679\n",
      "vargradembedding             678   +    678   ->    679\n",
      "igpixel                      678   +    678   ->    679\n",
      "igembedding                  678   +    678   ->    679\n",
      "leaveoneoutclassifier        678   +    678   ->    679\n",
      "riseclassifier               678   +    678   ->    679\n",
      "ours                         678   +    678   ->    679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|█████████████████████████████████████████████████████████████████████████████████████▌                                      | 690/1000 [14:23<2:03:05, 23.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       681   +    683   ->    684\n",
      "attention_rollout            680   +    682   ->    683\n",
      "attention_last               680   +    682   ->    683\n",
      "LRP                          680   +    682   ->    683\n",
      "gradcam                      680   +    682   ->    683\n",
      "gradcamgithub                680   +    682   ->    683\n",
      "vanillapixel                 680   +    682   ->    683\n",
      "vanillaembedding             680   +    682   ->    683\n",
      "sgpixel                      680   +    682   ->    683\n",
      "sgembedding                  680   +    682   ->    683\n",
      "vargradpixel                 680   +    682   ->    683\n",
      "vargradembedding             680   +    682   ->    683\n",
      "igpixel                      680   +    682   ->    683\n",
      "igembedding                  680   +    682   ->    683\n",
      "leaveoneoutclassifier        680   +    682   ->    683\n",
      "riseclassifier               680   +    682   ->    683\n",
      "ours                         680   +    682   ->    683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|██████████████████████████████████████████████████████████████████████████████████████                                      | 694/1000 [28:34<4:13:22, 49.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       685   +    687   ->    688\n",
      "attention_rollout            684   +    686   ->    687\n",
      "attention_last               684   +    686   ->    687\n",
      "LRP                          684   +    686   ->    687\n",
      "gradcam                      684   +    686   ->    687\n",
      "gradcamgithub                684   +    686   ->    687\n",
      "vanillapixel                 684   +    686   ->    687\n",
      "vanillaembedding             684   +    686   ->    687\n",
      "sgpixel                      684   +    686   ->    687\n",
      "sgembedding                  684   +    686   ->    687\n",
      "vargradpixel                 684   +    686   ->    687\n",
      "vargradembedding             684   +    686   ->    687\n",
      "igpixel                      684   +    686   ->    687\n",
      "igembedding                  684   +    686   ->    687\n",
      "leaveoneoutclassifier        684   +    686   ->    687\n",
      "riseclassifier               684   +    686   ->    687\n",
      "ours                         684   +    686   ->    687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|██████████████████████████████████████████████████████████████████████████████████████▌                                     | 698/1000 [42:44<6:24:09, 76.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       689   +    691   ->    692\n",
      "attention_rollout            688   +    690   ->    691\n",
      "attention_last               688   +    690   ->    691\n",
      "LRP                          688   +    690   ->    691\n",
      "gradcam                      688   +    690   ->    691\n",
      "gradcamgithub                688   +    690   ->    691\n",
      "vanillapixel                 688   +    690   ->    691\n",
      "vanillaembedding             688   +    690   ->    691\n",
      "sgpixel                      688   +    690   ->    691\n",
      "sgembedding                  688   +    690   ->    691\n",
      "vargradpixel                 688   +    690   ->    691\n",
      "vargradembedding             688   +    690   ->    691\n",
      "igpixel                      688   +    690   ->    691\n",
      "igembedding                  688   +    690   ->    691\n",
      "leaveoneoutclassifier        688   +    690   ->    691\n",
      "riseclassifier               688   +    690   ->    691\n",
      "ours                         688   +    690   ->    691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|██████████████████████████████████████████████████████████████████████████████████████▎                                    | 702/1000 [56:55<8:27:28, 102.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       693   +    695   ->    696\n",
      "attention_rollout            692   +    694   ->    695\n",
      "attention_last               692   +    694   ->    695\n",
      "LRP                          692   +    694   ->    695\n",
      "gradcam                      692   +    694   ->    695\n",
      "gradcamgithub                692   +    694   ->    695\n",
      "vanillapixel                 692   +    694   ->    695\n",
      "vanillaembedding             692   +    694   ->    695\n",
      "sgpixel                      692   +    694   ->    695\n",
      "sgembedding                  692   +    694   ->    695\n",
      "vargradpixel                 692   +    694   ->    695\n",
      "vargradembedding             692   +    694   ->    695\n",
      "igpixel                      692   +    694   ->    695\n",
      "igembedding                  692   +    694   ->    695\n",
      "leaveoneoutclassifier        692   +    694   ->    695\n",
      "riseclassifier               692   +    694   ->    695\n",
      "ours                         692   +    694   ->    695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|████████████████████████████████████████████████████████████████████████████████████▋                                   | 706/1000 [1:11:10<10:17:08, 125.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       697   +    700   ->    701\n",
      "attention_rollout            696   +    699   ->    700\n",
      "attention_last               696   +    699   ->    700\n",
      "LRP                          696   +    699   ->    700\n",
      "gradcam                      696   +    699   ->    700\n",
      "gradcamgithub                696   +    699   ->    700\n",
      "vanillapixel                 696   +    699   ->    700\n",
      "vanillaembedding             696   +    699   ->    700\n",
      "sgpixel                      696   +    699   ->    700\n",
      "sgembedding                  696   +    699   ->    700\n",
      "vargradpixel                 696   +    699   ->    700\n",
      "vargradembedding             696   +    699   ->    700\n",
      "igpixel                      696   +    699   ->    700\n",
      "igembedding                  696   +    699   ->    700\n",
      "leaveoneoutclassifier        696   +    699   ->    700\n",
      "riseclassifier               696   +    699   ->    700\n",
      "ours                         696   +    699   ->    700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|█████████████████████████████████████████████████████████████████████████████████████▏                                  | 710/1000 [1:25:21<11:46:37, 146.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       702   +    704   ->    705\n",
      "attention_rollout            701   +    703   ->    704\n",
      "attention_last               701   +    703   ->    704\n",
      "LRP                          701   +    703   ->    704\n",
      "gradcam                      701   +    703   ->    704\n",
      "gradcamgithub                701   +    703   ->    704\n",
      "vanillapixel                 701   +    703   ->    704\n",
      "vanillaembedding             701   +    703   ->    704\n",
      "sgpixel                      701   +    703   ->    704\n",
      "sgembedding                  701   +    703   ->    704\n",
      "vargradpixel                 701   +    703   ->    704\n",
      "vargradembedding             701   +    703   ->    704\n",
      "igpixel                      701   +    703   ->    704\n",
      "igembedding                  701   +    703   ->    704\n",
      "leaveoneoutclassifier        701   +    703   ->    704\n",
      "riseclassifier               701   +    703   ->    704\n",
      "ours                         701   +    703   ->    704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|█████████████████████████████████████████████████████████████████████████████████████▋                                  | 714/1000 [1:39:31<12:55:58, 162.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       706   +    708   ->    709\n",
      "attention_rollout            705   +    707   ->    708\n",
      "attention_last               705   +    707   ->    708\n",
      "LRP                          705   +    707   ->    708\n",
      "gradcam                      705   +    707   ->    708\n",
      "gradcamgithub                705   +    707   ->    708\n",
      "vanillapixel                 705   +    707   ->    708\n",
      "vanillaembedding             705   +    707   ->    708\n",
      "sgpixel                      705   +    707   ->    708\n",
      "sgembedding                  705   +    707   ->    708\n",
      "vargradpixel                 705   +    707   ->    708\n",
      "vargradembedding             705   +    707   ->    708\n",
      "igpixel                      705   +    707   ->    708\n",
      "igembedding                  705   +    707   ->    708\n",
      "leaveoneoutclassifier        705   +    707   ->    708\n",
      "riseclassifier               705   +    707   ->    708\n",
      "ours                         705   +    707   ->    708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|██████████████████████████████████████████████████████████████████████████████████████▏                                 | 718/1000 [1:53:41<13:46:44, 175.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       710   +    712   ->    713\n",
      "attention_rollout            709   +    711   ->    712\n",
      "attention_last               709   +    711   ->    712\n",
      "LRP                          709   +    711   ->    712\n",
      "gradcam                      709   +    711   ->    712\n",
      "gradcamgithub                709   +    711   ->    712\n",
      "vanillapixel                 709   +    711   ->    712\n",
      "vanillaembedding             709   +    711   ->    712\n",
      "sgpixel                      709   +    711   ->    712\n",
      "sgembedding                  709   +    711   ->    712\n",
      "vargradpixel                 709   +    711   ->    712\n",
      "vargradembedding             709   +    711   ->    712\n",
      "igpixel                      709   +    711   ->    712\n",
      "igembedding                  709   +    711   ->    712\n",
      "leaveoneoutclassifier        709   +    711   ->    712\n",
      "riseclassifier               709   +    711   ->    712\n",
      "ours                         709   +    711   ->    712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|██████████████████████████████████████████████████████████████████████████████████████▋                                 | 722/1000 [2:07:53<14:21:51, 186.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       714   +    716   ->    717\n",
      "attention_rollout            713   +    715   ->    716\n",
      "attention_last               713   +    715   ->    716\n",
      "LRP                          713   +    715   ->    716\n",
      "gradcam                      713   +    715   ->    716\n",
      "gradcamgithub                713   +    715   ->    716\n",
      "vanillapixel                 713   +    715   ->    716\n",
      "vanillaembedding             713   +    715   ->    716\n",
      "sgpixel                      713   +    715   ->    716\n",
      "sgembedding                  713   +    715   ->    716\n",
      "vargradpixel                 713   +    715   ->    716\n",
      "vargradembedding             713   +    715   ->    716\n",
      "igpixel                      713   +    715   ->    716\n",
      "igembedding                  713   +    715   ->    716\n",
      "leaveoneoutclassifier        713   +    715   ->    716\n",
      "riseclassifier               713   +    715   ->    716\n",
      "ours                         713   +    715   ->    716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|███████████████████████████████████████████████████████████████████████████████████████                                 | 726/1000 [2:22:03<14:43:23, 193.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       718   +    720   ->    721\n",
      "attention_rollout            717   +    719   ->    720\n",
      "attention_last               717   +    719   ->    720\n",
      "LRP                          717   +    719   ->    720\n",
      "gradcam                      717   +    719   ->    720\n",
      "gradcamgithub                717   +    719   ->    720\n",
      "vanillapixel                 717   +    719   ->    720\n",
      "vanillaembedding             717   +    719   ->    720\n",
      "sgpixel                      717   +    719   ->    720\n",
      "sgembedding                  717   +    719   ->    720\n",
      "vargradpixel                 717   +    719   ->    720\n",
      "vargradembedding             717   +    719   ->    720\n",
      "igpixel                      717   +    719   ->    720\n",
      "igembedding                  717   +    719   ->    720\n",
      "leaveoneoutclassifier        717   +    719   ->    720\n",
      "riseclassifier               717   +    719   ->    720\n",
      "ours                         717   +    719   ->    720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|███████████████████████████████████████████████████████████████████████████████████████▌                                | 730/1000 [2:36:14<14:55:24, 198.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       722   +    724   ->    725\n",
      "attention_rollout            721   +    723   ->    724\n",
      "attention_last               721   +    723   ->    724\n",
      "LRP                          721   +    723   ->    724\n",
      "gradcam                      721   +    723   ->    724\n",
      "gradcamgithub                721   +    723   ->    724\n",
      "vanillapixel                 721   +    723   ->    724\n",
      "vanillaembedding             721   +    723   ->    724\n",
      "sgpixel                      721   +    723   ->    724\n",
      "sgembedding                  721   +    723   ->    724\n",
      "vargradpixel                 721   +    723   ->    724\n",
      "vargradembedding             721   +    723   ->    724\n",
      "igpixel                      721   +    723   ->    724\n",
      "igembedding                  721   +    723   ->    724\n",
      "leaveoneoutclassifier        721   +    723   ->    724\n",
      "riseclassifier               721   +    723   ->    724\n",
      "ours                         721   +    723   ->    724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|████████████████████████████████████████████████████████████████████████████████████████                                | 734/1000 [2:50:26<15:00:04, 203.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       726   +    728   ->    729\n",
      "attention_rollout            725   +    727   ->    728\n",
      "attention_last               725   +    727   ->    728\n",
      "LRP                          725   +    727   ->    728\n",
      "gradcam                      725   +    727   ->    728\n",
      "gradcamgithub                725   +    727   ->    728\n",
      "vanillapixel                 725   +    727   ->    728\n",
      "vanillaembedding             725   +    727   ->    728\n",
      "sgpixel                      725   +    727   ->    728\n",
      "sgembedding                  725   +    727   ->    728\n",
      "vargradpixel                 725   +    727   ->    728\n",
      "vargradembedding             725   +    727   ->    728\n",
      "igpixel                      725   +    727   ->    728\n",
      "igembedding                  725   +    727   ->    728\n",
      "leaveoneoutclassifier        725   +    727   ->    728\n",
      "riseclassifier               725   +    727   ->    728\n",
      "ours                         725   +    727   ->    728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|████████████████████████████████████████████████████████████████████████████████████████▌                               | 738/1000 [3:04:40<15:00:00, 206.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       730   +    732   ->    733\n",
      "attention_rollout            729   +    731   ->    732\n",
      "attention_last               729   +    731   ->    732\n",
      "LRP                          729   +    731   ->    732\n",
      "gradcam                      729   +    731   ->    732\n",
      "gradcamgithub                729   +    731   ->    732\n",
      "vanillapixel                 729   +    731   ->    732\n",
      "vanillaembedding             729   +    731   ->    732\n",
      "sgpixel                      729   +    731   ->    732\n",
      "sgembedding                  729   +    731   ->    732\n",
      "vargradpixel                 729   +    731   ->    732\n",
      "vargradembedding             729   +    731   ->    732\n",
      "igpixel                      729   +    731   ->    732\n",
      "igembedding                  729   +    731   ->    732\n",
      "leaveoneoutclassifier        729   +    731   ->    732\n",
      "riseclassifier               729   +    731   ->    732\n",
      "ours                         729   +    731   ->    732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|█████████████████████████████████████████████████████████████████████████████████████████                               | 742/1000 [3:18:51<14:54:50, 208.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       734   +    736   ->    737\n",
      "attention_rollout            733   +    735   ->    736\n",
      "attention_last               733   +    735   ->    736\n",
      "LRP                          733   +    735   ->    736\n",
      "gradcam                      733   +    735   ->    736\n",
      "gradcamgithub                733   +    735   ->    736\n",
      "vanillapixel                 733   +    735   ->    736\n",
      "vanillaembedding             733   +    735   ->    736\n",
      "sgpixel                      733   +    735   ->    736\n",
      "sgembedding                  733   +    735   ->    736\n",
      "vargradpixel                 733   +    735   ->    736\n",
      "vargradembedding             733   +    735   ->    736\n",
      "igpixel                      733   +    735   ->    736\n",
      "igembedding                  733   +    735   ->    736\n",
      "leaveoneoutclassifier        733   +    735   ->    736\n",
      "riseclassifier               733   +    735   ->    736\n",
      "ours                         733   +    735   ->    736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|█████████████████████████████████████████████████████████████████████████████████████████▌                              | 746/1000 [3:33:02<14:46:44, 209.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       738   +    740   ->    741\n",
      "attention_rollout            737   +    739   ->    740\n",
      "attention_last               737   +    739   ->    740\n",
      "LRP                          737   +    739   ->    740\n",
      "gradcam                      737   +    739   ->    740\n",
      "gradcamgithub                737   +    739   ->    740\n",
      "vanillapixel                 737   +    739   ->    740\n",
      "vanillaembedding             737   +    739   ->    740\n",
      "sgpixel                      737   +    739   ->    740\n",
      "sgembedding                  737   +    739   ->    740\n",
      "vargradpixel                 737   +    739   ->    740\n",
      "vargradembedding             737   +    739   ->    740\n",
      "igpixel                      737   +    739   ->    740\n",
      "igembedding                  737   +    739   ->    740\n",
      "leaveoneoutclassifier        737   +    739   ->    740\n",
      "riseclassifier               737   +    739   ->    740\n",
      "ours                         737   +    739   ->    740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|██████████████████████████████████████████████████████████████████████████████████████████                              | 750/1000 [3:47:12<14:36:34, 210.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       742   +    744   ->    745\n",
      "attention_rollout            741   +    743   ->    744\n",
      "attention_last               741   +    743   ->    744\n",
      "LRP                          741   +    743   ->    744\n",
      "gradcam                      741   +    743   ->    744\n",
      "gradcamgithub                741   +    743   ->    744\n",
      "vanillapixel                 741   +    743   ->    744\n",
      "vanillaembedding             741   +    743   ->    744\n",
      "sgpixel                      741   +    743   ->    744\n",
      "sgembedding                  741   +    743   ->    744\n",
      "vargradpixel                 741   +    743   ->    744\n",
      "vargradembedding             741   +    743   ->    744\n",
      "igpixel                      741   +    743   ->    744\n",
      "igembedding                  741   +    743   ->    744\n",
      "leaveoneoutclassifier        741   +    743   ->    744\n",
      "riseclassifier               741   +    743   ->    744\n",
      "ours                         741   +    743   ->    744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|██████████████████████████████████████████████████████████████████████████████████████████▍                             | 754/1000 [4:01:25<14:26:05, 211.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       746   +    748   ->    749\n",
      "attention_rollout            745   +    747   ->    748\n",
      "attention_last               745   +    747   ->    748\n",
      "LRP                          745   +    747   ->    748\n",
      "gradcam                      745   +    747   ->    748\n",
      "gradcamgithub                745   +    747   ->    748\n",
      "vanillapixel                 745   +    747   ->    748\n",
      "vanillaembedding             745   +    747   ->    748\n",
      "sgpixel                      745   +    747   ->    748\n",
      "sgembedding                  745   +    747   ->    748\n",
      "vargradpixel                 745   +    747   ->    748\n",
      "vargradembedding             745   +    747   ->    748\n",
      "igpixel                      745   +    747   ->    748\n",
      "igembedding                  745   +    747   ->    748\n",
      "leaveoneoutclassifier        745   +    747   ->    748\n",
      "riseclassifier               745   +    747   ->    748\n",
      "ours                         745   +    747   ->    748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|██████████████████████████████████████████████████████████████████████████████████████████▉                             | 758/1000 [4:15:38<14:14:23, 211.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       750   +    753   ->    754\n",
      "attention_rollout            749   +    752   ->    753\n",
      "attention_last               749   +    752   ->    753\n",
      "LRP                          749   +    752   ->    753\n",
      "gradcam                      749   +    752   ->    753\n",
      "gradcamgithub                749   +    752   ->    753\n",
      "vanillapixel                 749   +    752   ->    753\n",
      "vanillaembedding             749   +    752   ->    753\n",
      "sgpixel                      749   +    752   ->    753\n",
      "sgembedding                  749   +    752   ->    753\n",
      "vargradpixel                 749   +    752   ->    753\n",
      "vargradembedding             749   +    752   ->    753\n",
      "igpixel                      749   +    752   ->    753\n",
      "igembedding                  749   +    752   ->    753\n",
      "leaveoneoutclassifier        749   +    752   ->    753\n",
      "riseclassifier               749   +    752   ->    753\n",
      "ours                         749   +    752   ->    753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|███████████████████████████████████████████████████████████████████████████████████████████▍                            | 762/1000 [4:29:51<14:01:53, 212.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       755   +    757   ->    758\n",
      "attention_rollout            754   +    756   ->    757\n",
      "attention_last               754   +    756   ->    757\n",
      "LRP                          754   +    756   ->    757\n",
      "gradcam                      754   +    756   ->    757\n",
      "gradcamgithub                754   +    756   ->    757\n",
      "vanillapixel                 754   +    756   ->    757\n",
      "vanillaembedding             754   +    756   ->    757\n",
      "sgpixel                      754   +    756   ->    757\n",
      "sgembedding                  754   +    756   ->    757\n",
      "vargradpixel                 754   +    756   ->    757\n",
      "vargradembedding             754   +    756   ->    757\n",
      "igpixel                      754   +    756   ->    757\n",
      "igembedding                  754   +    756   ->    757\n",
      "leaveoneoutclassifier        754   +    756   ->    757\n",
      "riseclassifier               754   +    756   ->    757\n",
      "ours                         754   +    756   ->    757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|███████████████████████████████████████████████████████████████████████████████████████████▉                            | 766/1000 [4:44:01<13:48:01, 212.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       759   +    761   ->    762\n",
      "attention_rollout            758   +    760   ->    761\n",
      "attention_last               758   +    760   ->    761\n",
      "LRP                          758   +    760   ->    761\n",
      "gradcam                      758   +    760   ->    761\n",
      "gradcamgithub                758   +    760   ->    761\n",
      "vanillapixel                 758   +    760   ->    761\n",
      "vanillaembedding             758   +    760   ->    761\n",
      "sgpixel                      758   +    760   ->    761\n",
      "sgembedding                  758   +    760   ->    761\n",
      "vargradpixel                 758   +    760   ->    761\n",
      "vargradembedding             758   +    760   ->    761\n",
      "igpixel                      758   +    760   ->    761\n",
      "igembedding                  758   +    760   ->    761\n",
      "leaveoneoutclassifier        758   +    760   ->    761\n",
      "riseclassifier               758   +    760   ->    761\n",
      "ours                         758   +    760   ->    761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|████████████████████████████████████████████████████████████████████████████████████████████▍                           | 770/1000 [4:58:13<13:34:44, 212.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       763   +    765   ->    766\n",
      "attention_rollout            762   +    764   ->    765\n",
      "attention_last               762   +    764   ->    765\n",
      "LRP                          762   +    764   ->    765\n",
      "gradcam                      762   +    764   ->    765\n",
      "gradcamgithub                762   +    764   ->    765\n",
      "vanillapixel                 762   +    764   ->    765\n",
      "vanillaembedding             762   +    764   ->    765\n",
      "sgpixel                      762   +    764   ->    765\n",
      "sgembedding                  762   +    764   ->    765\n",
      "vargradpixel                 762   +    764   ->    765\n",
      "vargradembedding             762   +    764   ->    765\n",
      "igpixel                      762   +    764   ->    765\n",
      "igembedding                  762   +    764   ->    765\n",
      "leaveoneoutclassifier        762   +    764   ->    765\n",
      "riseclassifier               762   +    764   ->    765\n",
      "ours                         762   +    764   ->    765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|████████████████████████████████████████████████████████████████████████████████████████████▉                           | 774/1000 [5:12:23<13:20:34, 212.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       767   +    769   ->    770\n",
      "attention_rollout            766   +    768   ->    769\n",
      "attention_last               766   +    768   ->    769\n",
      "LRP                          766   +    768   ->    769\n",
      "gradcam                      766   +    768   ->    769\n",
      "gradcamgithub                766   +    768   ->    769\n",
      "vanillapixel                 766   +    768   ->    769\n",
      "vanillaembedding             766   +    768   ->    769\n",
      "sgpixel                      766   +    768   ->    769\n",
      "sgembedding                  766   +    768   ->    769\n",
      "vargradpixel                 766   +    768   ->    769\n",
      "vargradembedding             766   +    768   ->    769\n",
      "igpixel                      766   +    768   ->    769\n",
      "igembedding                  766   +    768   ->    769\n",
      "leaveoneoutclassifier        766   +    768   ->    769\n",
      "riseclassifier               766   +    768   ->    769\n",
      "ours                         766   +    768   ->    769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|█████████████████████████████████████████████████████████████████████████████████████████████▎                          | 778/1000 [5:26:34<13:06:34, 212.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       771   +    773   ->    774\n",
      "attention_rollout            770   +    772   ->    773\n",
      "attention_last               770   +    772   ->    773\n",
      "LRP                          770   +    772   ->    773\n",
      "gradcam                      770   +    772   ->    773\n",
      "gradcamgithub                770   +    772   ->    773\n",
      "vanillapixel                 770   +    772   ->    773\n",
      "vanillaembedding             770   +    772   ->    773\n",
      "sgpixel                      770   +    772   ->    773\n",
      "sgembedding                  770   +    772   ->    773\n",
      "vargradpixel                 770   +    772   ->    773\n",
      "vargradembedding             770   +    772   ->    773\n",
      "igpixel                      770   +    772   ->    773\n",
      "igembedding                  770   +    772   ->    773\n",
      "leaveoneoutclassifier        770   +    772   ->    773\n",
      "riseclassifier               770   +    772   ->    773\n",
      "ours                         770   +    772   ->    773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|█████████████████████████████████████████████████████████████████████████████████████████████▊                          | 782/1000 [5:40:44<12:52:23, 212.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       775   +    777   ->    778\n",
      "attention_rollout            774   +    776   ->    777\n",
      "attention_last               774   +    776   ->    777\n",
      "LRP                          774   +    776   ->    777\n",
      "gradcam                      774   +    776   ->    777\n",
      "gradcamgithub                774   +    776   ->    777\n",
      "vanillapixel                 774   +    776   ->    777\n",
      "vanillaembedding             774   +    776   ->    777\n",
      "sgpixel                      774   +    776   ->    777\n",
      "sgembedding                  774   +    776   ->    777\n",
      "vargradpixel                 774   +    776   ->    777\n",
      "vargradembedding             774   +    776   ->    777\n",
      "igpixel                      774   +    776   ->    777\n",
      "igembedding                  774   +    776   ->    777\n",
      "leaveoneoutclassifier        774   +    776   ->    777\n",
      "riseclassifier               774   +    776   ->    777\n",
      "ours                         774   +    776   ->    777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|██████████████████████████████████████████████████████████████████████████████████████████████▎                         | 786/1000 [5:54:56<12:38:39, 212.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       779   +    781   ->    782\n",
      "attention_rollout            778   +    780   ->    781\n",
      "attention_last               778   +    780   ->    781\n",
      "LRP                          778   +    780   ->    781\n",
      "gradcam                      778   +    780   ->    781\n",
      "gradcamgithub                778   +    780   ->    781\n",
      "vanillapixel                 778   +    780   ->    781\n",
      "vanillaembedding             778   +    780   ->    781\n",
      "sgpixel                      778   +    780   ->    781\n",
      "sgembedding                  778   +    780   ->    781\n",
      "vargradpixel                 778   +    780   ->    781\n",
      "vargradembedding             778   +    780   ->    781\n",
      "igpixel                      778   +    780   ->    781\n",
      "igembedding                  778   +    780   ->    781\n",
      "leaveoneoutclassifier        778   +    780   ->    781\n",
      "riseclassifier               778   +    780   ->    781\n",
      "ours                         778   +    780   ->    781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|██████████████████████████████████████████████████████████████████████████████████████████████▊                         | 790/1000 [6:09:09<12:24:56, 212.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       783   +    785   ->    786\n",
      "attention_rollout            782   +    784   ->    785\n",
      "attention_last               782   +    784   ->    785\n",
      "LRP                          782   +    784   ->    785\n",
      "gradcam                      782   +    784   ->    785\n",
      "gradcamgithub                782   +    784   ->    785\n",
      "vanillapixel                 782   +    784   ->    785\n",
      "vanillaembedding             782   +    784   ->    785\n",
      "sgpixel                      782   +    784   ->    785\n",
      "sgembedding                  782   +    784   ->    785\n",
      "vargradpixel                 782   +    784   ->    785\n",
      "vargradembedding             782   +    784   ->    785\n",
      "igpixel                      782   +    784   ->    785\n",
      "igembedding                  782   +    784   ->    785\n",
      "leaveoneoutclassifier        782   +    784   ->    785\n",
      "riseclassifier               782   +    784   ->    785\n",
      "ours                         782   +    784   ->    785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|███████████████████████████████████████████████████████████████████████████████████████████████▎                        | 794/1000 [6:23:20<12:10:43, 212.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       787   +    789   ->    790\n",
      "attention_rollout            786   +    788   ->    789\n",
      "attention_last               786   +    788   ->    789\n",
      "LRP                          786   +    788   ->    789\n",
      "gradcam                      786   +    788   ->    789\n",
      "gradcamgithub                786   +    788   ->    789\n",
      "vanillapixel                 786   +    788   ->    789\n",
      "vanillaembedding             786   +    788   ->    789\n",
      "sgpixel                      786   +    788   ->    789\n",
      "sgembedding                  786   +    788   ->    789\n",
      "vargradpixel                 786   +    788   ->    789\n",
      "vargradembedding             786   +    788   ->    789\n",
      "igpixel                      786   +    788   ->    789\n",
      "igembedding                  786   +    788   ->    789\n",
      "leaveoneoutclassifier        786   +    788   ->    789\n",
      "riseclassifier               786   +    788   ->    789\n",
      "ours                         786   +    788   ->    789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|███████████████████████████████████████████████████████████████████████████████████████████████▊                        | 798/1000 [6:37:31<11:56:28, 212.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       791   +    793   ->    794\n",
      "attention_rollout            790   +    792   ->    793\n",
      "attention_last               790   +    792   ->    793\n",
      "LRP                          790   +    792   ->    793\n",
      "gradcam                      790   +    792   ->    793\n",
      "gradcamgithub                790   +    792   ->    793\n",
      "vanillapixel                 790   +    792   ->    793\n",
      "vanillaembedding             790   +    792   ->    793\n",
      "sgpixel                      790   +    792   ->    793\n",
      "sgembedding                  790   +    792   ->    793\n",
      "vargradpixel                 790   +    792   ->    793\n",
      "vargradembedding             790   +    792   ->    793\n",
      "igpixel                      790   +    792   ->    793\n",
      "igembedding                  790   +    792   ->    793\n",
      "leaveoneoutclassifier        790   +    792   ->    793\n",
      "riseclassifier               790   +    792   ->    793\n",
      "ours                         790   +    792   ->    793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████████████████████████████████████████████████████████████████████████████████████████████▏                       | 802/1000 [6:51:42<11:42:10, 212.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       795   +    797   ->    798\n",
      "attention_rollout            794   +    796   ->    797\n",
      "attention_last               794   +    796   ->    797\n",
      "LRP                          794   +    796   ->    797\n",
      "gradcam                      794   +    796   ->    797\n",
      "gradcamgithub                794   +    796   ->    797\n",
      "vanillapixel                 794   +    796   ->    797\n",
      "vanillaembedding             794   +    796   ->    797\n",
      "sgpixel                      794   +    796   ->    797\n",
      "sgembedding                  794   +    796   ->    797\n",
      "vargradpixel                 794   +    796   ->    797\n",
      "vargradembedding             794   +    796   ->    797\n",
      "igpixel                      794   +    796   ->    797\n",
      "igembedding                  794   +    796   ->    797\n",
      "leaveoneoutclassifier        794   +    796   ->    797\n",
      "riseclassifier               794   +    796   ->    797\n",
      "ours                         794   +    796   ->    797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|████████████████████████████████████████████████████████████████████████████████████████████████▋                       | 806/1000 [7:05:53<11:27:56, 212.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       799   +    801   ->    802\n",
      "attention_rollout            798   +    800   ->    801\n",
      "attention_last               798   +    800   ->    801\n",
      "LRP                          798   +    800   ->    801\n",
      "gradcam                      798   +    800   ->    801\n",
      "gradcamgithub                798   +    800   ->    801\n",
      "vanillapixel                 798   +    800   ->    801\n",
      "vanillaembedding             798   +    800   ->    801\n",
      "sgpixel                      798   +    800   ->    801\n",
      "sgembedding                  798   +    800   ->    801\n",
      "vargradpixel                 798   +    800   ->    801\n",
      "vargradembedding             798   +    800   ->    801\n",
      "igpixel                      798   +    800   ->    801\n",
      "igembedding                  798   +    800   ->    801\n",
      "leaveoneoutclassifier        798   +    800   ->    801\n",
      "riseclassifier               798   +    800   ->    801\n",
      "ours                         798   +    800   ->    801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|█████████████████████████████████████████████████████████████████████████████████████████████████▏                      | 810/1000 [7:20:04<11:13:39, 212.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       803   +    805   ->    806\n",
      "attention_rollout            802   +    804   ->    805\n",
      "attention_last               802   +    804   ->    805\n",
      "LRP                          802   +    804   ->    805\n",
      "gradcam                      802   +    804   ->    805\n",
      "gradcamgithub                802   +    804   ->    805\n",
      "vanillapixel                 802   +    804   ->    805\n",
      "vanillaembedding             802   +    804   ->    805\n",
      "sgpixel                      802   +    804   ->    805\n",
      "sgembedding                  802   +    804   ->    805\n",
      "vargradpixel                 802   +    804   ->    805\n",
      "vargradembedding             802   +    804   ->    805\n",
      "igpixel                      802   +    804   ->    805\n",
      "igembedding                  802   +    804   ->    805\n",
      "leaveoneoutclassifier        802   +    804   ->    805\n",
      "riseclassifier               802   +    804   ->    805\n",
      "ours                         802   +    804   ->    805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|█████████████████████████████████████████████████████████████████████████████████████████████████▋                      | 814/1000 [7:34:15<10:59:27, 212.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       807   +    809   ->    810\n",
      "attention_rollout            806   +    808   ->    809\n",
      "attention_last               806   +    808   ->    809\n",
      "LRP                          806   +    808   ->    809\n",
      "gradcam                      806   +    808   ->    809\n",
      "gradcamgithub                806   +    808   ->    809\n",
      "vanillapixel                 806   +    808   ->    809\n",
      "vanillaembedding             806   +    808   ->    809\n",
      "sgpixel                      806   +    808   ->    809\n",
      "sgembedding                  806   +    808   ->    809\n",
      "vargradpixel                 806   +    808   ->    809\n",
      "vargradembedding             806   +    808   ->    809\n",
      "igpixel                      806   +    808   ->    809\n",
      "igembedding                  806   +    808   ->    809\n",
      "leaveoneoutclassifier        806   +    808   ->    809\n",
      "riseclassifier               806   +    808   ->    809\n",
      "ours                         806   +    808   ->    809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|██████████████████████████████████████████████████████████████████████████████████████████████████▏                     | 818/1000 [7:48:26<10:45:21, 212.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       811   +    813   ->    814\n",
      "attention_rollout            810   +    812   ->    813\n",
      "attention_last               810   +    812   ->    813\n",
      "LRP                          810   +    812   ->    813\n",
      "gradcam                      810   +    812   ->    813\n",
      "gradcamgithub                810   +    812   ->    813\n",
      "vanillapixel                 810   +    812   ->    813\n",
      "vanillaembedding             810   +    812   ->    813\n",
      "sgpixel                      810   +    812   ->    813\n",
      "sgembedding                  810   +    812   ->    813\n",
      "vargradpixel                 810   +    812   ->    813\n",
      "vargradembedding             810   +    812   ->    813\n",
      "igpixel                      810   +    812   ->    813\n",
      "igembedding                  810   +    812   ->    813\n",
      "leaveoneoutclassifier        810   +    812   ->    813\n",
      "riseclassifier               810   +    812   ->    813\n",
      "ours                         810   +    812   ->    813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|██████████████████████████████████████████████████████████████████████████████████████████████████▋                     | 822/1000 [8:02:36<10:31:00, 212.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       815   +    817   ->    818\n",
      "attention_rollout            814   +    816   ->    817\n",
      "attention_last               814   +    816   ->    817\n",
      "LRP                          814   +    816   ->    817\n",
      "gradcam                      814   +    816   ->    817\n",
      "gradcamgithub                814   +    816   ->    817\n",
      "vanillapixel                 814   +    816   ->    817\n",
      "vanillaembedding             814   +    816   ->    817\n",
      "sgpixel                      814   +    816   ->    817\n",
      "sgembedding                  814   +    816   ->    817\n",
      "vargradpixel                 814   +    816   ->    817\n",
      "vargradembedding             814   +    816   ->    817\n",
      "igpixel                      814   +    816   ->    817\n",
      "igembedding                  814   +    816   ->    817\n",
      "leaveoneoutclassifier        814   +    816   ->    817\n",
      "riseclassifier               814   +    816   ->    817\n",
      "ours                         814   +    816   ->    817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|███████████████████████████████████████████████████████████████████████████████████████████████████                     | 826/1000 [8:16:48<10:16:59, 212.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       819   +    821   ->    822\n",
      "attention_rollout            818   +    820   ->    821\n",
      "attention_last               818   +    820   ->    821\n",
      "LRP                          818   +    820   ->    821\n",
      "gradcam                      818   +    820   ->    821\n",
      "gradcamgithub                818   +    820   ->    821\n",
      "vanillapixel                 818   +    820   ->    821\n",
      "vanillaembedding             818   +    820   ->    821\n",
      "sgpixel                      818   +    820   ->    821\n",
      "sgembedding                  818   +    820   ->    821\n",
      "vargradpixel                 818   +    820   ->    821\n",
      "vargradembedding             818   +    820   ->    821\n",
      "igpixel                      818   +    820   ->    821\n",
      "igembedding                  818   +    820   ->    821\n",
      "leaveoneoutclassifier        818   +    820   ->    821\n",
      "riseclassifier               818   +    820   ->    821\n",
      "ours                         818   +    820   ->    821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|███████████████████████████████████████████████████████████████████████████████████████████████████▌                    | 830/1000 [8:30:59<10:02:51, 212.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       823   +    825   ->    826\n",
      "attention_rollout            822   +    824   ->    825\n",
      "attention_last               822   +    824   ->    825\n",
      "LRP                          822   +    824   ->    825\n",
      "gradcam                      822   +    824   ->    825\n",
      "gradcamgithub                822   +    824   ->    825\n",
      "vanillapixel                 822   +    824   ->    825\n",
      "vanillaembedding             822   +    824   ->    825\n",
      "sgpixel                      822   +    824   ->    825\n",
      "sgembedding                  822   +    824   ->    825\n",
      "vargradpixel                 822   +    824   ->    825\n",
      "vargradembedding             822   +    824   ->    825\n",
      "igpixel                      822   +    824   ->    825\n",
      "igembedding                  822   +    824   ->    825\n",
      "leaveoneoutclassifier        822   +    824   ->    825\n",
      "riseclassifier               822   +    824   ->    825\n",
      "ours                         822   +    824   ->    825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|████████████████████████████████████████████████████████████████████████████████████████████████████▉                    | 834/1000 [8:45:10<9:48:38, 212.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       827   +    829   ->    830\n",
      "attention_rollout            826   +    828   ->    829\n",
      "attention_last               826   +    828   ->    829\n",
      "LRP                          826   +    828   ->    829\n",
      "gradcam                      826   +    828   ->    829\n",
      "gradcamgithub                826   +    828   ->    829\n",
      "vanillapixel                 826   +    828   ->    829\n",
      "vanillaembedding             826   +    828   ->    829\n",
      "sgpixel                      826   +    828   ->    829\n",
      "sgembedding                  826   +    828   ->    829\n",
      "vargradpixel                 826   +    828   ->    829\n",
      "vargradembedding             826   +    828   ->    829\n",
      "igpixel                      826   +    828   ->    829\n",
      "igembedding                  826   +    828   ->    829\n",
      "leaveoneoutclassifier        826   +    828   ->    829\n",
      "riseclassifier               826   +    828   ->    829\n",
      "ours                         826   +    828   ->    829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|█████████████████████████████████████████████████████████████████████████████████████████████████████▍                   | 838/1000 [8:59:23<9:34:48, 212.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       831   +    833   ->    834\n",
      "attention_rollout            830   +    832   ->    833\n",
      "attention_last               830   +    832   ->    833\n",
      "LRP                          830   +    832   ->    833\n",
      "gradcam                      830   +    832   ->    833\n",
      "gradcamgithub                830   +    832   ->    833\n",
      "vanillapixel                 830   +    832   ->    833\n",
      "vanillaembedding             830   +    832   ->    833\n",
      "sgpixel                      830   +    832   ->    833\n",
      "sgembedding                  830   +    832   ->    833\n",
      "vargradpixel                 830   +    832   ->    833\n",
      "vargradembedding             830   +    832   ->    833\n",
      "igpixel                      830   +    832   ->    833\n",
      "igembedding                  830   +    832   ->    833\n",
      "leaveoneoutclassifier        830   +    832   ->    833\n",
      "riseclassifier               830   +    832   ->    833\n",
      "ours                         830   +    832   ->    833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|█████████████████████████████████████████████████████████████████████████████████████████████████████▉                   | 842/1000 [9:13:34<9:20:31, 212.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       835   +    837   ->    838\n",
      "attention_rollout            834   +    836   ->    837\n",
      "attention_last               834   +    836   ->    837\n",
      "LRP                          834   +    836   ->    837\n",
      "gradcam                      834   +    836   ->    837\n",
      "gradcamgithub                834   +    836   ->    837\n",
      "vanillapixel                 834   +    836   ->    837\n",
      "vanillaembedding             834   +    836   ->    837\n",
      "sgpixel                      834   +    836   ->    837\n",
      "sgembedding                  834   +    836   ->    837\n",
      "vargradpixel                 834   +    836   ->    837\n",
      "vargradembedding             834   +    836   ->    837\n",
      "igpixel                      834   +    836   ->    837\n",
      "igembedding                  834   +    836   ->    837\n",
      "leaveoneoutclassifier        834   +    836   ->    837\n",
      "riseclassifier               834   +    836   ->    837\n",
      "ours                         834   +    836   ->    837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|██████████████████████████████████████████████████████████████████████████████████████████████████████▎                  | 846/1000 [9:27:45<9:06:16, 212.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       839   +    841   ->    842\n",
      "attention_rollout            838   +    840   ->    841\n",
      "attention_last               838   +    840   ->    841\n",
      "LRP                          838   +    840   ->    841\n",
      "gradcam                      838   +    840   ->    841\n",
      "gradcamgithub                838   +    840   ->    841\n",
      "vanillapixel                 838   +    840   ->    841\n",
      "vanillaembedding             838   +    840   ->    841\n",
      "sgpixel                      838   +    840   ->    841\n",
      "sgembedding                  838   +    840   ->    841\n",
      "vargradpixel                 838   +    840   ->    841\n",
      "vargradembedding             838   +    840   ->    841\n",
      "igpixel                      838   +    840   ->    841\n",
      "igembedding                  838   +    840   ->    841\n",
      "leaveoneoutclassifier        838   +    840   ->    841\n",
      "riseclassifier               838   +    840   ->    841\n",
      "ours                         838   +    840   ->    841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|██████████████████████████████████████████████████████████████████████████████████████████████████████▊                  | 850/1000 [9:41:57<8:52:12, 212.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       843   +    846   ->    847\n",
      "attention_rollout            842   +    845   ->    846\n",
      "attention_last               842   +    845   ->    846\n",
      "LRP                          842   +    845   ->    846\n",
      "gradcam                      842   +    845   ->    846\n",
      "gradcamgithub                842   +    845   ->    846\n",
      "vanillapixel                 842   +    845   ->    846\n",
      "vanillaembedding             842   +    845   ->    846\n",
      "sgpixel                      842   +    845   ->    846\n",
      "sgembedding                  842   +    845   ->    846\n",
      "vargradpixel                 842   +    845   ->    846\n",
      "vargradembedding             842   +    845   ->    846\n",
      "igpixel                      842   +    845   ->    846\n",
      "igembedding                  842   +    845   ->    846\n",
      "leaveoneoutclassifier        842   +    845   ->    846\n",
      "riseclassifier               842   +    845   ->    846\n",
      "ours                         842   +    845   ->    846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|███████████████████████████████████████████████████████████████████████████████████████████████████████▎                 | 854/1000 [9:56:08<8:37:57, 212.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       848   +    850   ->    851\n",
      "attention_rollout            847   +    849   ->    850\n",
      "attention_last               847   +    849   ->    850\n",
      "LRP                          847   +    849   ->    850\n",
      "gradcam                      847   +    849   ->    850\n",
      "gradcamgithub                847   +    849   ->    850\n",
      "vanillapixel                 847   +    849   ->    850\n",
      "vanillaembedding             847   +    849   ->    850\n",
      "sgpixel                      847   +    849   ->    850\n",
      "sgembedding                  847   +    849   ->    850\n",
      "vargradpixel                 847   +    849   ->    850\n",
      "vargradembedding             847   +    849   ->    850\n",
      "igpixel                      847   +    849   ->    850\n",
      "igembedding                  847   +    849   ->    850\n",
      "leaveoneoutclassifier        847   +    849   ->    850\n",
      "riseclassifier               847   +    849   ->    850\n",
      "ours                         847   +    849   ->    850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|██████████████████████████████████████████████████████████████████████████████████████████████████████▉                 | 858/1000 [10:10:19<8:23:37, 212.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       852   +    854   ->    855\n",
      "attention_rollout            851   +    853   ->    854\n",
      "attention_last               851   +    853   ->    854\n",
      "LRP                          851   +    853   ->    854\n",
      "gradcam                      851   +    853   ->    854\n",
      "gradcamgithub                851   +    853   ->    854\n",
      "vanillapixel                 851   +    853   ->    854\n",
      "vanillaembedding             851   +    853   ->    854\n",
      "sgpixel                      851   +    853   ->    854\n",
      "sgembedding                  851   +    853   ->    854\n",
      "vargradpixel                 851   +    853   ->    854\n",
      "vargradembedding             851   +    853   ->    854\n",
      "igpixel                      851   +    853   ->    854\n",
      "igembedding                  851   +    853   ->    854\n",
      "leaveoneoutclassifier        851   +    853   ->    854\n",
      "riseclassifier               851   +    853   ->    854\n",
      "ours                         851   +    853   ->    854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|███████████████████████████████████████████████████████████████████████████████████████████████████████▍                | 862/1000 [10:24:29<8:09:12, 212.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       856   +    858   ->    859\n",
      "attention_rollout            855   +    857   ->    858\n",
      "attention_last               855   +    857   ->    858\n",
      "LRP                          855   +    857   ->    858\n",
      "gradcam                      855   +    857   ->    858\n",
      "gradcamgithub                855   +    857   ->    858\n",
      "vanillapixel                 855   +    857   ->    858\n",
      "vanillaembedding             855   +    857   ->    858\n",
      "sgpixel                      855   +    857   ->    858\n",
      "sgembedding                  855   +    857   ->    858\n",
      "vargradpixel                 855   +    857   ->    858\n",
      "vargradembedding             855   +    857   ->    858\n",
      "igpixel                      855   +    857   ->    858\n",
      "igembedding                  855   +    857   ->    858\n",
      "leaveoneoutclassifier        855   +    857   ->    858\n",
      "riseclassifier               855   +    857   ->    858\n",
      "ours                         855   +    857   ->    858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|███████████████████████████████████████████████████████████████████████████████████████████████████████▉                | 866/1000 [10:38:40<7:55:04, 212.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       860   +    862   ->    863\n",
      "attention_rollout            859   +    861   ->    862\n",
      "attention_last               859   +    861   ->    862\n",
      "LRP                          859   +    861   ->    862\n",
      "gradcam                      859   +    861   ->    862\n",
      "gradcamgithub                859   +    861   ->    862\n",
      "vanillapixel                 859   +    861   ->    862\n",
      "vanillaembedding             859   +    861   ->    862\n",
      "sgpixel                      859   +    861   ->    862\n",
      "sgembedding                  859   +    861   ->    862\n",
      "vargradpixel                 859   +    861   ->    862\n",
      "vargradembedding             859   +    861   ->    862\n",
      "igpixel                      859   +    861   ->    862\n",
      "igembedding                  859   +    861   ->    862\n",
      "leaveoneoutclassifier        859   +    861   ->    862\n",
      "riseclassifier               859   +    861   ->    862\n",
      "ours                         859   +    861   ->    862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|████████████████████████████████████████████████████████████████████████████████████████████████████████▍               | 870/1000 [10:52:52<7:41:03, 212.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       864   +    866   ->    867\n",
      "attention_rollout            863   +    865   ->    866\n",
      "attention_last               863   +    865   ->    866\n",
      "LRP                          863   +    865   ->    866\n",
      "gradcam                      863   +    865   ->    866\n",
      "gradcamgithub                863   +    865   ->    866\n",
      "vanillapixel                 863   +    865   ->    866\n",
      "vanillaembedding             863   +    865   ->    866\n",
      "sgpixel                      863   +    865   ->    866\n",
      "sgembedding                  863   +    865   ->    866\n",
      "vargradpixel                 863   +    865   ->    866\n",
      "vargradembedding             863   +    865   ->    866\n",
      "igpixel                      863   +    865   ->    866\n",
      "igembedding                  863   +    865   ->    866\n",
      "leaveoneoutclassifier        863   +    865   ->    866\n",
      "riseclassifier               863   +    865   ->    866\n",
      "ours                         863   +    865   ->    866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|████████████████████████████████████████████████████████████████████████████████████████████████████████▉               | 874/1000 [11:07:01<7:26:36, 212.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       868   +    870   ->    871\n",
      "attention_rollout            867   +    869   ->    870\n",
      "attention_last               867   +    869   ->    870\n",
      "LRP                          867   +    869   ->    870\n",
      "gradcam                      867   +    869   ->    870\n",
      "gradcamgithub                867   +    869   ->    870\n",
      "vanillapixel                 867   +    869   ->    870\n",
      "vanillaembedding             867   +    869   ->    870\n",
      "sgpixel                      867   +    869   ->    870\n",
      "sgembedding                  867   +    869   ->    870\n",
      "vargradpixel                 867   +    869   ->    870\n",
      "vargradembedding             867   +    869   ->    870\n",
      "igpixel                      867   +    869   ->    870\n",
      "igembedding                  867   +    869   ->    870\n",
      "leaveoneoutclassifier        867   +    869   ->    870\n",
      "riseclassifier               867   +    869   ->    870\n",
      "ours                         867   +    869   ->    870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▎              | 878/1000 [11:21:12<7:12:28, 212.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       872   +    875   ->    876\n",
      "attention_rollout            871   +    874   ->    875\n",
      "attention_last               871   +    874   ->    875\n",
      "LRP                          871   +    874   ->    875\n",
      "gradcam                      871   +    874   ->    875\n",
      "gradcamgithub                871   +    874   ->    875\n",
      "vanillapixel                 871   +    874   ->    875\n",
      "vanillaembedding             871   +    874   ->    875\n",
      "sgpixel                      871   +    874   ->    875\n",
      "sgembedding                  871   +    874   ->    875\n",
      "vargradpixel                 871   +    874   ->    875\n",
      "vargradembedding             871   +    874   ->    875\n",
      "igpixel                      871   +    874   ->    875\n",
      "igembedding                  871   +    874   ->    875\n",
      "leaveoneoutclassifier        871   +    874   ->    875\n",
      "riseclassifier               871   +    874   ->    875\n",
      "ours                         871   +    874   ->    875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▊              | 882/1000 [11:35:24<6:58:24, 212.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       877   +    879   ->    880\n",
      "attention_rollout            876   +    878   ->    879\n",
      "attention_last               876   +    878   ->    879\n",
      "LRP                          876   +    878   ->    879\n",
      "gradcam                      876   +    878   ->    879\n",
      "gradcamgithub                876   +    878   ->    879\n",
      "vanillapixel                 876   +    878   ->    879\n",
      "vanillaembedding             876   +    878   ->    879\n",
      "sgpixel                      876   +    878   ->    879\n",
      "sgembedding                  876   +    878   ->    879\n",
      "vargradpixel                 876   +    878   ->    879\n",
      "vargradembedding             876   +    878   ->    879\n",
      "igpixel                      876   +    878   ->    879\n",
      "igembedding                  876   +    878   ->    879\n",
      "leaveoneoutclassifier        876   +    878   ->    879\n",
      "riseclassifier               876   +    878   ->    879\n",
      "ours                         876   +    878   ->    879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▎             | 886/1000 [11:49:35<6:44:13, 212.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       881   +    883   ->    884\n",
      "attention_rollout            880   +    882   ->    883\n",
      "attention_last               880   +    882   ->    883\n",
      "LRP                          880   +    882   ->    883\n",
      "gradcam                      880   +    882   ->    883\n",
      "gradcamgithub                880   +    882   ->    883\n",
      "vanillapixel                 880   +    882   ->    883\n",
      "vanillaembedding             880   +    882   ->    883\n",
      "sgpixel                      880   +    882   ->    883\n",
      "sgembedding                  880   +    882   ->    883\n",
      "vargradpixel                 880   +    882   ->    883\n",
      "vargradembedding             880   +    882   ->    883\n",
      "igpixel                      880   +    882   ->    883\n",
      "igembedding                  880   +    882   ->    883\n",
      "leaveoneoutclassifier        880   +    882   ->    883\n",
      "riseclassifier               880   +    882   ->    883\n",
      "ours                         880   +    882   ->    883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▊             | 890/1000 [12:03:45<6:29:56, 212.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       885   +    887   ->    888\n",
      "attention_rollout            884   +    886   ->    887\n",
      "attention_last               884   +    886   ->    887\n",
      "LRP                          884   +    886   ->    887\n",
      "gradcam                      884   +    886   ->    887\n",
      "gradcamgithub                884   +    886   ->    887\n",
      "vanillapixel                 884   +    886   ->    887\n",
      "vanillaembedding             884   +    886   ->    887\n",
      "sgpixel                      884   +    886   ->    887\n",
      "sgembedding                  884   +    886   ->    887\n",
      "vargradpixel                 884   +    886   ->    887\n",
      "vargradembedding             884   +    886   ->    887\n",
      "igpixel                      884   +    886   ->    887\n",
      "igembedding                  884   +    886   ->    887\n",
      "leaveoneoutclassifier        884   +    886   ->    887\n",
      "riseclassifier               884   +    886   ->    887\n",
      "ours                         884   +    886   ->    887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▎            | 894/1000 [12:17:55<6:15:43, 212.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       889   +    891   ->    892\n",
      "attention_rollout            888   +    890   ->    891\n",
      "attention_last               888   +    890   ->    891\n",
      "LRP                          888   +    890   ->    891\n",
      "gradcam                      888   +    890   ->    891\n",
      "gradcamgithub                888   +    890   ->    891\n",
      "vanillapixel                 888   +    890   ->    891\n",
      "vanillaembedding             888   +    890   ->    891\n",
      "sgpixel                      888   +    890   ->    891\n",
      "sgembedding                  888   +    890   ->    891\n",
      "vargradpixel                 888   +    890   ->    891\n",
      "vargradembedding             888   +    890   ->    891\n",
      "igpixel                      888   +    890   ->    891\n",
      "igembedding                  888   +    890   ->    891\n",
      "leaveoneoutclassifier        888   +    890   ->    891\n",
      "riseclassifier               888   +    890   ->    891\n",
      "ours                         888   +    890   ->    891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▊            | 898/1000 [12:32:08<6:01:51, 212.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       893   +    895   ->    896\n",
      "attention_rollout            892   +    894   ->    895\n",
      "attention_last               892   +    894   ->    895\n",
      "LRP                          892   +    894   ->    895\n",
      "gradcam                      892   +    894   ->    895\n",
      "gradcamgithub                892   +    894   ->    895\n",
      "vanillapixel                 892   +    894   ->    895\n",
      "vanillaembedding             892   +    894   ->    895\n",
      "sgpixel                      892   +    894   ->    895\n",
      "sgembedding                  892   +    894   ->    895\n",
      "vargradpixel                 892   +    894   ->    895\n",
      "vargradembedding             892   +    894   ->    895\n",
      "igpixel                      892   +    894   ->    895\n",
      "igembedding                  892   +    894   ->    895\n",
      "leaveoneoutclassifier        892   +    894   ->    895\n",
      "riseclassifier               892   +    894   ->    895\n",
      "ours                         892   +    894   ->    895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▏           | 902/1000 [12:46:20<5:47:41, 212.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       897   +    899   ->    900\n",
      "attention_rollout            896   +    898   ->    899\n",
      "attention_last               896   +    898   ->    899\n",
      "LRP                          896   +    898   ->    899\n",
      "gradcam                      896   +    898   ->    899\n",
      "gradcamgithub                896   +    898   ->    899\n",
      "vanillapixel                 896   +    898   ->    899\n",
      "vanillaembedding             896   +    898   ->    899\n",
      "sgpixel                      896   +    898   ->    899\n",
      "sgembedding                  896   +    898   ->    899\n",
      "vargradpixel                 896   +    898   ->    899\n",
      "vargradembedding             896   +    898   ->    899\n",
      "igpixel                      896   +    898   ->    899\n",
      "igembedding                  896   +    898   ->    899\n",
      "leaveoneoutclassifier        896   +    898   ->    899\n",
      "riseclassifier               896   +    898   ->    899\n",
      "ours                         896   +    898   ->    899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▋           | 906/1000 [13:00:31<5:33:25, 212.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▉           | 909/1000 [13:01:32<1:18:14, 51.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_last not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_8751.JPEG'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m     explanations\u001b[38;5;241m=\u001b[39m[np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mRandomState(idx)\u001b[38;5;241m.\u001b[39muniform(low\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, high\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-40\u001b[39m, size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, num_players)) \u001b[38;5;28;01mfor\u001b[39;00m idx, path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(paths)]\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 37\u001b[0m     explanations\u001b[38;5;241m=\u001b[39m[explanation_save_dict[backbone_type][explanation_method][adapt_path(path, \u001b[38;5;28mlist\u001b[39m(explanation_save_dict[backbone_type][explanation_method]\u001b[38;5;241m.\u001b[39mkeys()))][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexplanation\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     38\u001b[0m                   \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m paths]\n\u001b[1;32m     40\u001b[0m insertdelete_dict\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minsertion\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeletion\u001b[39m\u001b[38;5;124m'\u001b[39m: []}\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image, explanation \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(images, explanations):\n",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     35\u001b[0m     explanations\u001b[38;5;241m=\u001b[39m[np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mRandomState(idx)\u001b[38;5;241m.\u001b[39muniform(low\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, high\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-40\u001b[39m, size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, num_players)) \u001b[38;5;28;01mfor\u001b[39;00m idx, path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(paths)]\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 37\u001b[0m     explanations\u001b[38;5;241m=\u001b[39m[\u001b[43mexplanation_save_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbackbone_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mexplanation_method\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43madapt_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexplanation_save_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbackbone_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mexplanation_method\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexplanation\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     38\u001b[0m                   \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m paths]\n\u001b[1;32m     40\u001b[0m insertdelete_dict\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minsertion\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeletion\u001b[39m\u001b[38;5;124m'\u001b[39m: []}\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image, explanation \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(images, explanations):\n",
      "\u001b[0;31mKeyError\u001b[0m: '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_8751.JPEG'"
     ]
    }
   ],
   "source": [
    "if evaluation_stage==\"4_insert_delete\":\n",
    "    num_players=196\n",
    "    for idx, batch in enumerate(tqdm(data_loader, total = int(1000/data_loader.batch_size+0.5) if dataset_split==\"test\" and (len(explanation_method_to_run)!=1 or explanation_method_to_run[0]!=\"kernelshap\") else None)):\n",
    "        if dataset_split==\"test\" and (len(explanation_method_to_run)!=1 or explanation_method_to_run[0]!=\"kernelshap\"):\n",
    "            if idx>int(1000/data_loader.batch_size+0.5):\n",
    "                break\n",
    "        if (idx%parallel_mode[1])!=parallel_mode[0]:\n",
    "            continue                                \n",
    "\n",
    "        images=batch['images']\n",
    "        labels=batch['labels']\n",
    "        paths=batch['path']\n",
    "\n",
    "        updated_signal_list=[]\n",
    "\n",
    "        for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "\n",
    "            for explanation_method in explanation_method_to_run:\n",
    "                data_keys=insertdelete_save_dict[backbone_type][explanation_method].keys()\n",
    "                data_keys=[adapt_path(data_keys_path, paths) for data_keys_path in data_keys]\n",
    "                if explanation_method=='kernelshap':\n",
    "                    if all([(path in data_keys) or (path not in estimationerror_sample_path_list) for path in paths]):\n",
    "                        continue\n",
    "                    else:\n",
    "                        print(explanation_method,'not exist')\n",
    "                        updated_signal_list.append(explanation_method)                \n",
    "                else:\n",
    "                    if all([path in data_keys for path in paths]):\n",
    "                        continue\n",
    "                    else:\n",
    "                        print(explanation_method,'not exist')\n",
    "                        updated_signal_list.append(explanation_method)                      \n",
    "                \n",
    "                if explanation_method==\"random\":\n",
    "                    explanations=[np.random.RandomState(idx).uniform(low=0, high=1e-40, size=(10, num_players)) for idx, path in enumerate(paths)]\n",
    "                else:\n",
    "                    explanations=[explanation_save_dict[backbone_type][explanation_method][adapt_path(path, list(explanation_save_dict[backbone_type][explanation_method].keys()))]['explanation']\n",
    "                                  for path in paths]\n",
    "                \n",
    "                insertdelete_dict={'insertion': [], 'deletion': []}\n",
    "                for image, explanation in zip(images, explanations):\n",
    "                    image_loaded=image[np.newaxis, :].repeat(num_players+1, 1, 1, 1).to(surrogate_dict[backbone_type][\"pre-softmax\"].device)\n",
    "                    if np.isnan(explanation).any():\n",
    "                        print(explanation_method, \"Null found\")\n",
    "                      \n",
    "                    if explanation_method==\"random\":\n",
    "                        for metric_mode in insertdelete_dict.keys():\n",
    "                            mask=explanation_to_mask(explanation=np.array([get_relative_value(explanation_) for explanation_ in explanation]), mode=metric_mode)\n",
    "                            prob_=[]\n",
    "                            for random_iter in range(mask.shape[0]):\n",
    "                                surrogate_dict[backbone_type][\"pre-softmax\"].eval()\n",
    "                                with torch.no_grad():\n",
    "                                    output = surrogate_dict[backbone_type][\"pre-softmax\"](image_loaded,\n",
    "                                                                                          masks=torch.Tensor(mask[random_iter]).to(surrogate_dict[backbone_type][\"pre-softmax\"].device))\n",
    "                                if _config[\"output_dim\"]==1:\n",
    "                                    prob=output['logits'].sigmoid().cpu().numpy()\n",
    "                                else:\n",
    "                                    prob=output['logits'].softmax(dim=-1).cpu().numpy()\n",
    "                                prob_.append(prob.T)\n",
    "                            prob=np.array(prob_) # (10, num_players+1, num_classes)\n",
    "                            insertdelete_dict[metric_mode].append(prob)\n",
    "                        \n",
    "                    elif len(explanation.shape)==1:\n",
    "                        for metric_mode in insertdelete_dict.keys():\n",
    "                            mask=explanation_to_mask(explanation=get_relative_value(explanation)[np.newaxis,:], mode=metric_mode)\n",
    "                            surrogate_dict[backbone_type][\"pre-softmax\"].eval()\n",
    "                            with torch.no_grad():\n",
    "                                output = surrogate_dict[backbone_type][\"pre-softmax\"](image_loaded,\n",
    "                                                                                      masks=torch.Tensor(mask[0]).to(surrogate_dict[backbone_type][\"pre-softmax\"].device))\n",
    "                            if _config[\"output_dim\"]==1:\n",
    "                                prob=output['logits'].sigmoid().cpu().numpy()\n",
    "                            else:\n",
    "                                prob=output['logits'].softmax(dim=-1).cpu().numpy()\n",
    "                            prob=prob.T # ( , num_players)\n",
    "                            insertdelete_dict[metric_mode].append(prob) \n",
    "\n",
    "                    elif len(explanation.shape)==2:\n",
    "                        for metric_mode in insertdelete_dict.keys():\n",
    "                            mask=explanation_to_mask(explanation=np.array([get_relative_value(explanation_) for explanation_ in explanation]), mode=metric_mode)\n",
    "                            prob_=[]\n",
    "                            for class_idx in range(mask.shape[0]):\n",
    "                                surrogate_dict[backbone_type][\"pre-softmax\"].eval()\n",
    "                                with torch.no_grad():\n",
    "                                    output = surrogate_dict[backbone_type][\"pre-softmax\"](image_loaded,\n",
    "                                                                                          masks=torch.Tensor(mask[class_idx]).to(surrogate_dict[backbone_type][\"pre-softmax\"].device))                                  \n",
    "                                if _config[\"output_dim\"]==1:\n",
    "                                    prob=output['logits'].sigmoid().cpu().numpy()\n",
    "                                else:\n",
    "                                    prob=output['logits'].softmax(dim=-1).cpu().numpy()\n",
    "                                prob_.append(prob[:, class_idx])\n",
    "                            prob=np.array(prob_)\n",
    "                            insertdelete_dict[metric_mode].append(prob)\n",
    "                    else:\n",
    "                        raise\n",
    "                            \n",
    "                if explanation_method==\"random\":\n",
    "                    insertdelete_save_dict_update(backbone_type, explanation_method,\n",
    "                                                  path_list=paths, \n",
    "                                                  insert_list=insertdelete_dict[\"insertion\"], \n",
    "                                                  delete_list=insertdelete_dict[\"deletion\"], \n",
    "                                                  shape=(10, _config[\"output_dim\"], num_players+1))                       \n",
    "                elif len(explanations[0].shape)==1:\n",
    "                    insertdelete_save_dict_update(backbone_type, explanation_method,\n",
    "                                                  path_list=paths, \n",
    "                                                  insert_list=insertdelete_dict[\"insertion\"], \n",
    "                                                  delete_list=insertdelete_dict[\"deletion\"], \n",
    "                                                  shape=(_config[\"output_dim\"], num_players+1))                    \n",
    "                elif len(explanations[0].shape)==2:\n",
    "                    insertdelete_save_dict_update(backbone_type, explanation_method,\n",
    "                                                  path_list=paths, \n",
    "                                                  insert_list=insertdelete_dict[\"insertion\"], \n",
    "                                                  delete_list=insertdelete_dict[\"deletion\"], \n",
    "                                                  shape=(_config[\"output_dim\"], num_players+1))\n",
    "                else:\n",
    "                    raise\n",
    "        \n",
    "        \n",
    "        for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "            for explanation_method, insertdelete_save_dict_backbone_method in insertdelete_save_dict[backbone_type].items():\n",
    "                \n",
    "                if explanation_method not in updated_signal_list:\n",
    "                    continue                \n",
    "                \n",
    "                insertdelete_save_dict_path=f'results/4_insert_delete/{_config[\"datasets\"]}/{backbone_type}_{explanation_method}_{dataset_split}.pickle'\n",
    "\n",
    "                if os.path.isfile(insertdelete_save_dict_path):\n",
    "                    with open(insertdelete_save_dict_path, 'rb') as f:\n",
    "                        insertdelete_save_dict_loaded=pickle.load(f)\n",
    "                else:\n",
    "                    insertdelete_save_dict_loaded={}\n",
    "\n",
    "                len_original=len(insertdelete_save_dict_backbone_method)            \n",
    "                len_loaded=len(insertdelete_save_dict_loaded)\n",
    "                insertdelete_save_dict_backbone_method.update(insertdelete_save_dict_loaded)\n",
    "                len_updated=len(insertdelete_save_dict_backbone_method)\n",
    "\n",
    "                print(f'{explanation_method:24}  {len_original:6}   + {len_loaded:6}   -> {len_updated:6}')\n",
    "\n",
    "\n",
    "                with open(insertdelete_save_dict_path, \"wb\") as f:\n",
    "                    pickle.dump(insertdelete_save_dict_backbone_method, f)           \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ecfa8ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random                       901   +    914   ->    915\n",
      "attention_rollout            900   +    913   ->    914\n",
      "attention_last               899   +    913   ->    913\n",
      "LRP                          899   +    913   ->    913\n",
      "gradcam                      899   +    913   ->    913\n",
      "gradcamgithub                899   +    913   ->    913\n",
      "vanillapixel                 899   +    913   ->    913\n",
      "vanillaembedding             899   +    913   ->    913\n",
      "sgpixel                      899   +    913   ->    913\n",
      "sgembedding                  899   +    913   ->    913\n",
      "vargradpixel                 899   +    913   ->    913\n",
      "vargradembedding             899   +    913   ->    913\n",
      "igpixel                      899   +    913   ->    913\n",
      "igembedding                  899   +    913   ->    913\n",
      "leaveoneoutclassifier        899   +    913   ->    913\n",
      "leaveoneoutsurrogate           0   +      0   ->      0\n",
      "riseclassifier               899   +    913   ->    913\n",
      "risesurrogate                  0   +      0   ->      0\n",
      "ours                         899   +    913   ->    913\n",
      "kernelshap                     0   +      0   ->      0\n"
     ]
    }
   ],
   "source": [
    "if evaluation_stage==\"4_insert_delete\":\n",
    "    for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "        for explanation_method, insertdelete_save_dict_backbone_method in insertdelete_save_dict[backbone_type].items():\n",
    "            insertdelete_save_dict_path=f'results/4_insert_delete/{_config[\"datasets\"]}/{backbone_type}_{explanation_method}_{dataset_split}.pickle'\n",
    "\n",
    "            if os.path.isfile(insertdelete_save_dict_path):\n",
    "                with open(insertdelete_save_dict_path, 'rb') as f:\n",
    "                    insertdelete_save_dict_loaded=pickle.load(f)\n",
    "            else:\n",
    "                insertdelete_save_dict_loaded={}\n",
    "\n",
    "            len_original=len(insertdelete_save_dict_backbone_method)            \n",
    "            len_loaded=len(insertdelete_save_dict_loaded)\n",
    "            insertdelete_save_dict_backbone_method.update(insertdelete_save_dict_loaded)\n",
    "            len_updated=len(insertdelete_save_dict_backbone_method)\n",
    "\n",
    "            print(f'{explanation_method:24}  {len_original:6}   + {len_loaded:6}   -> {len_updated:6}')\n",
    "\n",
    "\n",
    "            with open(insertdelete_save_dict_path, \"wb\") as f:\n",
    "                pickle.dump(insertdelete_save_dict_backbone_method, f)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19f433b",
   "metadata": {},
   "source": [
    "# 5_sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415b792b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(num_players: int, num_mask_samples: int or None = None, paired_mask_samples: bool = True,\n",
    "                  mode: str = 'uniform', random_state: np.random.RandomState or None = None) -> np.array:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        num_players: the number of players in the coalitional game\n",
    "        num_mask_samples: the number of masks to generate\n",
    "        paired_mask_samples: if True, the generated masks are pairs of x and 1-x.\n",
    "        mode: the distribution that the number of masked features follows. ('uniform' or 'shapley')\n",
    "        random_state: random generator\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor of shape\n",
    "        (num_masks, num_players) if num_masks is int\n",
    "        (num_players) if num_masks is None\n",
    "\n",
    "    \"\"\"\n",
    "    random_state = random_state or np.random\n",
    "\n",
    "    num_samples_ = num_mask_samples or 1\n",
    "\n",
    "    if paired_mask_samples:\n",
    "        assert num_samples_ % 2 == 0, \"'num_samples' must be a multiple of 2 if 'paired' is True\"\n",
    "        num_samples_ = num_samples_ // 2\n",
    "    else:\n",
    "        num_samples_ = num_samples_\n",
    "\n",
    "    if mode == 'uniform':\n",
    "        masks = (random_state.rand(num_samples_, num_players) > random_state.rand(num_samples_, 1)).astype('int')\n",
    "    elif mode == 'shapley':\n",
    "        probs = 1 / (np.arange(1, num_players) * (num_players - np.arange(1, num_players)))\n",
    "        probs = probs / probs.sum()\n",
    "        masks = (random_state.rand(num_samples_, num_players) > 1 / num_players * random_state.choice(\n",
    "            np.arange(num_players - 1), p=probs, size=[num_samples_, 1])).astype('int')\n",
    "    else:\n",
    "        raise ValueError(\"'mode' must be 'random' or 'shapley'\")\n",
    "\n",
    "    if paired_mask_samples:\n",
    "        masks = np.stack([masks, 1 - masks], axis=1).reshape(num_samples_ * 2, num_players)\n",
    "\n",
    "    if num_mask_samples is None:\n",
    "        masks = masks.squeeze(0)\n",
    "        return masks  # (num_masks)\n",
    "    else:\n",
    "        return masks  # (num_samples, num_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a4bf39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if evaluation_stage==\"5_sensitivity\":\n",
    "    num_players=196\n",
    "    for idx, batch in enumerate(tqdm(data_loader, total = int(1000/data_loader.batch_size+0.5) if dataset_split==\"test\" else None)):   \n",
    "        if dataset_split==\"test\":\n",
    "            if idx>int(1000/data_loader.batch_size+0.5):\n",
    "                break\n",
    "        if (idx%parallel_mode[1])!=parallel_mode[0]:\n",
    "            continue                                \n",
    "\n",
    "        images=batch['images']\n",
    "        labels=batch['labels']\n",
    "        paths=batch['path']\n",
    "\n",
    "        for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):            \n",
    "            for image, path in zip(images, paths):\n",
    "                #path=path.replace('l0.cs.washington.edu','l2lambda.cs.washington.edu')\n",
    "                for num_included_players in [\"all\"] + list(range(14, 196, 14)):\n",
    "                    if num_included_players==\"all\":\n",
    "                        prob_all=[]\n",
    "                        mask_all=[]\n",
    "                        for random_iter in range(20):\n",
    "                            image_loaded=image[np.newaxis, :].repeat(50, 1, 1, 1).to(surrogate_dict[backbone_type][\"pre-softmax\"].device)\n",
    "                            mask=generate_mask(num_players=num_players,\n",
    "                                               num_mask_samples=50,\n",
    "                                               paired_mask_samples=False,\n",
    "                                               mode=\"uniform\",\n",
    "                                               random_state=np.random.RandomState(random_iter))\n",
    "\n",
    "                            surrogate_dict[backbone_type][\"pre-softmax\"].eval()\n",
    "                            with torch.no_grad():\n",
    "                                output = surrogate_dict[backbone_type][\"pre-softmax\"](image_loaded,\n",
    "                                                                                      masks=torch.Tensor(mask).to(surrogate_dict[backbone_type][\"pre-softmax\"].device))\n",
    "\n",
    "                            if _config[\"output_dim\"]==1:\n",
    "                                prob=output['logits'].sigmoid().cpu().numpy()\n",
    "                            else:\n",
    "                                prob=output['logits'].softmax(dim=-1).cpu().numpy()   \n",
    "                            prob_all.append(prob)\n",
    "                            mask_all.append(mask)\n",
    "                        prob_all=np.concatenate(prob_all, axis=0)\n",
    "                        mask_all=np.concatenate(mask_all, axis=0)\n",
    "                    else:\n",
    "                        prob_all=[]\n",
    "                        mask_all=[]\n",
    "                        for random_iter in range(20):\n",
    "                            image_loaded=image[np.newaxis, :].repeat(50, 1, 1, 1).to(surrogate_dict[backbone_type][\"pre-softmax\"].device)                                                        \n",
    "                            mask=np.zeros((50, num_players))\n",
    "                            mask[:, :num_included_players]=1\n",
    "                            for i in range(len(mask)):\n",
    "                                mask[i]=np.random.RandomState(42+10*random_iter+i).permutation(mask[i])\n",
    "\n",
    "                            surrogate_dict[backbone_type][\"pre-softmax\"].eval()\n",
    "                            with torch.no_grad():\n",
    "                                output = surrogate_dict[backbone_type][\"pre-softmax\"](image_loaded,\n",
    "                                                                                      masks=torch.Tensor(mask).to(surrogate_dict[backbone_type][\"pre-softmax\"].device))\n",
    "\n",
    "                            if _config[\"output_dim\"]==1:\n",
    "                                prob=output['logits'].sigmoid().cpu().numpy()\n",
    "                            else:\n",
    "                                prob=output['logits'].softmax(dim=-1).cpu().numpy()   \n",
    "                            prob_all.append(prob)\n",
    "                            mask_all.append(mask)\n",
    "                        prob_all=np.concatenate(prob_all, axis=0)\n",
    "                        mask_all=np.concatenate(mask_all, axis=0)\n",
    "                    \n",
    "                    for explanation_method in explanation_method_to_run:                \n",
    "                        if explanation_method==\"random\":\n",
    "                            continue\n",
    "                        explanation=explanation_save_dict[backbone_type][explanation_method][adapt_path(path, list(explanation_save_dict[backbone_type][explanation_method].keys())[0])]['explanation']\n",
    "                        explanation=explanation+np.random.RandomState(42).uniform(low=0, high=1e-40, size=explanation.shape)\n",
    "                        explanation_mask=explanation@(mask_all.T)\n",
    "\n",
    "                        if len(explanation.shape)==1:\n",
    "                            correlation = np.array([stats.spearmanr(explanation_mask, prob_all[:, i]).correlation for i in range(prob_all.shape[1])])\n",
    "                            assert correlation.shape==(_config[\"output_dim\"],)\n",
    "                            \n",
    "                        elif len(explanation.shape)==2:\n",
    "                            #correlation=stats.spearmanr(np.concatenate([explanation_mask, prob_all.T], axis=0), axis=1).correlation\n",
    "                            correlation = np.array([stats.spearmanr(explanation_mask[i], prob_all[:, i]).correlation for i in range(prob_all.shape[1])])\n",
    "                            assert correlation.shape==(_config[\"output_dim\"],)\n",
    "                        else:\n",
    "                            raise\n",
    "                        sensitivity_save_dit_update(backbone_type, explanation_method,\n",
    "                                                     num_included_players=num_included_players,\n",
    "                                                     path_list=[path], sensitivity_list=[correlation],\n",
    "                                                     shape=(_config[\"output_dim\"],))     \n",
    "                    \n",
    "                    \n",
    "                \n",
    "        for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "            for explanation_method, sensitivity_save_dit_backbone_method in sensitivity_save_dit[backbone_type].items():\n",
    "                sensitivity_save_dit_path=f'results/5_sensitivity/{_config[\"datasets\"]}/{backbone_type}_{explanation_method}_{dataset_split}.pickle'\n",
    "\n",
    "                if os.path.isfile(sensitivity_save_dit_path):\n",
    "                    with open(sensitivity_save_dit_path, 'rb') as f:\n",
    "                        sensitivity_save_dit_loaded=pickle.load(f)\n",
    "                else:\n",
    "                    sensitivity_save_dit_loaded={}\n",
    "\n",
    "                len_original=len(sensitivity_save_dit_backbone_method)            \n",
    "                len_loaded=len(sensitivity_save_dit_loaded)\n",
    "                sensitivity_save_dit_backbone_method.update(sensitivity_save_dit_loaded)\n",
    "                len_updated=len(sensitivity_save_dit_backbone_method)\n",
    "\n",
    "                print(f'{explanation_method:24}  {len_original:6}   + {len_loaded:6}   -> {len_updated:6}')\n",
    "\n",
    "\n",
    "                with open(sensitivity_save_dit_path, \"wb\") as f:\n",
    "                    pickle.dump(sensitivity_save_dit_backbone_method, f)        \n",
    "                    \n",
    "             \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89881f09",
   "metadata": {},
   "source": [
    "# 6_noretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca77a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluation_stage==\"6_noretraining\":\n",
    "    num_players=196\n",
    "    for idx, batch in enumerate(tqdm(data_loader, total = int(1000/data_loader.batch_size+0.5) if dataset_split==\"test\" else None)):   \n",
    "        if dataset_split==\"test\":\n",
    "            if idx>int(1000/data_loader.batch_size+0.5):\n",
    "                break\n",
    "        if (idx%parallel_mode[1])!=parallel_mode[0]:\n",
    "            continue                                \n",
    "\n",
    "        images=batch['images']\n",
    "        labels=batch['labels']\n",
    "        paths=batch['path']\n",
    "        updated_signal_list=[]\n",
    "\n",
    "        for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "\n",
    "            for explanation_method in [\"random\"]+explanation_method_to_run:\n",
    "                data_keys=noretraining_save_dict[backbone_type][explanation_method].keys()\n",
    "                data_keys=[adapt_path(data_keys_path, paths) for data_keys_path in data_keys]\n",
    "                if all([path in data_keys for path in paths]):\n",
    "                    continue\n",
    "                else:\n",
    "                    print(explanation_method,'not exist')\n",
    "                    updated_signal_list.append(explanation_method)                \n",
    "                \n",
    "                if explanation_method==\"random\":\n",
    "                    explanations=[np.random.RandomState(idx).uniform(low=0, high=1e-40, size=(10, num_players)) for idx, path in enumerate(paths)]\n",
    "                else:\n",
    "                    explanations=[explanation_save_dict[backbone_type][explanation_method][adapt_path(path, list(explanation_save_dict[backbone_type][explanation_method].keys()))]['explanation']\n",
    "                                  for path in paths]\n",
    "                noretraining_dict={'insertion': [], 'deletion': []}\n",
    "                for image, explanation, label in zip(images, explanations, labels):\n",
    "                    image_loaded=image[np.newaxis, :].repeat(num_players+1, 1, 1, 1).to(surrogate_dict[backbone_type][\"pre-softmax\"].device)\n",
    "                    if np.isnan(explanation).any():\n",
    "                        print(explanation_method, \"Null found\")\n",
    "                      \n",
    "                    if explanation_method==\"random\":\n",
    "                        for metric_mode in noretraining_dict.keys():\n",
    "                            mask=explanation_to_mask(explanation=np.array([get_relative_value(explanation_) for explanation_ in explanation]), mode=metric_mode)\n",
    "                            prob_=[]\n",
    "                            for random_iter in range(mask.shape[0]):\n",
    "                                surrogate_dict[backbone_type][\"pre-softmax\"].eval()\n",
    "                                with torch.no_grad():\n",
    "                                    output = surrogate_dict[backbone_type][\"pre-softmax\"](image_loaded,\n",
    "                                                                                          masks=torch.Tensor(mask[random_iter]).to(surrogate_dict[backbone_type][\"pre-softmax\"].device))\n",
    "                                if _config[\"output_dim\"]==1:\n",
    "                                    prob=output['logits'].sigmoid().cpu().numpy()\n",
    "                                else:\n",
    "                                    prob=output['logits'].softmax(dim=-1).cpu().numpy()\n",
    "                                prob_.append(prob.T)\n",
    "                            prob=np.array(prob_) # (10, num_classes, num_players+1)\n",
    "                            noretraining_dict[metric_mode].append(prob)#(prob.argmax(axis=1)==label.item()).astype(float))\n",
    "                        \n",
    "                    elif len(explanation.shape)==1:\n",
    "                        for metric_mode in noretraining_dict.keys():\n",
    "                            mask=explanation_to_mask(explanation=get_relative_value(explanation)[np.newaxis,:], mode=metric_mode)\n",
    "                            surrogate_dict[backbone_type][\"pre-softmax\"].eval()\n",
    "                            with torch.no_grad():\n",
    "                                output = surrogate_dict[backbone_type][\"pre-softmax\"](image_loaded,\n",
    "                                                                                      masks=torch.Tensor(mask[0]).to(surrogate_dict[backbone_type][\"pre-softmax\"].device))\n",
    "                            if _config[\"output_dim\"]==1:\n",
    "                                prob=output['logits'].sigmoid().cpu().numpy()\n",
    "                            else:\n",
    "                                prob=output['logits'].softmax(dim=-1).cpu().numpy()\n",
    "                            prob=prob.T # (num_classes, num_players+1)                            \n",
    "                            #noretraining_dict[metric_mode].append((prob.argmax(axis=0)==label.item()).astype(float)) \n",
    "                            noretraining_dict[metric_mode].append(prob) \n",
    "\n",
    "                    elif len(explanation.shape)==2:\n",
    "                        for metric_mode in noretraining_dict.keys():\n",
    "                            #mask=explanation_to_mask(explanation=explanation[label.item()], mode=metric_mode)\n",
    "                            if len(explanation)==1:\n",
    "                                mask=explanation_to_mask(explanation=get_relative_value(explanation[0])[np.newaxis,:], mode=metric_mode)\n",
    "                            else:\n",
    "                                mask=explanation_to_mask(explanation=get_relative_value(explanation[label.item()])[np.newaxis,:], mode=metric_mode)\n",
    "                            \n",
    "                            surrogate_dict[backbone_type][\"pre-softmax\"].eval()\n",
    "                            with torch.no_grad():\n",
    "                                output = surrogate_dict[backbone_type][\"pre-softmax\"](image_loaded,\n",
    "                                                                                      masks=torch.Tensor(mask[0]).to(surrogate_dict[backbone_type][\"pre-softmax\"].device))\n",
    "                            if _config[\"output_dim\"]==1:\n",
    "                                prob=output['logits'].sigmoid().cpu().numpy()\n",
    "                            else:\n",
    "                                prob=output['logits'].softmax(dim=-1).cpu().numpy()\n",
    "                            prob=prob.T\n",
    "                            #noretraining_dict[metric_mode].append((prob.argmax(axis=0)==label.item()).astype(float))\n",
    "                            noretraining_dict[metric_mode].append(prob)\n",
    "                    else:\n",
    "                        raise\n",
    "                          \n",
    "                if explanation_method==\"random\":\n",
    "                    noretraining_save_dict_update(backbone_type, explanation_method,\n",
    "                                                  path_list=paths, \n",
    "                                                  insert_list=noretraining_dict[\"insertion\"], \n",
    "                                                  delete_list=noretraining_dict[\"deletion\"], \n",
    "                                                  shape=(10, _config[\"output_dim\"], num_players+1))                       \n",
    "                elif len(explanations[0].shape)==1:\n",
    "                    noretraining_save_dict_update(backbone_type, explanation_method,\n",
    "                                                  path_list=paths, \n",
    "                                                  insert_list=noretraining_dict[\"insertion\"], \n",
    "                                                  delete_list=noretraining_dict[\"deletion\"], \n",
    "                                                  shape=(_config[\"output_dim\"], num_players+1))                    \n",
    "                elif len(explanations[0].shape)==2:\n",
    "                    noretraining_save_dict_update(backbone_type, explanation_method,\n",
    "                                                  path_list=paths, \n",
    "                                                  insert_list=noretraining_dict[\"insertion\"], \n",
    "                                                  delete_list=noretraining_dict[\"deletion\"], \n",
    "                                                  shape=(_config[\"output_dim\"], num_players+1))\n",
    "                else:\n",
    "                    raise\n",
    "        \n",
    "        try:        \n",
    "            for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "                for explanation_method, noretraining_save_dict_backbone_method in noretraining_save_dict[backbone_type].items():\n",
    "\n",
    "                    if explanation_method not in updated_signal_list:\n",
    "                        continue                \n",
    "\n",
    "                    noretraining_save_dict_path=f'results/6_noretraining/{_config[\"datasets\"]}/{backbone_type}_{explanation_method}_{dataset_split}.pickle'\n",
    "\n",
    "                    if os.path.isfile(noretraining_save_dict_path):\n",
    "                        try:\n",
    "                            with open(noretraining_save_dict_path, 'rb') as f:\n",
    "                                noretraining_save_dict_loaded=pickle.load(f)\n",
    "                        except:\n",
    "                            noretraining_save_dict_loaded={}\n",
    "                    else:\n",
    "                        noretraining_save_dict_loaded={}\n",
    "\n",
    "                    len_original=len(noretraining_save_dict_backbone_method)            \n",
    "                    len_loaded=len(noretraining_save_dict_loaded)\n",
    "                    noretraining_save_dict_backbone_method.update(noretraining_save_dict_loaded)\n",
    "                    len_updated=len(noretraining_save_dict_backbone_method)\n",
    "\n",
    "                    print(f'{explanation_method:24}  {len_original:6}   + {len_loaded:6}   -> {len_updated:6}')\n",
    "\n",
    "\n",
    "                    with open(noretraining_save_dict_path, \"wb\") as f:\n",
    "                        pickle.dump(noretraining_save_dict_backbone_method, f)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "                            \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8f74b5",
   "metadata": {},
   "source": [
    "# 7_classifiermasked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ff594c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluation_stage==\"7_classifiermasked\":\n",
    "    num_players=196\n",
    "    for idx, batch in enumerate(tqdm(data_loader, total = int(1000/data_loader.batch_size+0.5) if dataset_split==\"test\" else None)):   \n",
    "        if dataset_split==\"test\":\n",
    "            if idx>int(1000/data_loader.batch_size+0.5):\n",
    "                break\n",
    "        if (idx%parallel_mode[1])!=parallel_mode[0]:\n",
    "            continue                                \n",
    "\n",
    "        images=batch['images']\n",
    "        labels=batch['labels']\n",
    "        paths=batch['path']\n",
    "        updated_signal_list=[]\n",
    "\n",
    "        for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "\n",
    "            for explanation_method in [\"random\"]+explanation_method_to_run:\n",
    "                data_keys=classifiermasked_save_dict[backbone_type][explanation_method].keys()\n",
    "                data_keys=[adapt_path(data_keys_path, paths) for data_keys_path in data_keys]\n",
    "                if all([path in data_keys for path in paths]):\n",
    "                    continue\n",
    "                else:\n",
    "                    print(explanation_method,'not exist')\n",
    "                    updated_signal_list.append(explanation_method)                \n",
    "                \n",
    "                if explanation_method==\"random\":\n",
    "                    explanations=[np.random.RandomState(idx).uniform(low=0, high=1e-40, size=(10, num_players)) for idx, path in enumerate(paths)]\n",
    "                else:\n",
    "                    explanations=[explanation_save_dict[backbone_type][explanation_method][adapt_path(path, list(explanation_save_dict[backbone_type][explanation_method].keys()))]['explanation']\n",
    "                                  for path in paths]\n",
    "                classifiermasked_dict={'insertion': [], 'deletion': []}\n",
    "                for image, explanation, label in zip(images, explanations, labels):\n",
    "                    image_loaded=image[np.newaxis, :].repeat(num_players+1, 1, 1, 1).to(classifier_masked_dict[backbone_type].device)\n",
    "                    if np.isnan(explanation).any():\n",
    "                        print(explanation_method, \"Null found\")\n",
    "                      \n",
    "                    if explanation_method==\"random\":\n",
    "                        for metric_mode in classifiermasked_dict.keys():\n",
    "                            mask=explanation_to_mask(explanation=np.array([get_relative_value(explanation_) for explanation_ in explanation]), mode=metric_mode)\n",
    "                            prob_=[]\n",
    "                            for random_iter in range(mask.shape[0]):\n",
    "                                classifier_masked_dict[backbone_type].eval()\n",
    "                                with torch.no_grad():\n",
    "                                    output = classifier_masked_dict[backbone_type](image_loaded,\n",
    "                                                                                          masks=torch.Tensor(mask[random_iter]).to(classifier_masked_dict[backbone_type].device))\n",
    "                                if _config[\"output_dim\"]==1:\n",
    "                                    prob=output['logits'].sigmoid().cpu().numpy()\n",
    "                                else:\n",
    "                                    prob=output['logits'].softmax(dim=-1).cpu().numpy()\n",
    "                                prob_.append(prob.T)\n",
    "                            prob=np.array(prob_) # (10, num_classes, num_players+1)\n",
    "                            classifiermasked_dict[metric_mode].append(prob)#(prob.argmax(axis=1)==label.item()).astype(float))\n",
    "                        \n",
    "                    elif len(explanation.shape)==1:\n",
    "                        for metric_mode in classifiermasked_dict.keys():\n",
    "                            mask=explanation_to_mask(explanation=get_relative_value(explanation)[np.newaxis,:], mode=metric_mode)\n",
    "                            classifier_masked_dict[backbone_type].eval()\n",
    "                            with torch.no_grad():\n",
    "                                output = classifier_masked_dict[backbone_type](image_loaded,\n",
    "                                                                                      masks=torch.Tensor(mask[0]).to(classifier_masked_dict[backbone_type].device))\n",
    "                            if _config[\"output_dim\"]==1:\n",
    "                                prob=output['logits'].sigmoid().cpu().numpy()\n",
    "                            else:\n",
    "                                prob=output['logits'].softmax(dim=-1).cpu().numpy()\n",
    "                            prob=prob.T # (num_classes, num_players+1)                            \n",
    "                            #classifiermasked_dict[metric_mode].append((prob.argmax(axis=0)==label.item()).astype(float)) \n",
    "                            classifiermasked_dict[metric_mode].append(prob) \n",
    "\n",
    "                    elif len(explanation.shape)==2:\n",
    "                        for metric_mode in classifiermasked_dict.keys():\n",
    "                            #mask=explanation_to_mask(explanation=explanation[label.item()], mode=metric_mode)\n",
    "                            if len(explanation)==1:\n",
    "                                mask=explanation_to_mask(explanation=get_relative_value(explanation[0])[np.newaxis,:], mode=metric_mode)\n",
    "                            else:\n",
    "                                mask=explanation_to_mask(explanation=get_relative_value(explanation[label.item()])[np.newaxis,:], mode=metric_mode)\n",
    "                            \n",
    "                            classifier_masked_dict[backbone_type].eval()\n",
    "                            with torch.no_grad():\n",
    "                                output = classifier_masked_dict[backbone_type](image_loaded,\n",
    "                                                                                      masks=torch.Tensor(mask[0]).to(classifier_masked_dict[backbone_type].device))\n",
    "                            if _config[\"output_dim\"]==1:\n",
    "                                prob=output['logits'].sigmoid().cpu().numpy()\n",
    "                            else:\n",
    "                                prob=output['logits'].softmax(dim=-1).cpu().numpy()\n",
    "                            prob=prob.T\n",
    "                            #classifiermasked_dict[metric_mode].append((prob.argmax(axis=0)==label.item()).astype(float))\n",
    "                            classifiermasked_dict[metric_mode].append(prob)\n",
    "                    else:\n",
    "                        raise\n",
    "                          \n",
    "                if explanation_method==\"random\":\n",
    "                    classifiermasked_save_dict_update(backbone_type, explanation_method,\n",
    "                                                  path_list=paths, \n",
    "                                                  insert_list=classifiermasked_dict[\"insertion\"], \n",
    "                                                  delete_list=classifiermasked_dict[\"deletion\"], \n",
    "                                                  shape=(10, _config[\"output_dim\"], num_players+1))                       \n",
    "                elif len(explanations[0].shape)==1:\n",
    "                    classifiermasked_save_dict_update(backbone_type, explanation_method,\n",
    "                                                  path_list=paths, \n",
    "                                                  insert_list=classifiermasked_dict[\"insertion\"], \n",
    "                                                  delete_list=classifiermasked_dict[\"deletion\"], \n",
    "                                                  shape=(_config[\"output_dim\"], num_players+1))                    \n",
    "                elif len(explanations[0].shape)==2:\n",
    "                    classifiermasked_save_dict_update(backbone_type, explanation_method,\n",
    "                                                  path_list=paths, \n",
    "                                                  insert_list=classifiermasked_dict[\"insertion\"], \n",
    "                                                  delete_list=classifiermasked_dict[\"deletion\"], \n",
    "                                                  shape=(_config[\"output_dim\"], num_players+1))\n",
    "                else:\n",
    "                    raise\n",
    "        \n",
    "        try:        \n",
    "            for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "                for explanation_method, classifiermasked_save_dict_backbone_method in classifiermasked_save_dict[backbone_type].items():\n",
    "\n",
    "                    if explanation_method not in updated_signal_list:\n",
    "                        continue                \n",
    "\n",
    "                    classifiermasked_save_dict_path=f'results/7_classifiermasked/{_config[\"datasets\"]}/{backbone_type}_{explanation_method}_{dataset_split}.pickle'\n",
    "\n",
    "                    if os.path.isfile(classifiermasked_save_dict_path):\n",
    "                        try:\n",
    "                            with open(classifiermasked_save_dict_path, 'rb') as f:\n",
    "                                classifiermasked_save_dict_loaded=pickle.load(f)\n",
    "                        except:\n",
    "                            classifiermasked_save_dict_loaded={}\n",
    "                    else:\n",
    "                        classifiermasked_save_dict_loaded={}\n",
    "\n",
    "                    len_original=len(classifiermasked_save_dict_backbone_method)            \n",
    "                    len_loaded=len(classifiermasked_save_dict_loaded)\n",
    "                    classifiermasked_save_dict_backbone_method.update(classifiermasked_save_dict_loaded)\n",
    "                    len_updated=len(classifiermasked_save_dict_backbone_method)\n",
    "\n",
    "                    print(f'{explanation_method:24}  {len_original:6}   + {len_loaded:6}   -> {len_updated:6}')\n",
    "\n",
    "\n",
    "                    with open(classifiermasked_save_dict_path, \"wb\") as f:\n",
    "                        pickle.dump(classifiermasked_save_dict_backbone_method, f)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "                            \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedcf947",
   "metadata": {},
   "source": [
    "# 8_elapsedtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5e1873",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_random_explanation(num_players, num_samples=None):\n",
    "    if num_samples is None:\n",
    "        return np.random.RandomState(42).uniform(low=0, high=1e-40, size=(num_players,))\n",
    "    else:\n",
    "        return np.random.RandomState(42).uniform(low=0, high=1e-40, size=(num_samples, num_players))\n",
    "\n",
    "\n",
    "if evaluation_stage==\"8_elapsedtime\":\n",
    "    for idx, batch in enumerate(tqdm(data_loader)):#, total = int(1000/data_loader.batch_size+0.5) if dataset_split==\"test\" else None)):\n",
    "        if dataset_split==\"test\":\n",
    "            if idx>int(1000/data_loader.batch_size+0.5):\n",
    "                break\n",
    "        if (idx%parallel_mode[1])!=parallel_mode[0]:\n",
    "            continue\n",
    "            \n",
    "        images=batch['images']\n",
    "        labels=batch['labels']\n",
    "        paths=batch['path']\n",
    "        updated_signal_list=[]\n",
    "        \n",
    "        for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "            # Get classifier output\n",
    "            classifier_dict[backbone_type].eval()\n",
    "            with torch.no_grad():\n",
    "                classifier_output=classifier_dict[backbone_type](images.to(classifier_dict[backbone_type].device),\n",
    "                                                                 output_attentions=True)        \n",
    "            for explanation_method in explanation_method_to_run:\n",
    "                data_keys=elapsedtime_save_dict[backbone_type][explanation_method].keys()\n",
    "                data_keys=[adapt_path(data_keys_path, paths) for data_keys_path in data_keys]\n",
    "                if all([path in data_keys for path in paths]):\n",
    "                    continue\n",
    "                else:\n",
    "                    print(explanation_method,'not exist')\n",
    "                    updated_signal_list.append(explanation_method)\n",
    "                    \n",
    "                if explanation_method==\"random\":\n",
    "                    start_time=time.time()\n",
    "                    explanation_random_list=[get_random_explanation(num_players=196) for path in paths]\n",
    "                    elapsed_time=time.time()-start_time\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'random', path_list=paths, elapsed_time_list=[elapsed_time/len(paths) for i in range(len(paths))])\n",
    "                \n",
    "                elif explanation_method==\"attention_rollout\":\n",
    "                    attentions = np.asarray([att.cpu().detach().numpy() for att in classifier_output['self_attentions']]).transpose(1,0,2,3,4)\n",
    "                    start_time=time.time()\n",
    "                    explanation_attention_rollout_list=attentions_to_explanation(attentions, mode='rollout')\n",
    "                    elapsed_time=time.time()-start_time\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'attention_rollout', path_list=paths, elapsed_time_list=[elapsed_time/len(paths) for i in range(len(paths))])\n",
    "\n",
    "                elif explanation_method==\"attention_last\":\n",
    "                    attentions = np.asarray([att.cpu().detach().numpy() for att in classifier_output['self_attentions']]).transpose(1,0,2,3,4)\n",
    "                    start_time=time.time()\n",
    "                    explanation_attention_last_list=attentions_to_explanation(attentions, mode=-1)\n",
    "                    elapsed_time=time.time()-start_time\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'attention_last', path_list=paths, elapsed_time_list=[elapsed_time/len(paths) for i in range(len(paths))])            \n",
    "\n",
    "                elif explanation_method==\"LRP\":\n",
    "                    explanation_lrp_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        explanation_lrp_list.append(np.concatenate([get_lrp_module_explanation(backbone_type=backbone_type,\n",
    "                                                                              original_image=image.squeeze(0),\n",
    "                                                                              class_index=i,\n",
    "                                                                              mode='transformer_attribution').cpu().numpy() for i in range(_config[\"output_dim\"])], axis=0))\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'LRP', path_list=paths, elapsed_time_list=elapsed_time_list)\n",
    "\n",
    "                elif explanation_method==\"gradcam\":\n",
    "                    explanation_gradcam_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        explanation_gradcam = np.concatenate([get_lrp_module_explanation(backbone_type=backbone_type,\n",
    "                                                                                              original_image=image.squeeze(0),\n",
    "                                                                                              class_index=i,\n",
    "                                                                                              mode='attn_gradcam').cpu().numpy() for i in range(_config[\"output_dim\"])], axis=0)\n",
    "                        explanation_gradcam = np.nan_to_num(explanation_gradcam,nan=0)+np.random.uniform(low=0, high=1e-20, size=explanation_gradcam.shape)                    \n",
    "                        explanation_gradcam_list.append(explanation_gradcam)\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'gradcam', path_list=paths, elapsed_time_list=elapsed_time_list)\n",
    "\n",
    "\n",
    "                elif explanation_method==\"gradcamgithub\":\n",
    "                    explanation_gradcamgithub_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        explanation_gradcamgithub = np.concatenate([cam_dict[backbone_type](input_tensor=image.unsqueeze(0).to(next(cam_dict[backbone_type].model.parameters()).device),\n",
    "                                                                                            targets=[ClassifierOutputTarget(i)], resize=False).flatten()[np.newaxis,:] for i in range(_config[\"output_dim\"])], axis=0)\n",
    "                        explanation_gradcamgithub_list.append(explanation_gradcamgithub)\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'gradcamgithub', path_list=paths, elapsed_time_list=elapsed_time_list)\n",
    "\n",
    "                elif explanation_method==\"vanillapixel\":\n",
    "                    explanation_vanillapixel_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        grad=get_vanilla(image, saliency_pixel=saliency_pixel_dict[backbone_type])\n",
    "                        explanation_vanillapixel_list.append(grad[\"attributions_pixel_patchabssum\"].reshape(_config[\"output_dim\"],196).numpy())\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'vanillapixel', path_list=paths, elapsed_time_list=elapsed_time_list)\n",
    "\n",
    "                elif explanation_method==\"vanillaembedding\":\n",
    "                    explanation_vanillaembedding_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:\n",
    "                        start_time=time.time()\n",
    "                        grad=get_vanilla(image, saliency_embedding=saliency_embedding_dict[backbone_type])\n",
    "                        explanation_vanillaembedding_list.append(grad[\"attributions_embedding_abssum\"].reshape(_config[\"output_dim\"],196).numpy())\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'vanillaembedding', path_list=paths, elapsed_time_list=elapsed_time_list)\n",
    "\n",
    "                elif explanation_method==\"sgpixel\":\n",
    "                    explanation_sgpixel_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        grad=get_sg(image, noisetunnel_pixel=noisetunnel_pixel_dict[backbone_type])\n",
    "                        explanation_sgpixel_list.append(grad[\"attributions_pixel_patchabssum\"].reshape(_config[\"output_dim\"],196).numpy())\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'sgpixel', path_list=paths, elapsed_time_list=elapsed_time_list)\n",
    "\n",
    "\n",
    "                elif explanation_method==\"sgembedding\":\n",
    "                    explanation_sgembedding_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        grad=get_sg(image, noisetunnel_embedding=noisetunnel_embedding_dict[backbone_type])\n",
    "                        explanation_sgembedding_list.append(grad[\"attributions_embedding_abssum\"].reshape(_config[\"output_dim\"],196).numpy())\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'sgembedding', path_list=paths, elapsed_time_list=elapsed_time_list)                                \n",
    "\n",
    "                elif explanation_method==\"vargradpixel\":\n",
    "                    explanation_vargradpixel_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        grad=get_vargrad(image, noisetunnel_pixel=noisetunnel_pixel_dict[backbone_type])\n",
    "                        explanation_vargradpixel_list.append(grad[\"attributions_pixel_patchabssum\"].reshape(_config[\"output_dim\"],196).numpy())\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'vargradpixel', path_list=paths, elapsed_time_list=elapsed_time_list)\n",
    "\n",
    "                elif explanation_method==\"vargradembedding\":\n",
    "                    explanation_vargradembedding_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        grad=get_vargrad(image, noisetunnel_embedding=noisetunnel_embedding_dict[backbone_type])\n",
    "                        explanation_vargradembedding_list.append(grad[\"attributions_embedding_abssum\"].reshape(_config[\"output_dim\"],196).numpy())\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'vargradembedding', path_list=paths, elapsed_time_list=elapsed_time_list)                                \n",
    "\n",
    "                elif explanation_method==\"igpixel\":\n",
    "                    explanation_igpixel_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        grad=get_ig(image, ig_pixel=ig_pixel_dict[backbone_type])\n",
    "                        explanation_igpixel_list.append(grad[\"attributions_pixel_patchsum\"].reshape(_config[\"output_dim\"],196).numpy())\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'igpixel', path_list=paths, elapsed_time_list=elapsed_time_list)\n",
    "\n",
    "                elif explanation_method==\"igembedding\":\n",
    "                    explanation_igembedding_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        grad=get_ig(image, ig_embedding=ig_embedding_dict[backbone_type])\n",
    "                        explanation_igembedding_list.append(grad[\"attributions_embedding_sum\"].reshape(_config[\"output_dim\"],196).numpy())\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'igembedding', path_list=paths, elapsed_time_list=elapsed_time_list)                \n",
    "\n",
    "                elif explanation_method==\"leaveoneoutclassifier\":\n",
    "                    explanation_leaveoneoutclassifier_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        explanation_leaveoneoutclassifier = leave_one_out(classifier=classifier_dict_[backbone_type], image=image).reshape(_config[\"output_dim\"], 196)\n",
    "                        explanation_leaveoneoutclassifier_list.append(explanation_leaveoneoutclassifier)\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'leaveoneoutclassifier', path_list=paths, elapsed_time_list=elapsed_time_list)\n",
    "\n",
    "                elif explanation_method==\"leaveoneoutsurrogate\":\n",
    "                    explanation_leaveoneoutsurrogate_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        explanation_leaveoneoutsurrogate = leave_one_out(surrogate=surrogate_dict[backbone_type][\"pre-softmax\"], image=image).reshape(_config[\"output_dim\"], 196)\n",
    "                        explanation_leaveoneoutsurrogate_list.append(explanation_leaveoneoutsurrogate)\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'leaveoneoutsurrogate', path_list=paths, elapsed_time_list=elapsed_time_list)                \n",
    "\n",
    "                elif explanation_method==\"riseclassifier\":\n",
    "                    explanation_riseclassifier_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        explanation_riseclassifier = rise(classifier=classifier_dict_[backbone_type], image=image, N=2000, include_prob=0.5).reshape(_config[\"output_dim\"], 196)\n",
    "                        explanation_riseclassifier_list.append(explanation_riseclassifier)\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'riseclassifier', path_list=paths, elapsed_time_list=elapsed_time_list)\n",
    "\n",
    "                elif explanation_method==\"risesurrogate\":\n",
    "                    explanation_risesurrogate_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        explanation_risesurrogate = rise(surrogate=surrogate_dict[backbone_type][\"pre-softmax\"], image=image, N=2000, include_prob=0.5).reshape(_config[\"output_dim\"], 196)\n",
    "                        explanation_risesurrogate_list.append(explanation_risesurrogate)\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'risesurrogate', path_list=paths, elapsed_time_list=elapsed_time_list)                \n",
    "                    \n",
    "                elif explanation_method==\"ours\":\n",
    "                    start_time=time.time()\n",
    "                    explainer_dict[backbone_type].eval()\n",
    "                    with torch.no_grad():\n",
    "                        explanation_ours = explainer_dict[backbone_type](images.to(explainer_dict[backbone_type].device))[0].detach().cpu().numpy().transpose(0, 2, 1)\n",
    "                    elapsed_time=time.time()-start_time\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'ours', path_list=paths, elapsed_time_list=[elapsed_time/len(paths) for i in range(len(paths))])                                \n",
    "                else:\n",
    "                    raise\n",
    "\n",
    "        try:\n",
    "            for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "                for explanation_method, elapsedtime_save_dict_backbone_method in elapsedtime_save_dict[backbone_type].items():\n",
    "                    if explanation_method not in updated_signal_list:\n",
    "                        continue\n",
    "                    elapsedtime_save_dict_path=f'results/8_elapsedtime/{_config[\"datasets\"]}/{backbone_type}_{explanation_method}_{dataset_split}.pickle'\n",
    "\n",
    "                    if os.path.isfile(elapsedtime_save_dict_path):\n",
    "                        try:\n",
    "                            with open(elapsedtime_save_dict_path, 'rb') as f:\n",
    "                                elapsedtime_save_dict_loaded=pickle.load(f)\n",
    "                        except:\n",
    "                            elapsedtime_save_dict_loaded={}\n",
    "                    else:\n",
    "                        elapsedtime_save_dict_loaded={}\n",
    "\n",
    "                    len_original=len(elapsedtime_save_dict_backbone_method)            \n",
    "                    len_loaded=len(elapsedtime_save_dict_loaded)\n",
    "                    elapsedtime_save_dict_backbone_method.update(elapsedtime_save_dict_loaded)\n",
    "                    len_updated=len(elapsedtime_save_dict_backbone_method)\n",
    "\n",
    "                    print(f'{explanation_method:24}  {len_original:6}   + {len_loaded:6}   -> {len_updated:6}')\n",
    "\n",
    "                    with open(elapsedtime_save_dict_path, \"wb\") as f:\n",
    "                        pickle.dump(elapsedtime_save_dict_backbone_method, f)\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "            \n",
    "            \n",
    "    #         # Get ours\n",
    "    #         start_time=time.time();explainer_dict[backbone_type].eval()\n",
    "    #         with torch.no_grad():\n",
    "    #             values, _, _=explainer_dict[backbone_type](torch.Tensor(image).unsqueeze(0).to(explainer_dict[backbone_type].device))\n",
    "    #             values=values.cpu().numpy().transpose(0,2,1).squeeze(0)\n",
    "    #         values_ours=(values, time.time()-start_time)                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fe2ef4",
   "metadata": {},
   "source": [
    "# 9_estimationerror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3afb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimationerror_sample_path_list=pd.DataFrame(data_loader.dataset.data).groupby(\"label\").apply(lambda x: x.sample(n=10, random_state=42))[\"img_path\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9d02b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if evaluation_stage==\"9_estimationerror\":\n",
    "    for idx, batch in enumerate(tqdm(data_loader)):#, total = int(1000/data_loader.batch_size+0.5) if dataset_split==\"test\" else None)):\n",
    "#         if dataset_split==\"test\":\n",
    "#             if idx>int(1000/data_loader.batch_size+0.5):\n",
    "#                 break\n",
    "        if (idx%parallel_mode[1])!=parallel_mode[0]:\n",
    "            continue\n",
    "            \n",
    "        images=batch['images']\n",
    "        labels=batch['labels']\n",
    "        paths=batch['path']\n",
    "        updated_signal_list=[]\n",
    "        \n",
    "        for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "            # Get classifier output\n",
    "            classifier_dict[backbone_type].eval()\n",
    "            with torch.no_grad():\n",
    "                classifier_output=classifier_dict[backbone_type](images.to(classifier_dict[backbone_type].device),\n",
    "                                                                 output_attentions=True)        \n",
    "            for explanation_method in [\"kernelshapnopair\"]:\n",
    "                data_keys=estimationerror_save_dict[backbone_type][explanation_method].keys()\n",
    "                data_keys=[adapt_path(data_keys_path, paths) for data_keys_path in data_keys]\n",
    "                if all([(path in data_keys) or (path not in estimationerror_sample_path_list) for path in paths]):\n",
    "                    continue\n",
    "                else:\n",
    "                    print(explanation_method,'not exist')\n",
    "                    updated_signal_list.append(explanation_method)\n",
    "                    \n",
    "                if explanation_method==\"ours\":\n",
    "                    start_time=time.time()\n",
    "                    explainer_dict[backbone_type].eval()\n",
    "                    with torch.no_grad():\n",
    "                        explanation_ours = explainer_dict[backbone_type](images.to(explainer_dict[backbone_type].device))[0].detach().cpu().numpy().transpose(0, 2, 1)\n",
    "                    elapsed_time=time.time()-start_time\n",
    "                    estimationerror_save_dict_update(backbone_type, \n",
    "                                                     'ours', \n",
    "                                                     path_list=[path for path, paths in zip(paths, paths) if path in estimationerror_sample_path_list],\n",
    "                                                     estimation_list=[estimation for path, estimation in zip(paths, explanation_ours) if path in estimationerror_sample_path_list],\n",
    "                                                     label_list=[label for path, label in zip(paths, labels.cpu().numpy().tolist()) if path in estimationerror_sample_path_list],\n",
    "                                                     shape=(_config[\"output_dim\"], 196))\n",
    "                    \n",
    "                elif explanation_method==\"kernelshap\":\n",
    "                    path_list=[]\n",
    "                    explanation_kernelshap_list=[]\n",
    "                    label_list=[]\n",
    "                    \n",
    "                    surrogate_SHAP_wrapped_dict[backbone_type].eval()\n",
    "                    for path, image, label in zip(paths, images, labels.cpu().numpy().tolist()):\n",
    "                        if path in estimationerror_sample_path_list:\n",
    "                            start_time=time.time()      \n",
    "                            game = games.PredictionGame_torchimagetensor(surrogate_SHAP_wrapped_dict[backbone_type],\n",
    "                                                                         image)\n",
    "                            \n",
    "                            explanation_kernelshap = shapley.ShapleyRegression(game, \n",
    "                                                                    batch_size=64, \n",
    "                                                                    thresh=0.1,\n",
    "                                                                    variance_batches=60,\n",
    "                                                                    return_all=True)                              \n",
    "                            \n",
    "                            path_list.append(path)\n",
    "                            explanation_kernelshap_list.append(explanation_kernelshap)\n",
    "                            label_list.append(label)\n",
    "                            \n",
    "                    estimationerror_save_dict_update(backbone_type, \n",
    "                                                     'kernelshap', \n",
    "                                                     path_list=path_list, \n",
    "                                                     estimation_list=explanation_kernelshap_list, \n",
    "                                                     label_list=label_list,\n",
    "                                                     shape=None)  \n",
    "                    \n",
    "                elif explanation_method==\"kernelshapnopair\":\n",
    "                    path_list=[]\n",
    "                    explanation_kernelshap_list=[]\n",
    "                    label_list=[]\n",
    "                    \n",
    "                    surrogate_SHAP_wrapped_dict[backbone_type].eval()\n",
    "                    for path, image, label in zip(paths, images, labels.cpu().numpy().tolist()):\n",
    "                        if path in estimationerror_sample_path_list:\n",
    "                            start_time=time.time()      \n",
    "                            game = games.PredictionGame_torchimagetensor(surrogate_SHAP_wrapped_dict[backbone_type],\n",
    "                                                                         image)\n",
    "                            \n",
    "                            explanation_kernelshap = shapley.ShapleyRegression(game, \n",
    "                                                                    batch_size=128, \n",
    "                                                                    detect_convergence=False,\n",
    "                                                                    paired_sampling=False,\n",
    "                                                                    n_samples=200000,\n",
    "                                                                    variance_batches=60,\n",
    "                                                                    return_all=True)                              \n",
    "                            \n",
    "                            path_list.append(path)\n",
    "                            explanation_kernelshap_list.append(explanation_kernelshap)\n",
    "                            label_list.append(label)\n",
    "                            \n",
    "                    estimationerror_save_dict_update(backbone_type, \n",
    "                                                     'kernelshapnopair', \n",
    "                                                     path_list=path_list, \n",
    "                                                     estimation_list=explanation_kernelshap_list, \n",
    "                                                     label_list=label_list,\n",
    "                                                     shape=None)  \n",
    "                    \n",
    "                else:\n",
    "                    raise\n",
    "        \n",
    "        try:\n",
    "            for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "                for explanation_method, estimationerror_save_dict_backbone_method in estimationerror_save_dict[backbone_type].items():\n",
    "\n",
    "                    if explanation_method not in updated_signal_list:\n",
    "                        continue\n",
    "\n",
    "                    estimationerror_save_dict_path=f'results/9_estimationerror/{_config[\"datasets\"]}/{backbone_type}_{explanation_method}_{dataset_split}.pickle'\n",
    "\n",
    "                    if os.path.isfile(estimationerror_save_dict_path):\n",
    "                        try:\n",
    "                            with open(estimationerror_save_dict_path, 'rb') as f:\n",
    "                                estimationerror_save_dict_loaded=pickle.load(f)\n",
    "                        except:\n",
    "                            estimationerror_save_dict_loaded={}\n",
    "                    else:\n",
    "                        estimationerror_save_dict_loaded={}\n",
    "\n",
    "                    len_original=len(estimationerror_save_dict_backbone_method)            \n",
    "                    len_loaded=len(estimationerror_save_dict_loaded)\n",
    "                    estimationerror_save_dict_backbone_method.update(estimationerror_save_dict_loaded)\n",
    "                    len_updated=len(estimationerror_save_dict_backbone_method)\n",
    "\n",
    "                    print(f'{explanation_method:24}  {len_original:6}   + {len_loaded:6}   -> {len_updated:6}')\n",
    "                    \n",
    "                    with open(estimationerror_save_dict_path, \"wb\") as f:\n",
    "                        pickle.dump(estimationerror_save_dict_backbone_method, f)\n",
    "        except:\n",
    "            pass                    \n",
    "            \n",
    "    #         # Get ours\n",
    "    #         start_time=time.time();explainer_dict[backbone_type].eval()\n",
    "    #         with torch.no_grad():\n",
    "    #             values, _, _=explainer_dict[backbone_type](torch.Tensor(image).unsqueeze(0).to(explainer_dict[backbone_type].device))\n",
    "    #             values=values.cpu().numpy().transpose(0,2,1).squeeze(0)\n",
    "    #         values_ours=(values, time.time()-start_time)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c14574",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    for path, data in estimationerror_save_dict[backbone_type][\"kernelshap\"].items():\n",
    "        explanation_save_dict[backbone_type]['kernelshap'][path]={\"explanation\": data['estimation'][0].values.T,\n",
    "                                                                  \"elapsed_time\": np.nan}\n",
    "        print(path, data['estimation'][0].values.T.shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f1edf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    for explanation_method, estimationerror_save_dict_backbone_method in estimationerror_save_dict[backbone_type].items():\n",
    "        print(backbone_type, explanation_method, len(estimationerror_save_dict_backbone_method))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1639263",
   "metadata": {},
   "outputs": [],
   "source": [
    "[elapsed_time/len(paths) for i in range(len(paths))][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4297a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsedtime_save_dict_update??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da58468",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_masked_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529f6928",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86f61e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c839705",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.session.delete();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51e3403",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4b2d9d0",
   "metadata": {},
   "source": [
    "# sensitivity-n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d6d44f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_players=196\n",
    "for idx, batch in enumerate(tqdm(data_loader, total = int(1000/data_loader.batch_size+0.5) if dataset_split==\"test\" else None)):   \n",
    "    if dataset_split==\"test\":\n",
    "        if idx>int(1000/data_loader.batch_size+0.5):\n",
    "            break\n",
    "    if (idx%parallel_mode[1])!=parallel_mode[0]:\n",
    "        continue                                \n",
    "\n",
    "    images=batch['images']\n",
    "    labels=batch['labels']\n",
    "    paths=batch['path']\n",
    "\n",
    "    for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):            \n",
    "        for image, path, label in zip(images, paths, labels):\n",
    "            #path=path.replace('l0.cs.washington.edu','l2lambda.cs.washington.edu')\n",
    "            for num_included_players in [\"all\"]:\n",
    "                print(num_included_players)                \n",
    "                if num_included_players==\"all\":\n",
    "                    prob_all=[]\n",
    "                    mask_all=[]\n",
    "                    for random_iter in range(20):\n",
    "                        image_loaded=image[np.newaxis, :].repeat(50, 1, 1, 1).to(surrogate_dict[backbone_type][\"pre-softmax\"].device)\n",
    "                        mask=generate_mask(num_players=num_players,\n",
    "                                           num_mask_samples=50,\n",
    "                                           paired_mask_samples=False,\n",
    "                                           mode=\"uniform\",\n",
    "                                           random_state=np.random.RandomState(random_iter))\n",
    "\n",
    "                        surrogate_dict[backbone_type][\"pre-softmax\"].eval()\n",
    "                        with torch.no_grad():\n",
    "                            output = surrogate_dict[backbone_type][\"pre-softmax\"](image_loaded,\n",
    "                                                                                  masks=torch.Tensor(mask).to(surrogate_dict[backbone_type][\"pre-softmax\"].device))\n",
    "\n",
    "                        if _config[\"output_dim\"]==1:\n",
    "                            prob=output['logits'].sigmoid().cpu().numpy()\n",
    "                        else:\n",
    "                            prob=output['logits'].softmax(dim=-1).cpu().numpy()   \n",
    "                        prob_all.append(prob)\n",
    "                        mask_all.append(mask)\n",
    "                    prob_all=np.concatenate(prob_all, axis=0)\n",
    "                    mask_all=np.concatenate(mask_all, axis=0)\n",
    "\n",
    "                for explanation_method in explanation_method_to_run:                \n",
    "                    print(explanation_method)\n",
    "                    explanation=explanation_save_dict[backbone_type][explanation_method][adapt_path(path, list(explanation_save_dict[backbone_type][explanation_method].keys())[0])]['explanation']\n",
    "                    explanation=explanation+np.random.RandomState(42).uniform(low=0, high=1e-40, size=explanation.shape)\n",
    "                    explanation_mask=explanation@(mask_all.T)\n",
    "\n",
    "                    if len(explanation.shape)==1:\n",
    "                        correlation = np.array([stats.spearmanr(explanation_mask, prob_all[:, i]).correlation for i in range(prob_all.shape[1])])\n",
    "                        assert correlation.shape==(_config[\"output_dim\"],)\n",
    "\n",
    "                    elif len(explanation.shape)==2:\n",
    "                        #correlation=stats.spearmanr(np.concatenate([explanation_mask, prob_all.T], axis=0), axis=1).correlation\n",
    "                        correlation = np.array([stats.spearmanr(explanation_mask[i], prob_all[:, i]).correlation for i in range(prob_all.shape[1])])\n",
    "                        assert correlation.shape==(_config[\"output_dim\"],)\n",
    "                        \n",
    "                        fig=plt.figure(figsize=(20,5))\n",
    "                        ax=fig.add_subplot(121)\n",
    "                        ax.scatter(explanation_mask[label], prob_all[:,label])#\n",
    "                        ax.set_xlabel(\"sum_explanation\")\n",
    "                        ax.set_ylabel(\"model output\")\n",
    "                        ax.set_title(explanation_method)\n",
    "                        \n",
    "                        ax=fig.add_subplot(122)\n",
    "                        for i in range(prob_all.shape[1]):\n",
    "                            if i>3:\n",
    "                                break\n",
    "                            if i!=label:\n",
    "                                ax.scatter(explanation_mask[i], prob_all[:,i])#, s=4)#\n",
    "                        ax.set_xlabel(\"sum_explanation\")\n",
    "                        ax.set_ylabel(\"model output\")\n",
    "                        ax.set_title(explanation_method)                        \n",
    "                        \n",
    "                        plt.show()\n",
    "                        \n",
    "                    else:\n",
    "                        raise\n",
    "#                     sensitivity_save_dit_update(backbone_type, explanation_method,\n",
    "#                                                  num_included_players=num_included_players,\n",
    "#                                                  path_list=[path], sensitivity_list=[correlation],\n",
    "#                                                  shape=(_config[\"output_dim\"],))     \n",
    "            break\n",
    "\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b35df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure()\n",
    "ax=fig.add_subplot()\n",
    "ax.scatter(explanation_mask[label], prob_all[:,label])#\n",
    "ax.set_xlabel(\"sum_explanation\")\n",
    "ax.set_ylabel(\"model output\")\n",
    "ax.set_title(explanation_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcd97c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb3e1c7e",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a068a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "fig = plt.figure()\n",
    "ax_temp=fig.add_subplot()\n",
    "plt.clf()\n",
    "\n",
    "def visualize_result(x, values, pred=None, vmin_vmax='separate',\n",
    "                     image_labels=['normal','abnormal']*5,\n",
    "                     class_labels=['normal','abnormal']*5):\n",
    "    # colormap\n",
    "    from matplotlib import cm\n",
    "    from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "\n",
    "    color_num = 1000\n",
    "    img_mean = np.array([0.4914, 0.4822, 0.4465])[:, np.newaxis, np.newaxis]\n",
    "    img_std = np.array([0.2023, 0.1994, 0.2010])[:, np.newaxis, np.newaxis]    \n",
    "\n",
    "    if isinstance(vmin_vmax,tuple):\n",
    "        vmin, vmax= vmin_vmax\n",
    "        assert vmin < vmax\n",
    "        if vmin * vmax < 0:\n",
    "            ratio=vmax/(-vmin+vmax)\n",
    "            seismic = cm.get_cmap('seismic', color_num)\n",
    "            newcolors = seismic(np.linspace(0, 1, color_num))\n",
    "            newcolors[:int(color_num*(1-ratio))] = seismic(np.linspace(0, 0.5, int(color_num*(1-ratio))))\n",
    "            newcolors[-int(color_num*ratio)-1:] = seismic(np.linspace(0.5, 1, int(color_num*ratio)+1))\n",
    "            newcmp = ListedColormap(newcolors)\n",
    "        elif vmin > 0:\n",
    "            seismic = cm.get_cmap('seismic', color_num)\n",
    "            newcolors = seismic(np.linspace(0.5, 1, color_num))\n",
    "            newcmp = ListedColormap(newcolors)\n",
    "        else:\n",
    "            raise\n",
    "    elif vmin_vmax==\"separate\":\n",
    "        #seismic = cm.get_cmap('seismic', color_num)\n",
    "        #newcolors = seismic(np.linspace(0.5, 1, color_num))\n",
    "        #newcmp = ListedColormap(newcolors)        \n",
    "        if values.min()>0:\n",
    "            seismic = cm.get_cmap('seismic', color_num)\n",
    "            newcolors = seismic(np.linspace(0.5, 1, color_num))\n",
    "            newcmp = ListedColormap(newcolors)            \n",
    "        else:\n",
    "            seismic = cm.get_cmap('seismic', color_num)\n",
    "            newcolors = seismic(np.linspace(0, 1, color_num))\n",
    "            newcmp = ListedColormap(newcolors)\n",
    "\n",
    "\n",
    "    fig, axes = plt.subplots(values.shape[0], 1+values.shape[1], figsize=(2*(1+values.shape[1]), 2*(values.shape[0]+1)))\n",
    "\n",
    "    assert len(image_labels)==values.shape[0]==(len(image_labels) if pred is None else pred.shape[0])\n",
    "    assert len(class_labels)==values.shape[1]==(len(class_labels) if pred is None else pred.shape[1])\n",
    "    \n",
    "    for row in range(axes.shape[0]):\n",
    "        for col in range(axes.shape[1]):\n",
    "            if col==0: # Image\n",
    "                im = x[row].numpy() * img_std + img_mean # (C, H, W)\n",
    "                im = im.transpose(1, 2, 0).astype(float) # (H, W, C)\n",
    "                im = np.clip(im, a_min=0, a_max=1)\n",
    "\n",
    "                axes[row, 0].imshow(im, vmin=0, vmax=1)\n",
    "                axes[row, 0].set_ylabel('{}'.format(image_labels[row]), fontsize=12)\n",
    "            else: # Explanation\n",
    "                values_select=values[row, col-1]\n",
    "                values_select_min, values_select_max=values_select.min(),values_select.max()\n",
    "\n",
    "                if vmin_vmax==\"separate\":\n",
    "                    if values.min()>0:\n",
    "                        axes[row, col].imshow(values_select, cmap=newcmp,\n",
    "                                              vmin=values_select_min,\n",
    "                                              vmax=values_select_max)\n",
    "                    else:\n",
    "                        axes[row, col].imshow(values_select, cmap=newcmp, \n",
    "                                              vmin=-max([abs(values_select_min),abs(values_select_max)]), \n",
    "                                              vmax=max([abs(values_select_min),abs(values_select_max)]))\n",
    "                else:\n",
    "                    axes[row, col].imshow(values_select, cmap=newcmp, vmin=vmin, vmax=vmax)\n",
    "\n",
    "                if pred is None:\n",
    "                    axes[row, col].set_xlabel('{:.2f}/{:.2f}'.format(values_select_min, values_select_max), fontsize=12)\n",
    "                else:\n",
    "                    axes[row, col].set_xlabel('{:.2f} {:.2f}/{:.2f}'.format(pred[row, col-1], values_select_min, values_select_max), fontsize=12)            \n",
    "\n",
    "                # Class labels\n",
    "                if row == 0:\n",
    "                    axes[row, col].set_title('{}'.format(class_labels[col-1]), fontsize=12)  \n",
    "\n",
    "            axes[row, col].set_xticks([])\n",
    "            axes[row, col].set_yticks([])                           \n",
    "\n",
    "    if vmin_vmax!=\"separate\":\n",
    "        fig = plt.figure(figsize=(5, 0.5))\n",
    "        ax=fig.add_subplot()        \n",
    "        \n",
    "        sns.heatmap([[0,0],[0,0]],\n",
    "                    ax=ax_temp,\n",
    "                    cmap=newcmp,\n",
    "                    vmin=vmin,\n",
    "                    vmax=vmax,\n",
    "                    xticklabels=True,\n",
    "                    linewidths=1,\n",
    "                    linecolor=np.array([220,220,220,256])/256,\n",
    "                    cbar_ax=ax,\n",
    "                    cbar_kws={'fraction':0.1, \"ticks\":np.linspace(vmin, vmax, 5), \"orientation\": \"horizontal\"},\n",
    "                    cbar=True,\n",
    "                    alpha=1,edgecolor='black')#,legend=None)\n",
    "\n",
    "        #plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        fig=plt.figure()\n",
    "        ax=fig.add_subplot()\n",
    "        ax.hist(values.flatten(),bins=np.linspace(vmin, vmax, 20))\n",
    "        ax.set_yscale('log')\n",
    "        print(values.flatten().min(), values.flatten().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b53e07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prob_all_all=[]\n",
    "mask_all_all=[]\n",
    "for idx, image in enumerate(x):\n",
    "    print(idx)\n",
    "    prob_all=[]\n",
    "    mask_all=[]\n",
    "    for random_iter in range(20):\n",
    "        image_loaded=image[np.newaxis, :].repeat(50, 1, 1, 1).to(surrogate_dict[backbone_type][\"pre-softmax\"].device)\n",
    "        mask=generate_mask(num_players=num_players,\n",
    "                           num_mask_samples=50,\n",
    "                           paired_mask_samples=False,\n",
    "                           mode=\"uniform\",\n",
    "                           random_state=np.random.RandomState(random_iter))\n",
    "\n",
    "        surrogate_dict[backbone_type][\"pre-softmax\"].eval()\n",
    "        with torch.no_grad():\n",
    "            output = surrogate_dict[backbone_type][\"pre-softmax\"](image_loaded,\n",
    "                                                                  masks=torch.Tensor(mask).to(surrogate_dict[backbone_type][\"pre-softmax\"].device))\n",
    "\n",
    "        if _config[\"output_dim\"]==1:\n",
    "            prob=output['logits'].sigmoid().cpu().numpy()\n",
    "        else:\n",
    "            prob=output['logits'].softmax(dim=-1).cpu().numpy()   \n",
    "        prob_all.append(prob)\n",
    "        mask_all.append(mask)\n",
    "    prob_all=np.concatenate(prob_all, axis=0)\n",
    "    mask_all=np.concatenate(mask_all, axis=0)\n",
    "    #print(prob_all.shape, mask_all.shape)\n",
    "    prob_all_all.append(prob_all)\n",
    "    mask_all_all.append(mask_all)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0270ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "values.reshape(values.shape[0], values.shape[1], 196)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd15b98b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for idx, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    #get classifier\n",
    "    classifier_dict[backbone_type].eval()\n",
    "    with torch.no_grad():\n",
    "        output = classifier_dict[backbone_type](x.to(next(classifier_dict[backbone_type].parameters()).device), output_attentions=False)\n",
    "        if _config[\"output_dim\"]==1:\n",
    "            pred=output['logits'].detach().sigmoid().cpu().data.numpy()\n",
    "        else:\n",
    "            pred=output['logits'].detach().softmax().cpu().data.numpy()\n",
    "    del output    \n",
    "    \n",
    "    # Get explanation (modified_LRP)\n",
    "    values=np.concatenate([np.concatenate([get_lrp_module_explanation(backbone_type, image.squeeze(0), class_index=i, mode='transformer_attribution').cpu() for i in range(_config[\"output_dim\"])], axis=0)[np.newaxis,:] for image in x])\n",
    "    values=values.reshape(values.shape[0], values.shape[1], 14, 14)\n",
    "    \n",
    "    visualize_result(x, pred=pred, values=values,\n",
    "                     class_labels=y_labels[1:2] if _config[\"output_dim\"]==1 else y_labels,\n",
    "                     image_labels=y_labels,\n",
    "                     vmin_vmax=\"separate\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    values_lrp=values.reshape(values.shape[0], values.shape[1], 196)\n",
    "    \n",
    "    \n",
    "    explainer_dict[backbone_type].eval()\n",
    "    with torch.no_grad():\n",
    "        values=explainer_dict[backbone_type](x.to(next(explainer_dict[backbone_type].parameters()).device))    \n",
    "        values=values[0].reshape(-1, _config[\"output_dim\"], 14, 14).cpu().numpy()\n",
    "        \n",
    "    visualize_result(x, pred=pred, values=values,\n",
    "                     class_labels=y_labels[1:2] if _config[\"output_dim\"]==1 else y_labels,\n",
    "                     image_labels=y_labels,\n",
    "                     vmin_vmax=(-0.2, 0.2))    \n",
    "    \n",
    "    values_ours=values.reshape(values.shape[0], values.shape[1], 196)\n",
    "    \n",
    "    #visualize_result(x, pred=pred.repeat(10, axis=1), values=values.repeat(10, axis=1), separate=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vitshapley",
   "language": "python",
   "name": "vitshapley"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
