{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a263fe4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/homes/gws/chanwkim/vit-shapley/notebooks\n",
      "/homes/gws/chanwkim/vit-shapley\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir('../')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f348f8",
   "metadata": {},
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "548e5e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from collections import OrderedDict\n",
    "import copy\n",
    "import pickle\n",
    "import time\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from vit_shapley.datamodules.ImageNette_datamodule import ImageNetteDataModule\n",
    "from vit_shapley.datamodules.MURA_datamodule import MURADataModule\n",
    "from vit_shapley.datamodules.Pet_datamodule import PetDataModule\n",
    "\n",
    "from vit_shapley.modules.classifier import Classifier\n",
    "from vit_shapley.modules.classifier_masked import ClassifierMasked\n",
    "from vit_shapley.modules.surrogate import Surrogate\n",
    "from vit_shapley.modules.explainer import Explainer\n",
    "\n",
    "from vit_shapley.config import ex\n",
    "from vit_shapley.config import config, env_chanwkim, dataset_ImageNette, dataset_MURA, dataset_Pet\n",
    "_config=config()\n",
    "\n",
    "dataset_split=\"test\"\n",
    "parallel_mode = (3, 4)\n",
    "backbone_to_use=[\"vit_large_patch16_224\"]\n",
    "_config.update(dataset_ImageNette())\n",
    "evaluation_stage=[\"1_classifier_evaluate\",\n",
    "                  \"2_surrogate_evaluate\",\n",
    "                  \"3_explanation_generate\",\n",
    "                  \"4_insert_delete\",\n",
    "                  \"5_sensitivity\",\n",
    "                  \"6_noretraining\",\n",
    "                  \"7_classifiermasked\",\n",
    "                  \"8_elapsedtime\",\n",
    "                  \"9_estimationerror\"][3]\n",
    "\n",
    "_config.update(env_chanwkim()); _config.update({'gpus_classifier':[4,],\n",
    "                                                'gpus_surrogate':[4,],\n",
    "                                                'gpus_explainer':[4,]})\n",
    "\n",
    "_config.update({'classifier_backbone_type': None,\n",
    "                'classifier_download_weight': False,\n",
    "                'classifier_load_path': None})\n",
    "_config.update({'classifier_masked_mask_location': \"pre-softmax\",\n",
    "                'classifier_enable_pos_embed': True,\n",
    "                })\n",
    "_config.update({'surrogate_mask_location': \"pre-softmax\"})\n",
    "_config.update({'surrogate_backbone_type': None,\n",
    "                'surrogate_download_weight': False,\n",
    "                'surrogate_load_path': None})\n",
    "_config.update({'explainer_num_mask_samples': 2,\n",
    "                'explainer_paired_mask_samples': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc42bddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37ml0.cs.washington.edu         \u001b[m  Tue Feb 28 17:24:39 2023  \u001b[1m\u001b[30m515.65.01\u001b[m\r\n",
      "\u001b[36m[0]\u001b[m \u001b[34mNVIDIA GeForce RTX 2080 Ti\u001b[m |\u001b[31m 32'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m  247\u001b[m / \u001b[33m11264\u001b[m MB |\r\n",
      "\u001b[36m[1]\u001b[m \u001b[34mNVIDIA GeForce RTX 2080 Ti\u001b[m |\u001b[1m\u001b[31m 80'C\u001b[m, \u001b[1m\u001b[32m100 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m 7830\u001b[m / \u001b[33m11264\u001b[m MB | \u001b[1m\u001b[30mchanwkim\u001b[m(\u001b[33m7583M\u001b[m)\r\n",
      "\u001b[36m[2]\u001b[m \u001b[34mNVIDIA GeForce RTX 2080 Ti\u001b[m |\u001b[1m\u001b[31m 78'C\u001b[m, \u001b[1m\u001b[32m100 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m 8988\u001b[m / \u001b[33m11264\u001b[m MB | \u001b[1m\u001b[30mchanwkim\u001b[m(\u001b[33m8741M\u001b[m)\r\n",
      "\u001b[36m[3]\u001b[m \u001b[34mNVIDIA GeForce RTX 2080 Ti\u001b[m |\u001b[1m\u001b[31m 61'C\u001b[m, \u001b[1m\u001b[32m 99 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m 7830\u001b[m / \u001b[33m11264\u001b[m MB | \u001b[1m\u001b[30mchanwkim\u001b[m(\u001b[33m7583M\u001b[m)\r\n",
      "\u001b[36m[4]\u001b[m \u001b[34mNVIDIA GeForce RTX 2080 Ti\u001b[m |\u001b[31m 43'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m 1395\u001b[m / \u001b[33m11264\u001b[m MB |\r\n",
      "\u001b[36m[5]\u001b[m \u001b[34mNVIDIA GeForce RTX 2080 Ti\u001b[m |\u001b[1m\u001b[31m 50'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m 3594\u001b[m / \u001b[33m11264\u001b[m MB |\r\n",
      "\u001b[36m[6]\u001b[m \u001b[34mNVIDIA GeForce RTX 2080 Ti\u001b[m |\u001b[31m 35'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m10788\u001b[m / \u001b[33m11264\u001b[m MB | \u001b[1m\u001b[30mchanwkim\u001b[m(\u001b[33m10541M\u001b[m)\r\n",
      "\u001b[36m[7]\u001b[m \u001b[34mNVIDIA GeForce RTX 2080 Ti\u001b[m |\u001b[1m\u001b[31m 51'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m 7162\u001b[m / \u001b[33m11264\u001b[m MB |\r\n"
     ]
    }
   ],
   "source": [
    "!gpustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6058196f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _config[\"datasets\"]==\"ImageNette\":\n",
    "    backbone_type_config_dict_=OrderedDict({\n",
    "        \"vit_small_patch16_224\":{\n",
    "            \"classifier_path\": \"results/wandb_transformer_interpretability_project/1yndrggu/checkpoints/epoch=14-step=2204.ckpt\",\n",
    "            \"classifier_masked_path\": \"results/wandb_transformer_interpretability_project/fdm70w72/checkpoints/epoch=19-step=2939.ckpt\",\n",
    "            \"surrogate_path\":{\n",
    "                \"pre-softmax\": \"results/wandb_transformer_interpretability_project/3lfv4nmn/checkpoints/epoch=39-step=5879.ckpt\"\n",
    "            },\n",
    "            \"explainer_path\":\"results/wandb_transformer_interpretability_project/3biv2s85/checkpoints/epoch=60-step=9027.ckpt\"\n",
    "\n",
    "        },\n",
    "        \"deit_small_patch16_224\":{\n",
    "        },\n",
    "        \"vit_base_patch16_224\":{\n",
    "            \"classifier_path\": \"results/wandb_transformer_interpretability_project/2rq1issn/checkpoints/epoch=16-step=2498.ckpt\",\n",
    "            \"classifier_masked_path\": \"results/wandb_transformer_interpretability_project/x59c992d/checkpoints/epoch=21-step=3233.ckpt\",\n",
    "            \"surrogate_path\":{\n",
    "                #\"original\": \"results/wandb_transformer_interpretability_project/2rq1issn/checkpoints/epoch=16-step=2498.ckpt\",\n",
    "                \"pre-softmax\": \"results/wandb_transformer_interpretability_project/3i6zzjnp/checkpoints/epoch=38-step=5732.ckpt\",\n",
    "                #\"zero-input\": \"results/wandb_transformer_interpretability_project/zyybgzcm/checkpoints/epoch=22-step=3380.ckpt\",\n",
    "                #\"zero-embedding\": \"results/wandb_transformer_interpretability_project/1gi5gmrm/checkpoints/epoch=36-step=5438.ckpt\"\n",
    "                },\n",
    "            \"explainer_path\": \"results/wandb_transformer_interpretability_project/3ty85eft/checkpoints/epoch=83-step=12431.ckpt\"\n",
    "        },\n",
    "        \"deit_base_patch16_224\":{\n",
    "\n",
    "        },\n",
    "        \"vit_large_patch16_224\":{\n",
    "            \"classifier_path\": \"results/wandb_transformer_interpretability_project/1at36lgp/checkpoints/epoch=2-step=440.ckpt\",\n",
    "            \"surrogate_path\": {\n",
    "                \"pre-softmax\":\"results/wandb_transformer_interpretability_project/284sm0on/checkpoints/epoch=37-step=5585.ckpt\"\n",
    "            },\n",
    "            \"explainer_path\":\"results/wandb_transformer_interpretability_project/34gbowsg/checkpoints/epoch=91-step=13615.ckpt\"\n",
    "        }\n",
    "    })    \n",
    "elif _config[\"datasets\"]==\"MURA\":\n",
    "    backbone_type_config_dict_=OrderedDict({\n",
    "        \"vit_small_patch16_224\":{\n",
    "\n",
    "        },\n",
    "        \"deit_small_patch16_224\":{\n",
    "        },\n",
    "        \"vit_base_patch16_224\":{\n",
    "            \"classifier_path\":\"results/wandb_transformer_interpretability_project/1u2xgwks/checkpoints/epoch=15-step=8255.ckpt\",\n",
    "            \"surrogate_path\": {\n",
    "                #\"original\": \"results/wandb_transformer_interpretability_project/1u2xgwks/checkpoints/epoch=15-step=8255.ckpt\",\n",
    "                \"pre-softmax\": \"results/wandb_transformer_interpretability_project/22ompjqu/checkpoints/epoch=47-step=24767.ckpt\",\n",
    "                #\"zero-input\": \"results/wandb_transformer_interpretability_project/2z2qs6t0/checkpoints/epoch=44-step=23219.ckpt\",\n",
    "                #\"zero-embedding\": \"results/wandb_transformer_interpretability_project/1pbmwnvb/checkpoints/epoch=45-step=23735.ckpt\"\n",
    "            },\n",
    "            \"explainer_path\":\"results/wandb_transformer_interpretability_project/1dmhcwej/checkpoints/epoch=93-step=48597.ckpt\"\n",
    "        },\n",
    "        \"deit_base_patch16_224\":{\n",
    "\n",
    "        }\n",
    "    })\n",
    "    \n",
    "elif _config[\"datasets\"]==\"Pet\":\n",
    "    backbone_type_config_dict_=OrderedDict({\n",
    "        \"vit_small_patch16_224\":{\n",
    "\n",
    "        },\n",
    "        \"deit_small_patch16_224\":{\n",
    "        },\n",
    "        \"vit_base_patch16_224\":{\n",
    "            \"classifier_path\":\"results/wandb_transformer_interpretability_project/3g01rci7/checkpoints/epoch=9-step=909.ckpt\",\n",
    "            \"surrogate_path\": {\n",
    "                \"original\": \"results/wandb_transformer_interpretability_project/3g01rci7/checkpoints/epoch=9-step=909.ckpt\",\n",
    "                \"pre-softmax\": \"results/wandb_transformer_interpretability_project/146vf465/checkpoints/epoch=40-step=3730.ckpt\",\n",
    "                #\"zero-input\": \"results/wandb_transformer_interpretability_project/2z2qs6t0/checkpoints/epoch=44-step=23219.ckpt\",\n",
    "                #\"zero-embedding\": \"results/wandb_transformer_interpretability_project/1pbmwnvb/checkpoints/epoch=45-step=23735.ckpt\"\n",
    "            },\n",
    "            \"explainer_path\":\"results/wandb_transformer_interpretability_project/2oq7lhr7/checkpoints/epoch=85-step=7911.ckpt\"\n",
    "        },\n",
    "        \"deit_base_patch16_224\":{\n",
    "\n",
    "        }\n",
    "    })    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8426be8",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54d6d39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(num_players: int, num_mask_samples: int or None = None, paired_mask_samples: bool = True,\n",
    "                  mode: str = 'uniform', random_state: np.random.RandomState or None = None) -> np.array:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        num_players: the number of players in the coalitional game\n",
    "        num_mask_samples: the number of masks to generate\n",
    "        paired_mask_samples: if True, the generated masks are pairs of x and 1-x.\n",
    "        mode: the distribution that the number of masked features follows. ('uniform' or 'shapley')\n",
    "        random_state: random generator\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor of shape\n",
    "        (num_masks, num_players) if num_masks is int\n",
    "        (num_players) if num_masks is None\n",
    "\n",
    "    \"\"\"\n",
    "    random_state = random_state or np.random\n",
    "\n",
    "    num_samples_ = num_mask_samples or 1\n",
    "\n",
    "    if paired_mask_samples:\n",
    "        assert num_samples_ % 2 == 0, \"'num_samples' must be a multiple of 2 if 'paired' is True\"\n",
    "        num_samples_ = num_samples_ // 2\n",
    "    else:\n",
    "        num_samples_ = num_samples_\n",
    "\n",
    "    if mode == 'uniform':\n",
    "        masks = (random_state.rand(num_samples_, num_players) > random_state.rand(num_samples_, 1)).astype('int')\n",
    "    elif mode == 'shapley':\n",
    "        probs = 1 / (np.arange(1, num_players) * (num_players - np.arange(1, num_players)))\n",
    "        probs = probs / probs.sum()\n",
    "        masks = (random_state.rand(num_samples_, num_players) > 1 / num_players * random_state.choice(\n",
    "            np.arange(num_players - 1), p=probs, size=[num_samples_, 1])).astype('int')\n",
    "    else:\n",
    "        raise ValueError(\"'mode' must be 'random' or 'shapley'\")\n",
    "\n",
    "    if paired_mask_samples:\n",
    "        masks = np.stack([masks, 1 - masks], axis=1).reshape(num_samples_ * 2, num_players)\n",
    "\n",
    "    if num_mask_samples is None:\n",
    "        masks = masks.squeeze(0)\n",
    "        return masks  # (num_masks)\n",
    "    else:\n",
    "        return masks  # (num_samples, num_masks)\n",
    "\n",
    "def set_datamodule(datasets,\n",
    "                   dataset_location,\n",
    "                   explanation_location_train,\n",
    "                   explanation_mask_amount_train,\n",
    "                   explanation_mask_ascending_train,\n",
    "                   \n",
    "                   explanation_location_val,\n",
    "                   explanation_mask_amount_val,\n",
    "                   explanation_mask_ascending_val,                   \n",
    "                   \n",
    "                   explanation_location_test,\n",
    "                   explanation_mask_amount_test,\n",
    "                   explanation_mask_ascending_test,                   \n",
    "                   \n",
    "                   transforms_train,\n",
    "                   transforms_val,\n",
    "                   transforms_test,\n",
    "                   num_workers,\n",
    "                   per_gpu_batch_size,\n",
    "                   test_data_split):\n",
    "    dataset_parameters = {\n",
    "        \"dataset_location\": dataset_location,\n",
    "        \"explanation_location_train\": explanation_location_train,\n",
    "        \"explanation_mask_amount_train\": explanation_mask_amount_train,\n",
    "        \"explanation_mask_ascending_train\": explanation_mask_ascending_train,\n",
    "        \n",
    "        \"explanation_location_val\": explanation_location_val,\n",
    "        \"explanation_mask_amount_val\": explanation_mask_amount_val,\n",
    "        \"explanation_mask_ascending_val\": explanation_mask_ascending_val,\n",
    "        \n",
    "        \"explanation_location_test\": explanation_location_test,\n",
    "        \"explanation_mask_amount_test\": explanation_mask_amount_test,\n",
    "        \"explanation_mask_ascending_test\": explanation_mask_ascending_test,        \n",
    "        \n",
    "        \"transforms_train\": transforms_train,\n",
    "        \"transforms_val\": transforms_val,\n",
    "        \"transforms_test\": transforms_test,\n",
    "        \"num_workers\": num_workers,\n",
    "        \"per_gpu_batch_size\": per_gpu_batch_size,\n",
    "        \"test_data_split\": test_data_split\n",
    "    }\n",
    "\n",
    "    if datasets == \"CheXpert\":\n",
    "        datamodule = CheXpertDataModule(**dataset_parameters)\n",
    "    elif datasets == \"MIMIC\":\n",
    "        datamodule = MIMICDataModule(**dataset_parameters)\n",
    "    elif datasets == \"MURA\":\n",
    "        datamodule = MURADataModule(**dataset_parameters)\n",
    "    elif datasets == \"ImageNette\":\n",
    "        datamodule = ImageNetteDataModule(**dataset_parameters)\n",
    "    else:\n",
    "        ValueError(\"Invalid 'datasets' configuration\")\n",
    "    return datamodule\n",
    "\n",
    "datamodule = set_datamodule(datasets=_config[\"datasets\"],\n",
    "                            dataset_location=_config[\"dataset_location\"],\n",
    "\n",
    "                            explanation_location_train=_config[\"explanation_location_train\"],\n",
    "                            explanation_mask_amount_train=_config[\"explanation_mask_amount_train\"],\n",
    "                            explanation_mask_ascending_train=_config[\"explanation_mask_ascending_train\"],\n",
    "\n",
    "                            explanation_location_val=_config[\"explanation_location_val\"],\n",
    "                            explanation_mask_amount_val=_config[\"explanation_mask_amount_val\"],\n",
    "                            explanation_mask_ascending_val=_config[\"explanation_mask_ascending_val\"],\n",
    "\n",
    "                            explanation_location_test=_config[\"explanation_location_test\"],\n",
    "                            explanation_mask_amount_test=_config[\"explanation_mask_amount_test\"],\n",
    "                            explanation_mask_ascending_test=_config[\"explanation_mask_ascending_test\"],                            \n",
    "\n",
    "                            transforms_train=_config[\"transforms_train\"],\n",
    "                            transforms_val=_config[\"transforms_val\"],\n",
    "                            transforms_test=_config[\"transforms_test\"],\n",
    "                            num_workers=_config[\"num_workers\"],\n",
    "                            per_gpu_batch_size=_config[\"per_gpu_batch_size\"],\n",
    "                            test_data_split=_config[\"test_data_split\"])\n",
    "\n",
    "# The batch for training classifier consists of images and labels, but the batch for training explainer consists of images and masks.\n",
    "# The masks are generated to follow the Shapley distribution.\n",
    "\"\"\"\n",
    "original_getitem = copy.deepcopy(datamodule.dataset_cls.__getitem__)\n",
    "def __getitem__(self, idx):\n",
    "    if self.split == 'train':\n",
    "        masks = generate_mask(num_players=surrogate.num_players,\n",
    "                              num_mask_samples=_config[\"explainer_num_mask_samples\"],\n",
    "                              paired_mask_samples=_config[\"explainer_paired_mask_samples\"], mode='shapley')\n",
    "    elif self.split == 'val' or self.split == 'test':\n",
    "        # get cached if available\n",
    "        if not hasattr(self, \"masks_cached\"):\n",
    "            self.masks_cached = {}\n",
    "        masks = self.masks_cached.setdefault(idx, generate_mask(num_players=surrogate.num_players,\n",
    "                                                                num_mask_samples=_config[\n",
    "                                                                    \"explainer_num_mask_samples\"],\n",
    "                                                                paired_mask_samples=_config[\n",
    "                                                                    \"explainer_paired_mask_samples\"],\n",
    "                                                                mode='shapley'))\n",
    "    else:\n",
    "        raise ValueError(\"'split' variable must be train, val or test.\")\n",
    "    return {\"images\": original_getitem(self, idx)[\"images\"],\n",
    "            \"labels\": original_getitem(self, idx)[\"labels\"],\n",
    "            \"masks\": masks}\n",
    "datamodule.dataset_cls.__getitem__ = __getitem__\n",
    "\"\"\"\n",
    "\n",
    "datamodule.set_train_dataset()\n",
    "datamodule.set_val_dataset()\n",
    "datamodule.set_test_dataset()\n",
    "\n",
    "train_dataset=datamodule.train_dataset\n",
    "val_dataset=datamodule.val_dataset\n",
    "test_dataset=datamodule.test_dataset\n",
    "\n",
    "dset=test_dataset\n",
    "\n",
    "if dataset_split==\"train\":\n",
    "    dset.data = train_dataset.data\n",
    "elif dataset_split==\"val\":\n",
    "    dset.data = val_dataset.data     \n",
    "elif dataset_split==\"test\": \n",
    "    dset.data = test_dataset.data\n",
    "else:\n",
    "    raise\n",
    "\n",
    "labels = np.array([i['label'] for i in dset.data])\n",
    "num_classes = labels.max() + 1\n",
    "\n",
    "images_idx_list = [np.where(labels == category)[0] for category in range(num_classes)]\n",
    "\n",
    "images_idx=[]\n",
    "for classidx in range(4,4+int(10/len(images_idx_list))):\n",
    "    images_idx+=[category_idx[classidx] for category_idx in images_idx_list]\n",
    "\n",
    "xy=[dset[idx] for idx in images_idx]\n",
    "x, y = zip(*[(i['images'], i['labels']) for i in xy])\n",
    "x = torch.stack(x)\n",
    "y_labels=[dset.labels[i] for i in y]\n",
    "\n",
    "\n",
    "if _config[\"datasets\"]==\"ImageNette\":\n",
    "    label_name_list=['Cassette player', \n",
    "                      'Garbage truck', \n",
    "                      'Tench', \n",
    "                      'English springer', \n",
    "                      'Church', \n",
    "                      'Parachute', \n",
    "                      'French horn', \n",
    "                      'Chain saw', \n",
    "                      'Golf ball', \n",
    "                      'Gas pump']\n",
    "    \n",
    "elif _config[\"datasets\"]==\"MURA\":\n",
    "    label_name_list=[\"Normal\", \"Abnormal\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7380660e",
   "metadata": {},
   "source": [
    "# Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbc25f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vit_large_patch16_224\n"
     ]
    }
   ],
   "source": [
    "backbone_type_config_dict = OrderedDict()\n",
    "for idx, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict_.items()):\n",
    "    if backbone_type in backbone_to_use:\n",
    "        print(backbone_type)\n",
    "        backbone_type_config_dict[backbone_type]=backbone_type_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13aafbd0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "classifier_dict = OrderedDict()\n",
    "\n",
    "for idx, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    classifier_dict[backbone_type] = Classifier(backbone_type=backbone_type,\n",
    "                                               download_weight=_config['classifier_download_weight'],\n",
    "                                               load_path=backbone_type_config[\"classifier_path\"],\n",
    "                                               target_type=_config[\"target_type\"],\n",
    "                                               output_dim=_config[\"output_dim\"],\n",
    "                                               enable_pos_embed=_config[\"classifier_enable_pos_embed\"],\n",
    "\n",
    "                                               checkpoint_metric=None,\n",
    "                                               loss_weight=None,\n",
    "                                               optim_type=None,\n",
    "                                               learning_rate=None,\n",
    "                                               weight_decay=None,\n",
    "                                               decay_power=None,\n",
    "                                               warmup_steps=None).to(_config[\"gpus_classifier\"][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3016a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_dict_ = OrderedDict()\n",
    "\n",
    "for idx, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    classifier_dict_[backbone_type] = Surrogate(mask_location=_config[\"surrogate_mask_location\"],\n",
    "                                                   backbone_type=backbone_type,\n",
    "                                                   download_weight=_config['classifier_download_weight'],\n",
    "                                                   load_path=backbone_type_config[\"classifier_path\"],\n",
    "                                                   target_type=_config[\"target_type\"],\n",
    "                                                   output_dim=_config[\"output_dim\"],\n",
    "\n",
    "                                                   target_model=None,\n",
    "                                                   checkpoint_metric=None,\n",
    "                                                   optim_type=None,\n",
    "                                                   learning_rate=None,\n",
    "                                                   weight_decay=None,\n",
    "                                                   decay_power=None,\n",
    "                                                   warmup_steps=None).to(_config[\"gpus_classifier\"][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05bf9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluation_stage==\"7_classifiermasked\":\n",
    "    classifier_masked_dict = OrderedDict()\n",
    "\n",
    "    for idx, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "        classifier_masked_dict[backbone_type] = ClassifierMasked(mask_location=_config[\"classifier_masked_mask_location\"],\n",
    "                                                               backbone_type=backbone_type,\n",
    "                                                               download_weight=_config['classifier_download_weight'],\n",
    "                                                               load_path=backbone_type_config[\"classifier_masked_path\"],\n",
    "                                                               target_type=_config[\"target_type\"],\n",
    "                                                               output_dim=_config[\"output_dim\"],\n",
    "\n",
    "                                                               checkpoint_metric=None,\n",
    "                                                               loss_weight=None,                                                             \n",
    "                                                               optim_type=None,\n",
    "                                                               learning_rate=None,\n",
    "                                                               weight_decay=None,\n",
    "                                                               decay_power=None,\n",
    "                                                               warmup_steps=None).to(_config[\"gpus_classifier\"][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1a294aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate_dict = OrderedDict()\n",
    "\n",
    "for idx, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    mask_method_dict = OrderedDict()\n",
    "    for mask_location in backbone_type_config[\"surrogate_path\"].keys():\n",
    "        mask_method_dict[mask_location] = Surrogate(mask_location=mask_location if mask_location!=\"original\" else \"pre-softmax\",\n",
    "                                          backbone_type=backbone_type,\n",
    "                                          download_weight=_config['surrogate_download_weight'],\n",
    "                                          load_path=backbone_type_config[\"surrogate_path\"][mask_location],\n",
    "                                          target_type=_config[\"target_type\"],\n",
    "                                          output_dim=_config[\"output_dim\"],\n",
    "\n",
    "                                          target_model=None,\n",
    "                                          checkpoint_metric=None,\n",
    "                                          optim_type=None,\n",
    "                                          learning_rate=None,\n",
    "                                          weight_decay=None,\n",
    "                                          decay_power=None,\n",
    "                                          warmup_steps=None).to(_config[\"gpus_surrogate\"][idx])\n",
    "    surrogate_dict[backbone_type]=mask_method_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33775c46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77a2343",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8414a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vitmedical.modules.explainer import Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0c7ce1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9276fe1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_config.update({'explainer_normalization': \"additive\",\n",
    "                'explainer_activation': \"tanh\",\n",
    "                'explainer_link': 'sigmoid' if _config[\"output_dim\"]==1 else 'softmax',\n",
    "                'explainer_head_num_attention_blocks': 1,\n",
    "                'explainer_head_include_cls': True,\n",
    "                'explainer_head_num_mlp_layers': 3,\n",
    "                'explainer_head_mlp_layer_ratio': 4,\n",
    "                'explainer_residual': [],\n",
    "                'explainer_freeze_backbone': \"all\"})\n",
    "\n",
    "explainer_dict = OrderedDict()\n",
    "for idx, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    explainer_dict[backbone_type] = Explainer(normalization=_config[\"explainer_normalization\"],\n",
    "                                              normalization_class=_config[\"explainer_normalization_class\"],\n",
    "                                              activation=_config[\"explainer_activation\"],\n",
    "                                              surrogate=surrogate_dict[backbone_type][\"pre-softmax\"],\n",
    "                                              link=_config[\"explainer_link\"],\n",
    "                                              backbone_type=backbone_type,\n",
    "                                              download_weight=False,\n",
    "                                              residual=_config['explainer_residual'],\n",
    "                                              load_path=backbone_type_config[\"explainer_path\"],\n",
    "                                              target_type=_config[\"target_type\"],\n",
    "                                              output_dim=_config[\"output_dim\"],\n",
    "\n",
    "                                              explainer_head_num_attention_blocks=_config[\"explainer_head_num_attention_blocks\"],\n",
    "                                              explainer_head_include_cls=_config[\"explainer_head_include_cls\"],\n",
    "                                              explainer_head_num_mlp_layers=_config[\"explainer_head_num_mlp_layers\"],\n",
    "                                              explainer_head_mlp_layer_ratio=_config[\"explainer_head_mlp_layer_ratio\"],\n",
    "                                              explainer_norm=_config[\"explainer_norm\"],\n",
    "\n",
    "                                              efficiency_lambda=_config[\"explainer_efficiency_lambda\"],\n",
    "                                              efficiency_class_lambda=_config[\"explainer_efficiency_class_lambda\"],\n",
    "                                              freeze_backbone=_config[\"explainer_freeze_backbone\"],\n",
    "\n",
    "                                              checkpoint_metric=_config[\"checkpoint_metric\"],\n",
    "                                              optim_type=_config[\"optim_type\"],\n",
    "                                              learning_rate=_config[\"learning_rate\"],\n",
    "                                              weight_decay=_config[\"weight_decay\"],\n",
    "                                              decay_power=_config[\"decay_power\"],\n",
    "                                              warmup_steps=_config[\"warmup_steps\"]).to(_config[\"gpus_explainer\"][idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e7f6fb",
   "metadata": {},
   "source": [
    "# explanation methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02048ea8",
   "metadata": {},
   "source": [
    "## attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12d90ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_joint_attention(attentions, add_residual=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        attentions: (num_batches, num_layers, num_players, num_players)\n",
    "        add_residual: bool\n",
    "    Returns:\n",
    "        joint_attentions: (num_batches, num_layers, num_players, num_players)\n",
    "    \"\"\"\n",
    "    assert len(attentions.shape)==4\n",
    "    if add_residual:\n",
    "        residual_att = np.eye(attentions.shape[2])[np.newaxis, np.newaxis, ...]\n",
    "        aug_attentions = attentions + residual_att\n",
    "        aug_attentions = aug_attentions / aug_attentions.sum(axis=-1)[..., np.newaxis]\n",
    "    else:\n",
    "        aug_attentions =  attentions\n",
    "    \n",
    "    joint_attentions = np.zeros(aug_attentions.shape) # (num_batches, num_layers, num_players, num_players)\n",
    "\n",
    "    for i in np.arange(joint_attentions.shape[1]):\n",
    "        if i==0:\n",
    "            joint_attentions[:,i] = aug_attentions[:,0]\n",
    "        else:\n",
    "            joint_attentions[:,i] = (aug_attentions[:,i] @ joint_attentions[:,i-1])\n",
    "    return joint_attentions\n",
    "\n",
    "\n",
    "def attentions_to_explanation(attentions, mode='rollout'):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        attentions: (num_batches, num_layers, num_heads, num_players, num_players)\n",
    "    \"\"\"\n",
    "    assert len(attentions.shape)==5 and attentions.shape[-1]==attentions.shape[-2]\n",
    "    attentions_nohead = attentions.sum(axis=2)/attentions.shape[2] # (num_batch, num_layers, num_players, num_players)\n",
    "    attentions_nohead_residual = attentions_nohead + np.eye(attentions_nohead.shape[2])[np.newaxis, np.newaxis, ...] # (num_batch, num_layers, num_players, num_players)\n",
    "    attentions_nohead_residual_normalized = attentions_nohead_residual / attentions_nohead_residual.sum(axis=-1)[..., np.newaxis] # (num_batch, num_layers, num_players, num_players)\n",
    "    \n",
    "    if isinstance(mode, int):\n",
    "        return attentions_nohead_residual_normalized[:, mode, 0, 1:]\n",
    "    elif mode=='raw':\n",
    "        return attentions_nohead_residual_normalized[:, -1, 0, 1:]\n",
    "    elif mode=='rollout':\n",
    "        attentions_nohead_residual_normalized_rollout = compute_joint_attention(attentions_nohead_residual_normalized,\n",
    "                                                                                add_residual=False)\n",
    "        return attentions_nohead_residual_normalized_rollout[:, -1, 0, 1:]\n",
    "#explanation_to_mask(attention_rollout).argmin(axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220f301b",
   "metadata": {},
   "source": [
    "## lrp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8045007",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.transformer_explainability.baselines.ViT.ViT_new as ViT_new\n",
    "import utils.transformer_explainability.baselines.ViT.ViT_LRP as ViT_LRP\n",
    "import utils.transformer_explainability.baselines.ViT.ViT_orig_LRP as ViT_orig_LRP\n",
    "\n",
    "from utils.transformer_explainability.baselines.ViT.ViT_explanation_generator import Baselines, LRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e055b095",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "baselines_dict = OrderedDict()\n",
    "lrp_dict = OrderedDict()\n",
    "orig_lrp_dict = OrderedDict()\n",
    "\n",
    "\n",
    "for idx, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    checkpoint = torch.load(backbone_type_config[\"classifier_path\"], map_location=\"cpu\")\n",
    "    checkpoint[\"state_dict\"]=OrderedDict([(k.replace('backbone.',''), v) for k, v in checkpoint[\"state_dict\"].items()])\n",
    "    state_dict = checkpoint[\"state_dict\"]\n",
    "    \n",
    "    model = getattr(ViT_new, backbone_type)(num_classes=_config[\"output_dim\"]).to(_config[\"gpus_classifier\"][idx])\n",
    "    ret = model.load_state_dict(state_dict, strict=False)\n",
    "    print(f\"Model parameters were updated from a checkpoint file {backbone_type_config['classifier_path']}\")\n",
    "    print(f\"Unmatched parameters - missing_keys:    {ret.missing_keys}\")\n",
    "    print(f\"Unmatched parameters - unexpected_keys: {ret.unexpected_keys}\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output1=model(x.to(next(model.parameters()).device))\n",
    "        output2=classifier_dict[backbone_type](x.to(next(model.parameters()).device))['logits']\n",
    "        assert torch.allclose(output1,output2,atol=1e-03)\n",
    "    baselines = Baselines(model)\n",
    "    baselines_dict[backbone_type]=baselines        \n",
    "    \n",
    "    model_LRP=getattr(ViT_LRP, backbone_type)(num_classes=_config[\"output_dim\"]).to(_config[\"gpus_classifier\"][idx])\n",
    "    ret = model_LRP.load_state_dict(state_dict, strict=False)\n",
    "    print(f\"Model parameters were updated from a checkpoint file {backbone_type_config['classifier_path']}\")\n",
    "    print(f\"Unmatched parameters - missing_keys:    {ret.missing_keys}\")\n",
    "    print(f\"Unmatched parameters - unexpected_keys: {ret.unexpected_keys}\")\n",
    "    model_LRP.eval()      \n",
    "    lrp = LRP(model_LRP)\n",
    "    lrp_dict[backbone_type]=lrp\n",
    "    \n",
    "#     model_orig_LRP=getattr(ViT_orig_LRP, backbone_type)(num_classes=_config[\"output_dim\"]).to(_config[\"gpus_classifier\"][idx])\n",
    "#     ret = model_orig_LRP.load_state_dict(state_dict, strict=False)\n",
    "#     print(f\"Model parameters were updated from a checkpoint file {backbone_type_config['classifier_path']}\")\n",
    "#     print(f\"Unmatched parameters - missing_keys:    {ret.missing_keys}\")\n",
    "#     print(f\"Unmatched parameters - unexpected_keys: {ret.unexpected_keys}\")\n",
    "#     model_orig_LRP.eval()    \n",
    "#     orig_lrp = LRP(model_orig_LRP)  \n",
    "#     orig_lrp_dict[backbone_type]=orig_lrp\n",
    "    \n",
    "    \n",
    "def get_lrp_module_explanation(backbone_type, original_image, class_index=None, mode='transformer_attribution'):\n",
    "    if mode==\"transformer_attribution\": # ours\n",
    "        transformer_attribution = lrp_dict[backbone_type].generate_LRP(original_image.unsqueeze(0).to(next(lrp_dict[backbone_type].model.parameters()).device), method=\"transformer_attribution\", index=class_index).detach()\n",
    "    elif mode==\"rollout\": # rollout\n",
    "        transformer_attribution = baselines_dict[backbone_type].generate_rollout(original_image.unsqueeze(0).to(next(baselines_dict[backbone_type].model.parameters()).device), start_layer=1).detach()\n",
    "    elif mode==\"attn_last_layer\": # raw-attention\n",
    "        transformer_attribution = lrp_dict[backbone_type].generate_LRP(original_image.unsqueeze(0).to(next(lrp_dict[backbone_type].model.parameters()).device), method=\"last_layer_attn\", index=class_index).detach()\n",
    "    elif mode == 'attn_gradcam': # GradCAM\n",
    "        transformer_attribution = baselines_dict[backbone_type].generate_cam_attn(original_image.unsqueeze(0).to(next(baselines_dict[backbone_type].model.parameters()).device), index=class_index).detach()\n",
    "        transformer_attribution = transformer_attribution.reshape(1,-1)\n",
    "        #transformer_attribution=torch.nan_to_num(transformer_attribution,nan=0)\n",
    "        #transformer_attribution+=torch.rand(size=transformer_attribution.shape, device=transformer_attribution.device)*1e-20        \n",
    "        #transformer_attribution = (transformer_attribution - transformer_attribution.min()) / (transformer_attribution.max() - transformer_attribution.min())\n",
    "    elif mode == 'full_lrp':\n",
    "        transformer_attribution = orig_lrp_dict[backbone_type].generate_LRP(original_image.unsqueeze(0).to(next(orig_lrp_dict[backbone_type].model.parameters()).device), method=\"full\", index=class_index).detach()\n",
    "        #transformer_attribution = (transformer_attribution - transformer_attribution.min()) / (transformer_attribution.max() - transformer_attribution.min())\n",
    "    elif mode == 'lrp_last_layer':\n",
    "        transformer_attribution = orig_lrp_dict[backbone_type].generate_LRP(original_image.unsqueeze(0).to(next(orig_lrp_dict[backbone_type].model.parameters()).device), method=\"last_layer\", index=class_index).detach()\n",
    "        #transformer_attribution = (transformer_attribution - transformer_attribution.min()) / (transformer_attribution.max() - transformer_attribution.min())\n",
    "    #print(transformer_attribution.max(), transformer_attribution.min())\n",
    "    #print(transformer_attribution.shape)\n",
    "    return transformer_attribution    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40064fa3",
   "metadata": {},
   "source": [
    "## CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb8e73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.pytorch_grad_cam import GradCAM\n",
    "from utils.pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "\n",
    "def reshape_transform(tensor, height=14, width=14):\n",
    "    result = tensor[:, 1 :  , :].reshape(tensor.size(0),\n",
    "        height, width, tensor.size(2))\n",
    "\n",
    "    # Bring the channels to the first dimension,\n",
    "    # like in CNNs.\n",
    "    result = result.transpose(2, 3).transpose(1, 2)\n",
    "    return result\n",
    "\n",
    "class WrapperLogits(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model=model\n",
    "\n",
    "    def forward(self, images):\n",
    "        x = self.model(images)\n",
    "        return x['logits']\n",
    "\n",
    "cam_dict = OrderedDict()\n",
    "for backbone_type, backbone_type_config in backbone_type_config_dict.items():\n",
    "    cam_dict[backbone_type] = GradCAM(model=WrapperLogits(classifier_dict[backbone_type]),\n",
    "                                      target_layers=[classifier_dict[backbone_type].backbone.blocks[-1].norm1],\n",
    "                                      reshape_transform=reshape_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e2b912",
   "metadata": {},
   "source": [
    "## Gradient-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bc9f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import IntegratedGradients, InputXGradient, Saliency, NoiseTunnel\n",
    "import torch.nn as nn\n",
    "\n",
    "class FromPixel(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model=model\n",
    "\n",
    "    def forward(self, images):\n",
    "        x = self.model.backbone.patch_embed(images)\n",
    "        x = self.model.backbone.forward_features(x)['x']\n",
    "        logits = self.model.head(x)\n",
    "        \n",
    "        if _config[\"output_dim\"]==1:\n",
    "            return logits.sigmoid()\n",
    "        else:\n",
    "            return logits.softmax(dim=-1)\n",
    "    \n",
    "class FromEmbedding(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model=model\n",
    "\n",
    "    def forward(self, embedding):\n",
    "        x = self.model.backbone.forward_features(embedding)['x']\n",
    "        logits = self.model.head(x)\n",
    "        \n",
    "        if _config[\"output_dim\"]==1:\n",
    "            return logits.sigmoid()\n",
    "        else:\n",
    "            return logits.softmax(dim=-1)\n",
    "\n",
    "#Classifier Wrapping    \n",
    "classifier_pixel_dict = OrderedDict()\n",
    "classifier_embedding_dict = OrderedDict()\n",
    "\n",
    "for idx, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    classifier_pixel_dict[backbone_type]=FromPixel(classifier_dict_[backbone_type])\n",
    "    classifier_embedding_dict[backbone_type]=FromEmbedding(classifier_dict_[backbone_type])\n",
    "\n",
    "#Vanilla\n",
    "saliency_pixel_dict = OrderedDict()\n",
    "saliency_embedding_dict = OrderedDict()\n",
    "\n",
    "for idx, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    saliency_pixel_dict[backbone_type] = Saliency(classifier_pixel_dict[backbone_type])\n",
    "    saliency_embedding_dict[backbone_type] = Saliency(classifier_embedding_dict[backbone_type])      \n",
    "\n",
    "#NoiseTunnel\n",
    "noisetunnel_pixel_dict = OrderedDict()\n",
    "noisetunnel_embedding_dict = OrderedDict()\n",
    "\n",
    "for idx, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    noisetunnel_pixel_dict[backbone_type] = NoiseTunnel(saliency_pixel_dict[backbone_type])\n",
    "    noisetunnel_embedding_dict[backbone_type] = NoiseTunnel(saliency_embedding_dict[backbone_type])      \n",
    "\n",
    "#IntegratedGradients    \n",
    "ig_pixel_dict = OrderedDict()\n",
    "ig_embedding_dict = OrderedDict()\n",
    "\n",
    "for idx, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    ig_pixel_dict[backbone_type] = IntegratedGradients(classifier_pixel_dict[backbone_type])\n",
    "    ig_embedding_dict[backbone_type] = IntegratedGradients(classifier_embedding_dict[backbone_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb4e500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attributions_pixel_process(attributions_pixel):\n",
    "    attributions_pixel_sum = attributions_pixel.sum(axis=-3)\n",
    "    attributions_pixel_abssum = attributions_pixel.abs().sum(axis=-3)\n",
    "    attributions_pixel_patchsum = F.conv2d(attributions_pixel,\n",
    "                                           weight=torch.ones(size=(1, 3, 16, 16),\n",
    "                                                             dtype=attributions_pixel.dtype,\n",
    "                                                             device=attributions_pixel.device),\n",
    "                                           stride=16).squeeze(axis=1)#.flatten(1, 2)\n",
    "    attributions_pixel_pathabssum = F.conv2d(attributions_pixel.abs(),\n",
    "                                             weight=torch.ones(size=(1, 3, 16, 16),\n",
    "                                                               dtype=attributions_pixel.dtype,\n",
    "                                                               device=attributions_pixel.device),\n",
    "                                             stride=16).squeeze(axis=1)#.flatten(1, 2) \n",
    "    \n",
    "    return {'attributions_pixel_sum': attributions_pixel_sum.detach().cpu(),# makes sense? (but cannot used for benchmarking)\n",
    "            'attributions_pixel_abssum': attributions_pixel_abssum.detach().cpu(),# makes sense (but cannot used for benchmarking)\n",
    "            'attributions_pixel_patchsum': attributions_pixel_patchsum.detach().cpu(),  # makes sense?\n",
    "            'attributions_pixel_patchabssum': attributions_pixel_pathabssum.detach().cpu()  # makes sense    \n",
    "           }\n",
    "    \n",
    "    \n",
    "def attributions_embedding_process(attributions_embedding):\n",
    "    attributions_embedding_sum = attributions_embedding.sum(axis=-1)\n",
    "    attributions_embedding_abssum = attributions_embedding.abs().sum(axis=-1)\n",
    "    return {'attributions_embedding_sum': attributions_embedding_sum.detach().cpu(), # makes sense?\n",
    "            'attributions_embedding_abssum': attributions_embedding_abssum.detach().cpu() # makes sense\n",
    "           }  \n",
    "\n",
    "def get_vanilla(image, saliency_pixel=None, saliency_embedding=None):\n",
    "    result={}\n",
    "    with torch.no_grad():\n",
    "        if saliency_pixel is not None:\n",
    "            attributions_pixel = [saliency_pixel.attribute(inputs=image.unsqueeze(0).to(next(saliency_pixel.forward_func.parameters()).device), \n",
    "                                                           target=i) for i in range(_config[\"output_dim\"])]\n",
    "\n",
    "            attributions_pixel = torch.concat(attributions_pixel)\n",
    "            result.update(attributions_pixel_process(attributions_pixel))\n",
    "            \n",
    "        if saliency_embedding is not None:\n",
    "            attributions_embedding = [saliency_embedding.attribute(inputs=saliency_embedding.forward_func.model.backbone.patch_embed(image.unsqueeze(0).to(next(saliency_embedding.forward_func.parameters()).device)).detach(),\n",
    "                                                                   target=i) for i in range(_config[\"output_dim\"])]\n",
    "\n",
    "            attributions_embedding = torch.concat(attributions_embedding)\n",
    "            result.update(attributions_embedding_process(attributions_embedding))\n",
    "        \n",
    "    return result\n",
    "\n",
    "def get_sg(image, noisetunnel_pixel=None, noisetunnel_embedding=None):\n",
    "    result={}    \n",
    "    with torch.no_grad():\n",
    "        if noisetunnel_pixel is not None:\n",
    "            attributions_pixel = [noisetunnel_pixel.attribute(inputs=image.unsqueeze(0).to(next(noisetunnel_pixel.forward_func.parameters()).device),\n",
    "                                                              nt_type='smoothgrad',\n",
    "                                                              nt_samples=10,\n",
    "                                                              target=i) for i in range(_config[\"output_dim\"])]\n",
    "\n",
    "            attributions_pixel = torch.concat(attributions_pixel)\n",
    "            result.update(attributions_pixel_process(attributions_pixel))\n",
    "\n",
    "        if noisetunnel_embedding is not None:\n",
    "            attributions_embedding  = [noisetunnel_embedding.attribute(inputs=noisetunnel_embedding.forward_func.model.backbone.patch_embed(image.unsqueeze(0).to(next(noisetunnel_embedding.forward_func.parameters()).device)).detach(),\n",
    "                                                                       nt_type='smoothgrad',\n",
    "                                                                       nt_samples=10,            \n",
    "                                                                       target=i) for i in range(_config[\"output_dim\"])]\n",
    "\n",
    "            attributions_embedding = torch.concat(attributions_embedding)\n",
    "            result.update(attributions_embedding_process(attributions_embedding))\n",
    "\n",
    "        \n",
    "    return result\n",
    "\n",
    "def get_vargrad(image, noisetunnel_pixel=None, noisetunnel_embedding=None):\n",
    "    result={}    \n",
    "    with torch.no_grad():\n",
    "        if noisetunnel_pixel is not None:\n",
    "            attributions_pixel = [noisetunnel_pixel.attribute(inputs=image.unsqueeze(0).to(next(noisetunnel_pixel.forward_func.parameters()).device),\n",
    "                                                              nt_type='vargrad',\n",
    "                                                              nt_samples=10,\n",
    "                                                              target=i) for i in range(_config[\"output_dim\"])]\n",
    "\n",
    "            attributions_pixel = torch.concat(attributions_pixel)\n",
    "            result.update(attributions_pixel_process(attributions_pixel))   \n",
    "\n",
    "        if noisetunnel_embedding is not None:\n",
    "            attributions_embedding  = [noisetunnel_embedding.attribute(inputs=noisetunnel_embedding.forward_func.model.backbone.patch_embed(image.unsqueeze(0).to(next(noisetunnel_embedding.forward_func.parameters()).device)).detach(),\n",
    "                                                                       nt_type='vargrad',\n",
    "                                                                       nt_samples=10,            \n",
    "                                                                       target=i) for i in range(_config[\"output_dim\"])]\n",
    "\n",
    "            attributions_embedding = torch.concat(attributions_embedding)\n",
    "            result.update(attributions_embedding_process(attributions_embedding))            \n",
    "\n",
    "        \n",
    "    return result\n",
    "\n",
    "def get_ig(image, ig_pixel=None, ig_embedding=None):\n",
    "    result={}    \n",
    "    with torch.no_grad():\n",
    "        if ig_pixel is not None:\n",
    "            attributions_pixel = [ig_pixel.attribute(inputs=image.unsqueeze(0).to(next(ig_pixel.forward_func.parameters()).device),\n",
    "                                                                            baselines=torch.zeros_like(image).unsqueeze(0).to(next(ig_pixel.forward_func.parameters()).device),\n",
    "                                                                            target=i,\n",
    "                                                                            n_steps=10) for i in range(_config[\"output_dim\"])]\n",
    "\n",
    "            attributions_pixel = torch.concat(attributions_pixel)\n",
    "            result.update(attributions_pixel_process(attributions_pixel))           \n",
    "\n",
    "        if ig_embedding is not None:\n",
    "            attributions_embedding = [ig_embedding.attribute(inputs=ig_embedding.forward_func.model.backbone.patch_embed(image.unsqueeze(0).to(next(ig_embedding.forward_func.parameters()).device)).detach(),\n",
    "                                                                                           baselines=ig_embedding.forward_func.model.backbone.patch_embed(torch.zeros_like(image).unsqueeze(0).to(next(ig_embedding.forward_func.parameters()).device)).detach(),\n",
    "                                                                                           target=i,\n",
    "                                                                                           n_steps=10) for i in range(_config[\"output_dim\"])]\n",
    "\n",
    "            attributions_embedding = torch.concat(attributions_embedding)\n",
    "            result.update(attributions_embedding_process(attributions_embedding))          \n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24d0766",
   "metadata": {},
   "source": [
    "## leave-one-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232428ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_one_out(image, surrogate=None, classifier=None):\n",
    "    with torch.no_grad():\n",
    "        mask=torch.cat([torch.ones(1, 196) ,1-torch.eye(196)])\n",
    "        if surrogate is not None:\n",
    "            out=surrogate(image.unsqueeze(0).repeat(196+1, 1, 1, 1).to(surrogate.device), \n",
    "                          masks=mask.to(surrogate.device))\n",
    "        elif classifier is not None:\n",
    "            mask_scaled = torch.repeat_interleave(torch.repeat_interleave(mask.reshape(-1, 14, 14), 16, dim=2), 16, dim=1)\n",
    "            image_masked=image * mask_scaled.unsqueeze(1)\n",
    "            \n",
    "            if classifier.__class__==Classifier:\n",
    "                out=classifier(image_masked.to(classifier.device))\n",
    "            elif classifier.__class__==Surrogate:\n",
    "                out=classifier(image_masked.to(classifier.device),\n",
    "                              masks=torch.ones((len(image_masked),196)))\n",
    "            else:\n",
    "                raise\n",
    "            \n",
    "        if _config[\"output_dim\"]==1:\n",
    "            prob=out['logits'].sigmoid().detach().cpu().numpy()\n",
    "        else:\n",
    "            prob=out['logits'].softmax(dim=-1).detach().cpu().numpy()    \n",
    "        \n",
    "        result=prob[0:1]-prob[1:]\n",
    "\n",
    "    return result.transpose(1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef518bd",
   "metadata": {},
   "source": [
    "# RISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f988c172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rise(image, surrogate=None, classifier=None, include_prob=0.5, N=2000):\n",
    "    assert (surrogate is None) != (classifier is None)\n",
    "    \n",
    "    prob_list=[]\n",
    "    mask_list=[]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(N//100):\n",
    "            mask=torch.rand(100, 196)<include_prob\n",
    "            if surrogate is not None:\n",
    "                out=surrogate(image.unsqueeze(0).repeat(100, 1, 1, 1).to(surrogate.device), \n",
    "                              masks=(mask).to(surrogate.device))\n",
    "            elif classifier is not None:\n",
    "                mask_scaled = torch.repeat_interleave(torch.repeat_interleave(mask.reshape(-1, 14, 14), 16, dim=2), 16, dim=1)\n",
    "                image_masked = image * mask_scaled.unsqueeze(1)\n",
    "                del mask_scaled\n",
    "                if classifier.__class__==Classifier:\n",
    "                    out=classifier(image_masked.to(classifier.device))\n",
    "                elif classifier.__class__==Surrogate:\n",
    "                    out=classifier(image_masked.to(classifier.device),\n",
    "                                  masks=torch.ones_like(mask))\n",
    "                else:\n",
    "                    raise\n",
    "                #out=surrogate_dict[backbone_type](image_masked.to(surrogate_dict[backbone_type].device), \n",
    "                #             masks=torch.ones((100,196)).to(surrogate_dict[backbone_type].device))                \n",
    "            else:\n",
    "                raise\n",
    "            \n",
    "            if _config[\"output_dim\"]==1:\n",
    "                prob=out['logits'].sigmoid().detach().cpu().numpy()\n",
    "            else:\n",
    "                prob=out['logits'].softmax(dim=-1).detach().cpu().numpy()    \n",
    "            \n",
    "            del out\n",
    "            prob_list.append(prob)\n",
    "            mask_list.append(mask.numpy())\n",
    "            del mask\n",
    "            \n",
    "            \n",
    "    prob_list_array=np.concatenate(prob_list) # (num_trials, num_classes)\n",
    "    mask_list_array=np.concatenate(mask_list) # (num_trials, num_players)\n",
    "\n",
    "    result = (prob_list_array.T @ mask_list_array) # (num_classes, num_players)\n",
    "    result = result/mask_list_array.sum(axis=0)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e1e499",
   "metadata": {},
   "source": [
    "# KernelSHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fb0a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.shapreg import removal, games, shapley\n",
    "\n",
    "class SurrogateSHAPWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model=model\n",
    "        if _config[\"output_dim\"]==1:\n",
    "            self.activation=nn.Sigmoid()\n",
    "        else:\n",
    "            self.activation=nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        images, mask = x\n",
    "        mask = mask.squeeze(1).flatten(1)\n",
    "        out=self.model(images, mask)['logits']\n",
    "        out=self.activation(out)\n",
    "        return out\n",
    "\n",
    "surrogate_SHAP_wrapped_dict = OrderedDict()\n",
    "\n",
    "for idx, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    surrogate_SHAP_wrapped_dict[backbone_type]=SurrogateSHAPWrapper(surrogate_dict[backbone_type][\"pre-softmax\"])    \n",
    "\n",
    "def get_shap(surrogate_SHAP_wrapped, x, batch_size=64, thresh=0.2, variance_batches=60):\n",
    "    game = games.PredictionGame_torchimagetensor(surrogate_SHAP_wrapped, x)\n",
    "    explanation = shapley.ShapleyRegression(game, batch_size=batch_size, thresh=thresh, variance_batches=variance_batches)\n",
    "    return explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3c4f5f",
   "metadata": {},
   "source": [
    "# save_dict_setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9cee727",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation_save_dict={}\n",
    "for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    explanation_save_dict_backbone={\"random\":{},\n",
    "                                    \"attention_rollout\":{},\n",
    "                                    \"attention_last\":{},\n",
    "                                    \"LRP\":{},\n",
    "                                    \"gradcam\":{},\n",
    "                                    \"gradcamgithub\": {},\n",
    "                                    \"vanillapixel\": {},\n",
    "                                    \"vanillaembedding\": {},\n",
    "                                    \"sgpixel\": {},\n",
    "                                    \"sgembedding\": {},\n",
    "                                    \"vargradpixel\": {},\n",
    "                                    \"vargradembedding\": {},               \n",
    "                                    \"igpixel\": {},\n",
    "                                    \"igembedding\": {},\n",
    "                                    \"leaveoneoutclassifier\": {},\n",
    "                                    \"leaveoneoutsurrogate\": {},\n",
    "                                    \"riseclassifier\": {},\n",
    "                                    \"risesurrogate\": {},\n",
    "                                    \"ours\": {},\n",
    "                                    \"kernelshap\": {}\n",
    "                                    }\n",
    "    explanation_save_dict[backbone_type]=explanation_save_dict_backbone\n",
    "    \n",
    "def explanation_save_dict_update(backbone_type, explanation_method,\n",
    "                                 path_list, explanation_list, elapsed_time_list, \n",
    "                                 shape=None):\n",
    "    explanation_save_dict_backbone_method=explanation_save_dict[backbone_type][explanation_method]\n",
    "        \n",
    "    assert len(path_list) == len(explanation_list) == len(elapsed_time_list)\n",
    "    \n",
    "    for explanation, path, elapsed_time in zip(explanation_list, path_list, elapsed_time_list):\n",
    "        assert type(explanation)==np.ndarray\n",
    "        assert type(path)==str\n",
    "        assert type(elapsed_time)==float\n",
    "        if shape is not None:\n",
    "            assert explanation.shape==shape\n",
    "        explanation_save_dict_backbone_method[path]={\"explanation\": explanation.astype(float),\n",
    "                                                     \"elapsed_time\": elapsed_time}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79d38051",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random                         0   +    924   ->    924\n",
      "attention_rollout              0   +    924   ->    924\n",
      "attention_last                 0   +    923   ->    923\n",
      "LRP                            0   +    924   ->    924\n",
      "gradcam                        0   +    924   ->    924\n",
      "gradcamgithub                  0   +    924   ->    924\n",
      "vanillapixel                   0   +    924   ->    924\n",
      "vanillaembedding               0   +    924   ->    924\n",
      "sgpixel                        0   +    924   ->    924\n",
      "sgembedding                    0   +    924   ->    924\n",
      "vargradpixel                   0   +    924   ->    924\n",
      "vargradembedding               0   +    924   ->    924\n",
      "igpixel                        0   +    924   ->    924\n",
      "igembedding                    0   +    924   ->    924\n",
      "leaveoneoutclassifier          0   +    924   ->    924\n",
      "leaveoneoutsurrogate           0   +      0   ->      0\n",
      "riseclassifier                 0   +    924   ->    924\n",
      "risesurrogate                  0   +      0   ->      0\n",
      "ours                           0   +    924   ->    924\n",
      "kernelshap                     0   +      0   ->      0\n"
     ]
    }
   ],
   "source": [
    "for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    for explanation_method, explanation_save_dict_backbone_method in explanation_save_dict[backbone_type].items():\n",
    "        try:\n",
    "            explanation_save_dict_path=f'results/3_explanation_generate/{_config[\"datasets\"]}/{backbone_type}_{explanation_method}_{dataset_split}.pickle'\n",
    "\n",
    "            if os.path.isfile(explanation_save_dict_path):\n",
    "                with open(explanation_save_dict_path, 'rb') as f:\n",
    "                    explanation_save_dict_loaded=pickle.load(f)\n",
    "            else:\n",
    "                explanation_save_dict_loaded={}\n",
    "\n",
    "            len_original=len(explanation_save_dict_backbone_method)            \n",
    "            len_loaded=len(explanation_save_dict_loaded)\n",
    "            explanation_save_dict_backbone_method.update(explanation_save_dict_loaded)\n",
    "            len_updated=len(explanation_save_dict_backbone_method)\n",
    "\n",
    "            print(f'{explanation_method:24}  {len_original:6}   + {len_loaded:6}   -> {len_updated:6}') \n",
    "        except:\n",
    "            print('aa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a028c2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "insertdelete_save_dict={}\n",
    "for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    insertdelete_save_dict_backbone={\"random\":{},\n",
    "                                     \"attention_rollout\":{},\n",
    "                                     \"attention_last\":{},\n",
    "                                     \"LRP\":{},\n",
    "                                     \"gradcam\":{},\n",
    "                                     \"gradcamgithub\": {},\n",
    "                                     \"vanillapixel\": {},\n",
    "                                     \"vanillaembedding\": {},\n",
    "                                     \"sgpixel\": {},\n",
    "                                     \"sgembedding\": {},\n",
    "                                     \"vargradpixel\": {},\n",
    "                                     \"vargradembedding\": {},               \n",
    "                                     \"igpixel\": {},\n",
    "                                     \"igembedding\": {},\n",
    "                                     \"leaveoneoutclassifier\": {},\n",
    "                                     \"leaveoneoutsurrogate\": {},\n",
    "                                     \"riseclassifier\": {},\n",
    "                                     \"risesurrogate\": {},\n",
    "                                     \"ours\": {},\n",
    "                                     \"kernelshap\":{}\n",
    "                                    }\n",
    "    insertdelete_save_dict[backbone_type]=insertdelete_save_dict_backbone\n",
    "    \n",
    "def insertdelete_save_dict_update(backbone_type, explanation_method,\n",
    "                                 path_list, insert_list, delete_list,\n",
    "                                 shape=None):\n",
    "    insertdelete_save_dict_backbone_method=insertdelete_save_dict[backbone_type][explanation_method]\n",
    "        \n",
    "    assert len(path_list) == len(insert_list) == len(delete_list)\n",
    "    \n",
    "    for insert, delete, path in zip(insert_list, delete_list, path_list):\n",
    "        assert type(insert)==np.ndarray\n",
    "        assert type(delete)==np.ndarray\n",
    "        assert type(path)==str\n",
    "        if shape is not None:\n",
    "            assert insert.shape==shape\n",
    "            assert delete.shape==shape\n",
    "        insertdelete_save_dict_backbone_method[path]={\"insert\": insert.astype(float),\n",
    "                                                      \"delete\": delete.astype(float)\n",
    "                                                      }    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34817dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random                         0   +    677   ->    677\n",
      "attention_rollout              0   +    677   ->    677\n",
      "attention_last                 0   +    677   ->    677\n",
      "LRP                            0   +    677   ->    677\n",
      "gradcam                        0   +    677   ->    677\n",
      "gradcamgithub                  0   +    677   ->    677\n",
      "vanillapixel                   0   +    677   ->    677\n",
      "vanillaembedding               0   +    677   ->    677\n",
      "sgpixel                        0   +    677   ->    677\n",
      "sgembedding                    0   +    677   ->    677\n",
      "vargradpixel                   0   +    677   ->    677\n",
      "vargradembedding               0   +    677   ->    677\n",
      "igpixel                        0   +    677   ->    677\n",
      "igembedding                    0   +    677   ->    677\n",
      "leaveoneoutclassifier          0   +    677   ->    677\n",
      "leaveoneoutsurrogate           0   +      0   ->      0\n",
      "riseclassifier                 0   +    677   ->    677\n",
      "risesurrogate                  0   +      0   ->      0\n",
      "ours                           0   +    677   ->    677\n",
      "kernelshap                     0   +      0   ->      0\n"
     ]
    }
   ],
   "source": [
    "for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    for explanation_method, insertdelete_save_dict_backbone_method in insertdelete_save_dict[backbone_type].items():\n",
    "        insertdelete_save_dict_path=f'results/4_insert_delete/{_config[\"datasets\"]}/{backbone_type}_{explanation_method}_{dataset_split}.pickle'\n",
    "\n",
    "        if os.path.isfile(insertdelete_save_dict_path):\n",
    "            with open(insertdelete_save_dict_path, 'rb') as f:\n",
    "                insertdelete_save_dict_loaded=pickle.load(f)\n",
    "        else:\n",
    "            insertdelete_save_dict_loaded={}\n",
    "            \n",
    "        len_original=len(insertdelete_save_dict_backbone_method)            \n",
    "        len_loaded=len(insertdelete_save_dict_loaded)\n",
    "        insertdelete_save_dict_backbone_method.update(insertdelete_save_dict_loaded)\n",
    "        len_updated=len(insertdelete_save_dict_backbone_method)\n",
    "            \n",
    "        print(f'{explanation_method:24}  {len_original:6}   + {len_loaded:6}   -> {len_updated:6}')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7a38937",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity_save_dit={}\n",
    "for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    sensitivity_save_dit_backbone={\"attention_rollout\":{},\n",
    "                                   \"attention_last\":{},\n",
    "                                   \"LRP\":{},\n",
    "                                   \"gradcam\":{},\n",
    "                                   \"gradcamgithub\": {},\n",
    "                                   \"vanillapixel\": {},\n",
    "                                   \"vanillaembedding\": {},\n",
    "                                   \"sgpixel\": {},\n",
    "                                   \"sgembedding\": {},\n",
    "                                   \"vargradpixel\": {},\n",
    "                                   \"vargradembedding\": {},               \n",
    "                                   \"igpixel\": {},\n",
    "                                   \"igembedding\": {},\n",
    "                                   \"leaveoneoutclassifier\": {},\n",
    "                                   \"leaveoneoutsurrogate\": {},\n",
    "                                   \"riseclassifier\": {},\n",
    "                                   \"risesurrogate\": {},\n",
    "                                   \"ours\": {},\n",
    "                                   }\n",
    "    sensitivity_save_dit[backbone_type]=sensitivity_save_dit_backbone\n",
    "    \n",
    "def sensitivity_save_dit_update(backbone_type, explanation_method, num_included_players,\n",
    "                                path_list, sensitivity_list,\n",
    "                                shape=None):\n",
    "    \n",
    "    sensitivity_save_dit_backbone_method=sensitivity_save_dit[backbone_type][explanation_method]\n",
    "        \n",
    "    assert len(path_list) == len(sensitivity_list)\n",
    "    \n",
    "    for sensitivity, path in zip(sensitivity_list, path_list):\n",
    "        assert type(sensitivity)==np.ndarray\n",
    "        assert type(path)==str\n",
    "        if shape is not None:\n",
    "            assert sensitivity.shape==shape\n",
    "        sensitivity_save_dit_backbone_method.setdefault(path, {})\n",
    "        sensitivity_save_dit_backbone_method[path][num_included_players]=sensitivity.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df2a81b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_rollout              0   +    816   ->    816\n",
      "attention_last                 0   +    816   ->    816\n",
      "LRP                            0   +    816   ->    816\n",
      "gradcam                        0   +    816   ->    816\n",
      "gradcamgithub                  0   +    816   ->    816\n",
      "vanillapixel                   0   +    816   ->    816\n",
      "vanillaembedding               0   +    816   ->    816\n",
      "sgpixel                        0   +    816   ->    816\n",
      "sgembedding                    0   +    816   ->    816\n",
      "vargradpixel                   0   +    816   ->    816\n",
      "vargradembedding               0   +    815   ->    815\n",
      "igpixel                        0   +    815   ->    815\n",
      "igembedding                    0   +    815   ->    815\n",
      "leaveoneoutclassifier          0   +    815   ->    815\n",
      "leaveoneoutsurrogate           0   +      0   ->      0\n",
      "riseclassifier                 0   +    815   ->    815\n",
      "risesurrogate                  0   +      0   ->      0\n",
      "ours                           0   +    814   ->    814\n"
     ]
    }
   ],
   "source": [
    "for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    for explanation_method, sensitivity_save_dit_backbone_method in sensitivity_save_dit[backbone_type].items():\n",
    "        sensitivity_save_dit_path=f'results/5_sensitivity/{_config[\"datasets\"]}/{backbone_type}_{explanation_method}_{dataset_split}.pickle'\n",
    "\n",
    "        if os.path.isfile(sensitivity_save_dit_path):\n",
    "            with open(sensitivity_save_dit_path, 'rb') as f:\n",
    "                sensitivity_save_dit_loaded=pickle.load(f)\n",
    "        else:\n",
    "            sensitivity_save_dit_loaded={}\n",
    "\n",
    "        len_original=len(sensitivity_save_dit_backbone_method)            \n",
    "        len_loaded=len(sensitivity_save_dit_loaded)\n",
    "        sensitivity_save_dit_backbone_method.update(sensitivity_save_dit_loaded)\n",
    "        len_updated=len(sensitivity_save_dit_backbone_method)\n",
    "\n",
    "        print(f'{explanation_method:24}  {len_original:6}   + {len_loaded:6}   -> {len_updated:6}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c313913",
   "metadata": {},
   "outputs": [],
   "source": [
    "noretraining_save_dict={}\n",
    "for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    noretraining_save_dict_backbone={\"random\":{},\n",
    "                                     \"attention_rollout\":{},\n",
    "                                     \"attention_last\":{},\n",
    "                                     \"LRP\":{},\n",
    "                                     \"gradcam\":{},\n",
    "                                     \"gradcamgithub\": {},\n",
    "                                     \"vanillapixel\": {},\n",
    "                                     \"vanillaembedding\": {},\n",
    "                                     \"sgpixel\": {},\n",
    "                                     \"sgembedding\": {},\n",
    "                                     \"vargradpixel\": {},\n",
    "                                     \"vargradembedding\": {},               \n",
    "                                     \"igpixel\": {},\n",
    "                                     \"igembedding\": {},\n",
    "                                     \"leaveoneoutclassifier\": {},\n",
    "                                     \"leaveoneoutsurrogate\": {},\n",
    "                                     \"riseclassifier\": {},\n",
    "                                     \"risesurrogate\": {},\n",
    "                                     \"ours\": {},\n",
    "                                    }\n",
    "    noretraining_save_dict[backbone_type]=noretraining_save_dict_backbone\n",
    "    \n",
    "def noretraining_save_dict_update(backbone_type, explanation_method,\n",
    "                                 path_list, insert_list, delete_list,\n",
    "                                 shape=None):\n",
    "    noretraining_save_dict_backbone_method=noretraining_save_dict[backbone_type][explanation_method]\n",
    "        \n",
    "    assert len(path_list) == len(insert_list) == len(delete_list)\n",
    "    \n",
    "    for insert, delete, path in zip(insert_list, delete_list, path_list):\n",
    "        assert type(insert)==np.ndarray\n",
    "        assert type(delete)==np.ndarray\n",
    "        assert type(path)==str\n",
    "        if shape is not None:\n",
    "            assert insert.shape==shape\n",
    "            assert delete.shape==shape\n",
    "        noretraining_save_dict_backbone_method[path]={\"insert\": insert.astype(float),\n",
    "                                                      \"delete\": delete.astype(float)\n",
    "                                                      }    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "644df8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random                         0   +      0   ->      0\n",
      "attention_rollout              0   +      0   ->      0\n",
      "attention_last                 0   +      0   ->      0\n",
      "LRP                            0   +      0   ->      0\n",
      "gradcam                        0   +      0   ->      0\n",
      "gradcamgithub                  0   +      0   ->      0\n",
      "vanillapixel                   0   +      0   ->      0\n",
      "vanillaembedding               0   +      0   ->      0\n",
      "sgpixel                        0   +      0   ->      0\n",
      "sgembedding                    0   +      0   ->      0\n",
      "vargradpixel                   0   +      0   ->      0\n",
      "vargradembedding               0   +      0   ->      0\n",
      "igpixel                        0   +      0   ->      0\n",
      "igembedding                    0   +      0   ->      0\n",
      "leaveoneoutclassifier          0   +      0   ->      0\n",
      "leaveoneoutsurrogate           0   +      0   ->      0\n",
      "riseclassifier                 0   +      0   ->      0\n",
      "risesurrogate                  0   +      0   ->      0\n",
      "ours                           0   +      0   ->      0\n"
     ]
    }
   ],
   "source": [
    "for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    for explanation_method, noretraining_save_dict_backbone_method in noretraining_save_dict[backbone_type].items():\n",
    "        noretraining_save_dict_path=f'results/6_noretraining/{_config[\"datasets\"]}/{backbone_type}_{explanation_method}_{dataset_split}.pickle'\n",
    "\n",
    "        if os.path.isfile(noretraining_save_dict_path):\n",
    "            with open(noretraining_save_dict_path, 'rb') as f:\n",
    "                noretraining_save_dict_loaded=pickle.load(f)\n",
    "        else:\n",
    "            noretraining_save_dict_loaded={}\n",
    "            \n",
    "        len_original=len(noretraining_save_dict_backbone_method)            \n",
    "        len_loaded=len(noretraining_save_dict_loaded)\n",
    "        noretraining_save_dict_backbone_method.update(noretraining_save_dict_loaded)\n",
    "        len_updated=len(noretraining_save_dict_backbone_method)\n",
    "            \n",
    "        print(f'{explanation_method:24}  {len_original:6}   + {len_loaded:6}   -> {len_updated:6}')                                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cc56a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiermasked_save_dict={}\n",
    "for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    classifiermasked_save_dict_backbone={\"random\":{},\n",
    "                                     \"attention_rollout\":{},\n",
    "                                     \"attention_last\":{},\n",
    "                                     \"LRP\":{},\n",
    "                                     \"gradcam\":{},\n",
    "                                     \"gradcamgithub\": {},\n",
    "                                     \"vanillapixel\": {},\n",
    "                                     \"vanillaembedding\": {},\n",
    "                                     \"sgpixel\": {},\n",
    "                                     \"sgembedding\": {},\n",
    "                                     \"vargradpixel\": {},\n",
    "                                     \"vargradembedding\": {},               \n",
    "                                     \"igpixel\": {},\n",
    "                                     \"igembedding\": {},\n",
    "                                     \"leaveoneoutclassifier\": {},\n",
    "                                     \"leaveoneoutsurrogate\": {},\n",
    "                                     \"riseclassifier\": {},\n",
    "                                     \"risesurrogate\": {},\n",
    "                                     \"ours\": {},\n",
    "                                    }\n",
    "    classifiermasked_save_dict[backbone_type]=classifiermasked_save_dict_backbone\n",
    "    \n",
    "def classifiermasked_save_dict_update(backbone_type, explanation_method,\n",
    "                                 path_list, insert_list, delete_list,\n",
    "                                 shape=None):\n",
    "    classifiermasked_save_dict_backbone_method=classifiermasked_save_dict[backbone_type][explanation_method]\n",
    "        \n",
    "    assert len(path_list) == len(insert_list) == len(delete_list)\n",
    "    \n",
    "    for insert, delete, path in zip(insert_list, delete_list, path_list):\n",
    "        assert type(insert)==np.ndarray\n",
    "        assert type(delete)==np.ndarray\n",
    "        assert type(path)==str\n",
    "        if shape is not None:\n",
    "            assert insert.shape==shape\n",
    "            assert delete.shape==shape\n",
    "        classifiermasked_save_dict_backbone_method[path]={\"insert\": insert.astype(float),\n",
    "                                                      \"delete\": delete.astype(float)\n",
    "                                                      }    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfd4cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    for explanation_method, classifiermasked_save_dict_backbone_method in classifiermasked_save_dict[backbone_type].items():\n",
    "        classifiermasked_save_dict_path=f'results/7_classifiermasked/{_config[\"datasets\"]}/{backbone_type}_{explanation_method}_{dataset_split}.pickle'\n",
    "\n",
    "        if os.path.isfile(classifiermasked_save_dict_path):\n",
    "            with open(classifiermasked_save_dict_path, 'rb') as f:\n",
    "                classifiermasked_save_dict_loaded=pickle.load(f)\n",
    "        else:\n",
    "            classifiermasked_save_dict_loaded={}\n",
    "            \n",
    "        len_original=len(classifiermasked_save_dict_backbone_method)            \n",
    "        len_loaded=len(classifiermasked_save_dict_loaded)\n",
    "        classifiermasked_save_dict_backbone_method.update(classifiermasked_save_dict_loaded)\n",
    "        len_updated=len(classifiermasked_save_dict_backbone_method)\n",
    "            \n",
    "        print(f'{explanation_method:24}  {len_original:6}   + {len_loaded:6}   -> {len_updated:6}')                                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882fc10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsedtime_save_dict={}\n",
    "for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    elapsedtime_save_dict_backbone={\"random\":{},\n",
    "                                     \"attention_rollout\":{},\n",
    "                                     \"attention_last\":{},\n",
    "                                     \"LRP\":{},\n",
    "                                     \"gradcam\":{},\n",
    "                                     \"gradcamgithub\": {},\n",
    "                                     \"vanillapixel\": {},\n",
    "                                     \"vanillaembedding\": {},\n",
    "                                     \"sgpixel\": {},\n",
    "                                     \"sgembedding\": {},\n",
    "                                     \"vargradpixel\": {},\n",
    "                                     \"vargradembedding\": {},               \n",
    "                                     \"igpixel\": {},\n",
    "                                     \"igembedding\": {},\n",
    "                                     \"leaveoneoutclassifier\": {},\n",
    "                                     \"leaveoneoutsurrogate\": {},\n",
    "                                     \"riseclassifier\": {},\n",
    "                                     \"risesurrogate\": {},\n",
    "                                     \"ours\": {},\n",
    "                                    }\n",
    "    elapsedtime_save_dict[backbone_type]=elapsedtime_save_dict_backbone\n",
    "    \n",
    "def elapsedtime_save_dict_update(backbone_type, explanation_method,\n",
    "                                 path_list, elapsed_time_list,\n",
    "                                 shape=None):\n",
    "    elapsedtime_save_dict_backbone_method=elapsedtime_save_dict[backbone_type][explanation_method]\n",
    "        \n",
    "    assert len(path_list) == len(elapsed_time_list)\n",
    "    \n",
    "    for elapsed_time, path in zip(elapsed_time_list, path_list):\n",
    "        assert type(elapsed_time)==float\n",
    "        assert type(path)==str\n",
    "        elapsedtime_save_dict_backbone_method[path]={\"time\": elapsed_time}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50926815",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    for explanation_method, elapsedtime_save_dict_backbone_method in elapsedtime_save_dict[backbone_type].items():\n",
    "        elapsedtime_save_dict_path=f'results/8_elapsedtime/{_config[\"datasets\"]}/{backbone_type}_{explanation_method}_{dataset_split}.pickle'\n",
    "\n",
    "        if os.path.isfile(elapsedtime_save_dict_path):\n",
    "            with open(elapsedtime_save_dict_path, 'rb') as f:\n",
    "                elapsedtime_save_dict_loaded=pickle.load(f)\n",
    "        else:\n",
    "            elapsedtime_save_dict_loaded={}\n",
    "            \n",
    "        len_original=len(elapsedtime_save_dict_backbone_method)            \n",
    "        len_loaded=len(elapsedtime_save_dict_loaded)\n",
    "        elapsedtime_save_dict_backbone_method.update(elapsedtime_save_dict_loaded)\n",
    "        len_updated=len(elapsedtime_save_dict_backbone_method)\n",
    "            \n",
    "        print(f'{explanation_method:24}  {len_original:6}   + {len_loaded:6}   -> {len_updated:6}')                                                                                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf838f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimationerror_save_dict={}\n",
    "for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    estimationerror_save_dict_backbone={\"kernelshap\":{},\n",
    "                                        \"kernelshapnopair\":{},\n",
    "                                        \"ours\": {}\n",
    "                                        }\n",
    "    estimationerror_save_dict[backbone_type]=estimationerror_save_dict_backbone\n",
    "    \n",
    "def estimationerror_save_dict_update(backbone_type, explanation_method,\n",
    "                                     path_list, estimation_list, label_list,\n",
    "                                     shape=None):\n",
    "    estimationerror_save_dict_backbone_method=estimationerror_save_dict[backbone_type][explanation_method]\n",
    "        \n",
    "    assert len(path_list) == len(estimation_list) == len(label_list)\n",
    "    \n",
    "    for path, estimation, label in zip(path_list, estimation_list, label_list):\n",
    "        assert type(path)==str\n",
    "        #assert type(estimation)==np.ndarray\n",
    "        assert type(label)==int\n",
    "        \n",
    "        if shape is not None:\n",
    "            assert estimation.shape==shape        \n",
    "        \n",
    "        estimationerror_save_dict_backbone_method[path]={\"estimation\": estimation,\n",
    "                                                         \"label\": label}        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cfd7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    for explanation_method, estimationerror_save_dict_backbone_method in estimationerror_save_dict[backbone_type].items():\n",
    "        estimationerror_save_dict_path=f'results/9_estimationerror/{_config[\"datasets\"]}/{backbone_type}_{explanation_method}_{dataset_split}.pickle'\n",
    "\n",
    "        if os.path.isfile(estimationerror_save_dict_path):\n",
    "            with open(estimationerror_save_dict_path, 'rb') as f:\n",
    "                estimationerror_save_dict_loaded=pickle.load(f)\n",
    "        else:\n",
    "            estimationerror_save_dict_loaded={}\n",
    "            \n",
    "        len_original=len(estimationerror_save_dict_backbone_method)            \n",
    "        len_loaded=len(estimationerror_save_dict_loaded)\n",
    "        estimationerror_save_dict_backbone_method.update(estimationerror_save_dict_loaded)\n",
    "        len_updated=len(estimationerror_save_dict_backbone_method)\n",
    "            \n",
    "        print(f'{explanation_method:24}  {len_original:6}   + {len_loaded:6}   -> {len_updated:6}')                                                                                                                            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7c1111",
   "metadata": {},
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16405739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relative_value(x, random_seed=None):\n",
    "    assert len(x.shape)==1\n",
    "    \n",
    "    if isinstance(random_seed, int):\n",
    "        rng = np.random.default_rng(random_seed)\n",
    "        perm = rng.permutation(np.arange(len(x)))\n",
    "    else:\n",
    "        perm = np.random.permutation(np.arange(len(x)))    \n",
    "\n",
    "    argsorted=np.arange(len(x))[perm][np.argsort(x[perm])]\n",
    "    relative_value=np.argsort(argsorted)\n",
    "\n",
    "    return relative_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "252a017d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt_path(path_original, dict_keys):\n",
    "    path_list = ['l0.cs.washington.edu', 'l1lambda.cs.washington.edu', 'l2lambda.cs.washington.edu',\n",
    "                 'l3.cs.washington.edu', 'deeper.cs.washington.edu', 'sync', '/homes/gws/chanwkim/', '/mmfs1/home/chanwkim/']\n",
    "    dict_keys=list(dict_keys)\n",
    "\n",
    "\n",
    "    for path1 in path_list:\n",
    "        if path1 in path_original:\n",
    "            for path2 in path_list:\n",
    "                path_replaced=path_original.replace(path1, path2)\n",
    "                if path_replaced in dict_keys:\n",
    "                    return path_replaced\n",
    "    return path_original\n",
    "    #raise ValueError(f\"not found {path_original}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aef1c66",
   "metadata": {},
   "source": [
    "# Methods to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b71c18f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['random', 'attention_rollout', 'attention_last', 'LRP', 'gradcam', 'gradcamgithub', 'vanillapixel', 'vanillaembedding', 'sgpixel', 'sgembedding', 'vargradpixel', 'vargradembedding', 'igpixel', 'igembedding', 'leaveoneoutclassifier', 'riseclassifier', 'ours']\n"
     ]
    }
   ],
   "source": [
    "explanation_method_to_run_=[\"random\", \"attention_rollout\", \"attention_last\", \n",
    "                            \"LRP\", \"gradcam\", \"gradcamgithub\",\n",
    "                            \"vanillapixel\", \"vanillaembedding\",\n",
    "                            \"sgpixel\", \"sgembedding\",\n",
    "                            \"vargradpixel\", \"vargradembedding\",\n",
    "                            \"igpixel\", \"igembedding\",                           \n",
    "                            \"leaveoneoutclassifier\",\n",
    "                            \"riseclassifier\", \n",
    "                            \"ours\"]\n",
    "#explanation_method_to_run_=[\"kernelshap\"]\n",
    "# explanation_method_to_run_=[\"random\", \"attention_rollout\", \"attention_last\", \n",
    "#                             \"LRP\", \"gradcam\", \n",
    "#                             \"vanillaembedding\",\n",
    "#                             \"sgembedding\",\n",
    "#                             \"vargradembedding\",\n",
    "#                             \"igembedding\",                           \n",
    "#                             \"leaveoneoutclassifier\",\n",
    "#                             \"riseclassifier\", \n",
    "#                             \"ours\"]\n",
    "explanation_method_to_run=[]\n",
    "explanation_method_to_run+=explanation_method_to_run_[:]\n",
    "\n",
    "\n",
    "print(explanation_method_to_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "213d97c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1963\n",
      "1963\n"
     ]
    }
   ],
   "source": [
    "data_loader=DataLoader(dset, batch_size=1, shuffle=False, drop_last=False, num_workers=4) #16\n",
    "print(len(dset))\n",
    "print(len(data_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045b09d1",
   "metadata": {},
   "source": [
    "# 1_classifier_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4328d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluation_stage==\"1_classifier_evaluate\":    \n",
    "    classifier_result_list_all={}\n",
    "    for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "        classifier_result_list_all[backbone_type]={}\n",
    "           \n",
    "    \n",
    "    for idx, batch in enumerate(tqdm(data_loader)):\n",
    "\n",
    "        images=batch['images']\n",
    "        labels=batch['labels']\n",
    "        paths=batch['path']\n",
    "\n",
    "        for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "            # Get classifier output\n",
    "            classifier_dict[backbone_type].eval()\n",
    "            with torch.no_grad():\n",
    "                classifier_output=classifier_dict[backbone_type](images.to(classifier_dict[backbone_type].device),\n",
    "                                                                 output_attentions=True)\n",
    "            if _config[\"output_dim\"]==1:\n",
    "                prob=classifier_output['logits'].sigmoid().cpu().numpy()\n",
    "            else:\n",
    "                prob=classifier_output['logits'].softmax(dim=-1).cpu().numpy()          \n",
    "                \n",
    "                \n",
    "            for path, label, prob in zip(paths, labels, prob):\n",
    "                classifier_result_list_all[backbone_type][path]={'label':label.item(), 'prob':prob.astype(float)}\n",
    "                \n",
    "    for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "        classifier_result_list_path=f'results/1_classifier_evaluate/{_config[\"datasets\"]}/{backbone_type}_{dataset_split}.pickle'\n",
    "        with open(classifier_result_list_path, \"wb\") as f:\n",
    "            pickle.dump(classifier_result_list_all[backbone_type], f)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d237daa",
   "metadata": {},
   "source": [
    "# 2_surrogate_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e7849b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluation_stage==\"2_surrogate_evaluate\":\n",
    "    result_list_all={}\n",
    "    for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "        result_list_all[backbone_type]=[]\n",
    "\n",
    "    dset_loader=DataLoader(dset, batch_size=64, num_workers=4, shuffle=False, drop_last=True)\n",
    "\n",
    "    for batch_idx, batch in enumerate(tqdm(dset_loader, unit='batch')):  \n",
    "        for num_mask in range(0,196+1,14):\n",
    "            mask=torch.zeros((len(batch[\"images\"]), 196))\n",
    "            mask[:,:num_mask]=1\n",
    "            for i in range(len(mask)):\n",
    "                mask[i]=mask[i][torch.randperm(len(mask[i]))]\n",
    "            for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "                surrogate_dict[backbone_type][\"original\"].eval()\n",
    "                with torch.no_grad():\n",
    "                    out_original=surrogate_dict[backbone_type][\"original\"](batch[\"images\"].to(surrogate_dict[backbone_type][\"original\"].device),\n",
    "                                                                          torch.ones((len(batch[\"images\"]), 196)).to(surrogate_dict[backbone_type][\"original\"].device))\n",
    "\n",
    "                for mask_location_model in [\"original\" , \"pre-softmax\", \"zero-input\", \"zero-embedding\"]:\n",
    "                    if mask_location_model==\"original\":\n",
    "                        kl_divergence=0\n",
    "\n",
    "                        if _config[\"output_dim\"]==1:\n",
    "                            accuracy=((out_original[\"logits\"].sigmoid()>0.5).cpu().int()==batch['labels']).float().mean().item()\n",
    "                        else:\n",
    "                            accuracy=(torch.argmax(out_original[\"logits\"], dim=1).cpu()==batch['labels']).float().mean().item()\n",
    "\n",
    "                        result_list_all[backbone_type].append({\"batch_idx\": batch_idx,\n",
    "                                            \"backbone_type\": backbone_type,\n",
    "                                            \"num_mask\": num_mask,\n",
    "                                            \"mask_location_model\": mask_location_model,\n",
    "                                            \"mask_location_parameter\": \"original\",\n",
    "                                            \"kl_divergence\": kl_divergence,\n",
    "                                            \"accuracy\": accuracy})\n",
    "\n",
    "                    for mask_location_parameter in [\"pre-softmax\", \"post-softmax\", \"zero-input\", \"zero-embedding\", \"random-sampling\"]:\n",
    "                        surrogate_dict[backbone_type][mask_location_model].eval()\n",
    "                        with torch.no_grad():\n",
    "                            out_surrogate=surrogate_dict[backbone_type][mask_location_model](batch[\"images\"].to(surrogate_dict[backbone_type][mask_location_model].device), \n",
    "                                                                                             mask.to(surrogate_dict[backbone_type][mask_location_model].device),\n",
    "                                                                                             mask_location_parameter)\n",
    "\n",
    "\n",
    "                        if _config[\"output_dim\"]==1:\n",
    "                            kl_divergence = F.kl_div(input=torch.concat([F.logsigmoid(out_surrogate[\"logits\"]), F.logsigmoid(-out_surrogate[\"logits\"])], dim=1),\n",
    "                                                    target=torch.concat([torch.sigmoid(out_original[\"logits\"]), torch.sigmoid(-out_original[\"logits\"])], dim=1),\n",
    "                                                    reduction=\"batchmean\",\n",
    "                                                    log_target=False)                        \n",
    "\n",
    "                        else:\n",
    "                            kl_divergence=F.kl_div(input=torch.log_softmax(out_surrogate[\"logits\"], dim=1),\n",
    "                                                   target=torch.softmax(out_original[\"logits\"], dim=1),\n",
    "                                                   log_target=False,\n",
    "                                                   reduction='batchmean').item()                           \n",
    "\n",
    "                        if _config[\"output_dim\"]==1:\n",
    "                            accuracy=((out_surrogate[\"logits\"].sigmoid()>0.5).cpu().int()==batch['labels']).float().mean().item()\n",
    "                        else:\n",
    "                            accuracy=(torch.argmax(out_surrogate[\"logits\"], dim=1).cpu()==batch['labels']).float().mean().item()\n",
    "\n",
    "\n",
    "                        result_list_all[backbone_type].append({\"batch_idx\": batch_idx,\n",
    "                                            \"backbone_type\": backbone_type,\n",
    "                                            \"num_mask\": num_mask,\n",
    "                                            \"mask_location_model\": mask_location_model,\n",
    "                                            \"mask_location_parameter\": mask_location_parameter,\n",
    "                                            \"kl_divergence\": kl_divergence,\n",
    "                                            \"accuracy\": accuracy})\n",
    "                        \n",
    "                        \n",
    "    for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "        result_df=pd.DataFrame(result_list_all[backbone_type])\n",
    "\n",
    "        result_df.to_csv(f'results/4_0_surrogate_evaluate/{_config[\"datasets\"]}/{backbone_type}.csv')                            \n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d352d73b",
   "metadata": {},
   "source": [
    "# 3_explanation_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb324d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def adapt_path(path_original, path_format):\n",
    "#     path_list=['l0.cs.washington.edu', 'l1lambda.cs.washington.edu', 'l2lambda.cs.washington.edu', 'l3.cs.washington.edu', 'deeper.cs.washington.edu']\n",
    "    \n",
    "#     for path1 in path_list:\n",
    "#         if path1 in path_original:\n",
    "#             for path2 in path_list:\n",
    "#                 if path2 in path_format:\n",
    "#                     return path_original.replace(path1, path2)\n",
    "#             raise\n",
    "#     return path_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe91774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_explanation(num_players, num_samples=None):\n",
    "    if num_samples is None:\n",
    "        return np.random.RandomState(42).uniform(low=0, high=1e-40, size=(num_players,))\n",
    "    else:\n",
    "        return np.random.RandomState(42).uniform(low=0, high=1e-40, size=(num_samples, num_players))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439314e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_use=['Garbage truck', \n",
    "              'Tench', \n",
    "              'English springer', \n",
    "              'Parachute',  \n",
    "              'Golf ball', \n",
    "              'Gas pump']\n",
    "kernelshap_sample_idx_list_all=[]\n",
    "for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    for random_seed in [2, 3, 4, 5]:\n",
    "        label_data_list=np.array([i['label'] for i in dset.data])\n",
    "        kernelshap_sample_idx_list=[np.random.RandomState(random_seed).choice(np.arange(len(label_data_list))[(label_data_list==label_idx)]) for label_idx in [label_name_list.index(label) for label in label_to_use]]\n",
    "        kernelshap_sample_idx_list_all+=kernelshap_sample_idx_list\n",
    "kernelshap_sample_path_list_all=[dset[i]['path'] for i in kernelshap_sample_idx_list_all]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1478944c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if evaluation_stage==\"3_explanation_generate\":\n",
    "    for idx, batch in enumerate(tqdm(data_loader)):#, total = int(1000/data_loader.batch_size+0.5) if dataset_split==\"test\" else None)):\n",
    "#         if dataset_split==\"test\":\n",
    "#             if idx>int(1000/data_loader.batch_size+0.5):\n",
    "#                 break\n",
    "        if (idx%parallel_mode[1])!=parallel_mode[0]:\n",
    "            continue\n",
    "            \n",
    "        images=batch['images']\n",
    "        labels=batch['labels']\n",
    "        paths=batch['path']\n",
    "        updated_signal_list=[]\n",
    "        \n",
    "        for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "            # Get classifier output\n",
    "            classifier_dict[backbone_type].eval()\n",
    "            with torch.no_grad():\n",
    "                classifier_output=classifier_dict[backbone_type](images.to(classifier_dict[backbone_type].device),\n",
    "                                                                 output_attentions=True)        \n",
    "            for explanation_method in explanation_method_to_run:\n",
    "                data_keys=explanation_save_dict[backbone_type][explanation_method].keys()\n",
    "                data_keys=[adapt_path(data_keys_path, paths) for data_keys_path in data_keys]\n",
    "                if all([path in data_keys for path in paths]):\n",
    "                    continue\n",
    "                else:\n",
    "                    print(explanation_method,'not exist')\n",
    "                    updated_signal_list.append(explanation_method)\n",
    "                if explanation_method==\"random\":\n",
    "                    start_time=time.time()\n",
    "                    explanation_random_list=[get_random_explanation(num_players=196) for path in paths]\n",
    "                    elapsed_time=time.time()-start_time\n",
    "                    explanation_save_dict_update(backbone_type, 'random', path_list=paths, explanation_list=explanation_random_list, elapsed_time_list=[elapsed_time/len(paths) for i in range(len(paths))], shape=(196, ))\n",
    "                \n",
    "                elif explanation_method==\"attention_rollout\":\n",
    "                    attentions = np.asarray([att.cpu().detach().numpy() for att in classifier_output['self_attentions']]).transpose(1,0,2,3,4)\n",
    "                    start_time=time.time()\n",
    "                    explanation_attention_rollout_list=attentions_to_explanation(attentions, mode='rollout')\n",
    "                    elapsed_time=time.time()-start_time\n",
    "                    explanation_save_dict_update(backbone_type, 'attention_rollout', path_list=paths, explanation_list=explanation_attention_rollout_list, elapsed_time_list=[elapsed_time/len(paths) for i in range(len(paths))], shape=(196,))\n",
    "\n",
    "                elif explanation_method==\"attention_last\":\n",
    "                    attentions = np.asarray([att.cpu().detach().numpy() for att in classifier_output['self_attentions']]).transpose(1,0,2,3,4)\n",
    "                    start_time=time.time()\n",
    "                    explanation_attention_last_list=attentions_to_explanation(attentions, mode=-1)\n",
    "                    elapsed_time=time.time()-start_time\n",
    "                    explanation_save_dict_update(backbone_type, 'attention_last', path_list=paths, explanation_list=explanation_attention_last_list, elapsed_time_list=[elapsed_time/len(paths) for i in range(len(paths))], shape=(196,))            \n",
    "\n",
    "                elif explanation_method==\"LRP\":\n",
    "                    explanation_lrp_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        explanation_lrp_list.append(np.concatenate([get_lrp_module_explanation(backbone_type=backbone_type,\n",
    "                                                                              original_image=image.squeeze(0),\n",
    "                                                                              class_index=i,\n",
    "                                                                              mode='transformer_attribution').cpu().numpy() for i in range(_config[\"output_dim\"])], axis=0))\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    explanation_save_dict_update(backbone_type, 'LRP', path_list=paths, explanation_list=explanation_lrp_list, elapsed_time_list=elapsed_time_list, shape=(_config[\"output_dim\"], 196))\n",
    "\n",
    "                elif explanation_method==\"gradcam\":\n",
    "                    explanation_gradcam_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        explanation_gradcam = np.concatenate([get_lrp_module_explanation(backbone_type=backbone_type,\n",
    "                                                                                              original_image=image.squeeze(0),\n",
    "                                                                                              class_index=i,\n",
    "                                                                                              mode='attn_gradcam').cpu().numpy() for i in range(_config[\"output_dim\"])], axis=0)\n",
    "                        explanation_gradcam = np.nan_to_num(explanation_gradcam,nan=0)+np.random.uniform(low=0, high=1e-20, size=explanation_gradcam.shape)                    \n",
    "                        explanation_gradcam_list.append(explanation_gradcam)\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    explanation_save_dict_update(backbone_type, 'gradcam', path_list=paths, explanation_list=explanation_gradcam_list, elapsed_time_list=elapsed_time_list, shape=(_config[\"output_dim\"], 196))\n",
    "\n",
    "\n",
    "                elif explanation_method==\"gradcamgithub\":\n",
    "                    explanation_gradcamgithub_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        explanation_gradcamgithub = np.concatenate([cam_dict[backbone_type](input_tensor=image.unsqueeze(0).to(next(cam_dict[backbone_type].model.parameters()).device),\n",
    "                                                                                            targets=[ClassifierOutputTarget(i)], resize=False).flatten()[np.newaxis,:] for i in range(_config[\"output_dim\"])], axis=0)\n",
    "                        explanation_gradcamgithub_list.append(explanation_gradcamgithub)\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    explanation_save_dict_update(backbone_type, 'gradcamgithub', path_list=paths, explanation_list=explanation_gradcamgithub_list, elapsed_time_list=elapsed_time_list, shape=(_config[\"output_dim\"], 196))\n",
    "\n",
    "                elif explanation_method==\"vanillapixel\":\n",
    "                    explanation_vanillapixel_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        grad=get_vanilla(image, saliency_pixel=saliency_pixel_dict[backbone_type])\n",
    "                        explanation_vanillapixel_list.append(grad[\"attributions_pixel_patchabssum\"].reshape(_config[\"output_dim\"],196).numpy())\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    explanation_save_dict_update(backbone_type, 'vanillapixel', path_list=paths, explanation_list=explanation_vanillapixel_list, elapsed_time_list=elapsed_time_list, shape=(_config[\"output_dim\"], 196))\n",
    "\n",
    "                elif explanation_method==\"vanillaembedding\":\n",
    "                    explanation_vanillaembedding_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:\n",
    "                        start_time=time.time()\n",
    "                        grad=get_vanilla(image, saliency_embedding=saliency_embedding_dict[backbone_type])\n",
    "                        explanation_vanillaembedding_list.append(grad[\"attributions_embedding_abssum\"].reshape(_config[\"output_dim\"],196).numpy())\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    explanation_save_dict_update(backbone_type, 'vanillaembedding', path_list=paths, explanation_list=explanation_vanillaembedding_list, elapsed_time_list=elapsed_time_list, shape=(_config[\"output_dim\"], 196))\n",
    "\n",
    "                elif explanation_method==\"sgpixel\":\n",
    "                    explanation_sgpixel_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        grad=get_sg(image, noisetunnel_pixel=noisetunnel_pixel_dict[backbone_type])\n",
    "                        explanation_sgpixel_list.append(grad[\"attributions_pixel_patchabssum\"].reshape(_config[\"output_dim\"],196).numpy())\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    explanation_save_dict_update(backbone_type, 'sgpixel', path_list=paths, explanation_list=explanation_sgpixel_list, elapsed_time_list=elapsed_time_list, shape=(_config[\"output_dim\"], 196))\n",
    "\n",
    "\n",
    "                elif explanation_method==\"sgembedding\":\n",
    "                    explanation_sgembedding_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        grad=get_sg(image, noisetunnel_embedding=noisetunnel_embedding_dict[backbone_type])\n",
    "                        explanation_sgembedding_list.append(grad[\"attributions_embedding_abssum\"].reshape(_config[\"output_dim\"],196).numpy())\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    explanation_save_dict_update(backbone_type, 'sgembedding', path_list=paths, explanation_list=explanation_sgembedding_list, elapsed_time_list=elapsed_time_list, shape=(_config[\"output_dim\"], 196))                                \n",
    "\n",
    "                elif explanation_method==\"vargradpixel\":\n",
    "                    explanation_vargradpixel_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        grad=get_vargrad(image, noisetunnel_pixel=noisetunnel_pixel_dict[backbone_type])\n",
    "                        explanation_vargradpixel_list.append(grad[\"attributions_pixel_patchabssum\"].reshape(_config[\"output_dim\"],196).numpy())\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    explanation_save_dict_update(backbone_type, 'vargradpixel', path_list=paths, explanation_list=explanation_vargradpixel_list, elapsed_time_list=elapsed_time_list, shape=(_config[\"output_dim\"], 196))\n",
    "\n",
    "                elif explanation_method==\"vargradembedding\":\n",
    "                    explanation_vargradembedding_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        grad=get_vargrad(image, noisetunnel_embedding=noisetunnel_embedding_dict[backbone_type])\n",
    "                        explanation_vargradembedding_list.append(grad[\"attributions_embedding_abssum\"].reshape(_config[\"output_dim\"],196).numpy())\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    explanation_save_dict_update(backbone_type, 'vargradembedding', path_list=paths, explanation_list=explanation_vargradembedding_list, elapsed_time_list=elapsed_time_list, shape=(_config[\"output_dim\"], 196))                                \n",
    "\n",
    "                elif explanation_method==\"igpixel\":\n",
    "                    explanation_igpixel_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        grad=get_ig(image, ig_pixel=ig_pixel_dict[backbone_type])\n",
    "                        explanation_igpixel_list.append(grad[\"attributions_pixel_patchsum\"].reshape(_config[\"output_dim\"],196).numpy())\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    explanation_save_dict_update(backbone_type, 'igpixel', path_list=paths, explanation_list=explanation_igpixel_list, elapsed_time_list=elapsed_time_list, shape=(_config[\"output_dim\"], 196))\n",
    "\n",
    "                elif explanation_method==\"igembedding\":\n",
    "                    explanation_igembedding_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        grad=get_ig(image, ig_embedding=ig_embedding_dict[backbone_type])\n",
    "                        explanation_igembedding_list.append(grad[\"attributions_embedding_sum\"].reshape(_config[\"output_dim\"],196).numpy())\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    explanation_save_dict_update(backbone_type, 'igembedding', path_list=paths, explanation_list=explanation_igembedding_list, elapsed_time_list=elapsed_time_list, shape=(_config[\"output_dim\"], 196))                \n",
    "\n",
    "                elif explanation_method==\"leaveoneoutclassifier\":\n",
    "                    explanation_leaveoneoutclassifier_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        explanation_leaveoneoutclassifier = leave_one_out(classifier=classifier_dict_[backbone_type], image=image).reshape(_config[\"output_dim\"], 196)\n",
    "                        explanation_leaveoneoutclassifier_list.append(explanation_leaveoneoutclassifier)\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    explanation_save_dict_update(backbone_type, 'leaveoneoutclassifier', path_list=paths, explanation_list=explanation_leaveoneoutclassifier_list, elapsed_time_list=elapsed_time_list, shape=(_config[\"output_dim\"], 196))\n",
    "\n",
    "                elif explanation_method==\"leaveoneoutsurrogate\":\n",
    "                    explanation_leaveoneoutsurrogate_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        explanation_leaveoneoutsurrogate = leave_one_out(surrogate=surrogate_dict[backbone_type][\"pre-softmax\"], image=image).reshape(_config[\"output_dim\"], 196)\n",
    "                        explanation_leaveoneoutsurrogate_list.append(explanation_leaveoneoutsurrogate)\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    explanation_save_dict_update(backbone_type, 'leaveoneoutsurrogate', path_list=paths, explanation_list=explanation_leaveoneoutsurrogate_list, elapsed_time_list=elapsed_time_list, shape=(_config[\"output_dim\"], 196))                \n",
    "\n",
    "                elif explanation_method==\"riseclassifier\":\n",
    "                    explanation_riseclassifier_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        explanation_riseclassifier = rise(classifier=classifier_dict_[backbone_type], image=image, N=2000, include_prob=0.5).reshape(_config[\"output_dim\"], 196)\n",
    "                        explanation_riseclassifier_list.append(explanation_riseclassifier)\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    explanation_save_dict_update(backbone_type, 'riseclassifier', path_list=paths, explanation_list=explanation_riseclassifier_list, elapsed_time_list=elapsed_time_list, shape=(_config[\"output_dim\"], 196))\n",
    "\n",
    "                elif explanation_method==\"risesurrogate\":\n",
    "                    explanation_risesurrogate_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        explanation_risesurrogate = rise(surrogate=surrogate_dict[backbone_type][\"pre-softmax\"], image=image, N=2000, include_prob=0.5).reshape(_config[\"output_dim\"], 196)\n",
    "                        explanation_risesurrogate_list.append(explanation_risesurrogate)\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    explanation_save_dict_update(backbone_type, 'risesurrogate', path_list=paths, explanation_list=explanation_risesurrogate_list, elapsed_time_list=elapsed_time_list, shape=(_config[\"output_dim\"], 196))                \n",
    "                    \n",
    "                elif explanation_method==\"ours\":\n",
    "                    start_time=time.time()\n",
    "                    explainer_dict[backbone_type].eval()\n",
    "                    with torch.no_grad():\n",
    "                        explanation_ours = explainer_dict[backbone_type](images.to(explainer_dict[backbone_type].device))[0].detach().cpu().numpy().transpose(0, 2, 1)\n",
    "                    elapsed_time=time.time()-start_time\n",
    "                    explanation_save_dict_update(backbone_type, 'ours', path_list=paths, explanation_list=explanation_ours, elapsed_time_list=[elapsed_time/len(paths) for i in range(len(paths))], shape=(_config[\"output_dim\"], 196))                                \n",
    "                    \n",
    "                elif explanation_method==\"kernelshap\":                    \n",
    "                    explanation_kernelshap_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    path_list=[]\n",
    "                    surrogate_SHAP_wrapped_dict[backbone_type].eval()\n",
    "                    for path,image in zip(paths, images):\n",
    "                        if path not in kernelshap_sample_path_list_all:\n",
    "                            continue\n",
    "                        print(path)\n",
    "                        start_time=time.time()                        \n",
    "                        explanation_kernelshap_ret = get_shap(surrogate_SHAP_wrapped_dict[backbone_type], image, thresh=0.2)\n",
    "                        explanation_kernelshap = explanation_kernelshap_ret.values.T\n",
    "                        explanation_kernelshap_list.append(explanation_kernelshap)\n",
    "                        path_list.append(path)\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    explanation_save_dict_update(backbone_type, 'kernelshap', path_list=path_list, explanation_list=explanation_kernelshap_list, elapsed_time_list=elapsed_time_list, shape=(_config[\"output_dim\"], 196))                    \n",
    "                else:\n",
    "                    raise\n",
    "\n",
    "        try:\n",
    "            for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "                for explanation_method, explanation_save_dict_backbone_method in explanation_save_dict[backbone_type].items():\n",
    "                    if explanation_method not in updated_signal_list:\n",
    "                        continue\n",
    "                    explanation_save_dict_path=f'results/3_explanation_generate/{_config[\"datasets\"]}/{backbone_type}_{explanation_method}_{dataset_split}.pickle'\n",
    "\n",
    "                    if os.path.isfile(explanation_save_dict_path):\n",
    "                        try:\n",
    "                            with open(explanation_save_dict_path, 'rb') as f:\n",
    "                                explanation_save_dict_loaded=pickle.load(f)\n",
    "                        except:\n",
    "                            explanation_save_dict_loaded={}\n",
    "                    else:\n",
    "                        explanation_save_dict_loaded={}\n",
    "\n",
    "                    len_original=len(explanation_save_dict_backbone_method)            \n",
    "                    len_loaded=len(explanation_save_dict_loaded)\n",
    "                    explanation_save_dict_backbone_method.update(explanation_save_dict_loaded)\n",
    "                    len_updated=len(explanation_save_dict_backbone_method)\n",
    "\n",
    "                    print(f'{explanation_method:24}  {len_original:6}   + {len_loaded:6}   -> {len_updated:6}')\n",
    "\n",
    "                    with open(explanation_save_dict_path, \"wb\") as f:\n",
    "                        pickle.dump(explanation_save_dict_backbone_method, f)\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "            \n",
    "            \n",
    "    #         # Get ours\n",
    "    #         start_time=time.time();explainer_dict[backbone_type].eval()\n",
    "    #         with torch.no_grad():\n",
    "    #             values, _, _=explainer_dict[backbone_type](torch.Tensor(image).unsqueeze(0).to(explainer_dict[backbone_type].device))\n",
    "    #             values=values.cpu().numpy().transpose(0,2,1).squeeze(0)\n",
    "    #         values_ours=(values, time.time()-start_time)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41b8d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluation_stage==\"3_explanation_generate\":\n",
    "    for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "        for explanation_method, explanation_save_dict_backbone_method in explanation_save_dict[backbone_type].items():\n",
    "            explanation_save_dict_path=f'results/3_explanation_generate/{_config[\"datasets\"]}/{backbone_type}_{explanation_method}_{dataset_split}.pickle'\n",
    "\n",
    "            if os.path.isfile(explanation_save_dict_path):\n",
    "                with open(explanation_save_dict_path, 'rb') as f:\n",
    "                    explanation_save_dict_loaded=pickle.load(f)\n",
    "            else:\n",
    "                explanation_save_dict_loaded={}\n",
    "\n",
    "            len_original=len(explanation_save_dict_backbone_method)            \n",
    "            len_loaded=len(explanation_save_dict_loaded)\n",
    "            explanation_save_dict_backbone_method.update(explanation_save_dict_loaded)\n",
    "            len_updated=len(explanation_save_dict_backbone_method)\n",
    "\n",
    "            print(f'{explanation_method:24}  {len_original:6}   + {len_loaded:6}   -> {len_updated:6}')\n",
    "\n",
    "            with open(explanation_save_dict_path, \"wb\") as f:\n",
    "                pickle.dump(explanation_save_dict_backbone_method, f)       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcab1c5",
   "metadata": {},
   "source": [
    "# 4_insert_delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1c78578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explanation_to_mask(explanation, mode='insertion'):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        explanation: (num_batches, num_players)\n",
    "    Returns:\n",
    "        explanation_expaned_bool: (num_batches, num_players+1, num_players)\n",
    "    \"\"\"\n",
    "    \n",
    "    explanation_expaned=np.repeat(explanation[:,np.newaxis,:], explanation.shape[-1], axis=1) # (num_batches, num_players, num_players)\n",
    "    \n",
    "    if mode=='insertion':\n",
    "        explanation_expaned_bool = explanation_expaned > ((np.sort(explanation, axis=-1)[:, : :-1])[:, :, np.newaxis]) # (num_batches, num_players, num_players)\n",
    "        explanation_expaned_bool = np.concatenate([explanation_expaned_bool,\n",
    "                                                   np.ones(shape=(explanation_expaned_bool.shape[0], 1, explanation_expaned_bool.shape[2]))==1], axis=1) # (num_batches, num_players+1, num_players)\n",
    "        #print(explanation_expaned_bool.shape)\n",
    "    elif mode=='deletion':\n",
    "        explanation_expaned_bool = explanation_expaned < ((np.sort(explanation, axis=-1)[:, : :-1])[:, :, np.newaxis]) # (num_batches, num_players, num_players)\n",
    "        explanation_expaned_bool = np.concatenate([np.ones(shape=(explanation_expaned_bool.shape[0], 1, explanation_expaned_bool.shape[2]))==1,\n",
    "                                                   explanation_expaned_bool],axis=1) # (num_batches, num_players+1, num_players)        \n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f'{mode} should be insertion or deletion.')\n",
    "    \n",
    "    \n",
    "    return explanation_expaned_bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835e5d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_stage=\"4_insert_delete\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161dc89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation_method_to_run=[\"kernelshap\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e30e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimationerror_sample_path_list=pd.DataFrame(data_loader.dataset.data).groupby(\"label\").apply(lambda x: x.sample(n=10, random_state=42))[\"img_path\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37cb8db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a1e097",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_mode=(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c495b86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "[path in data_keys for path in paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c028ed84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_4351.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_2252.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_19792.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_4290.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_4062.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/ILSVRC2012_val_00017020.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_10200.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_12260.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_3260.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_6190.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_8971.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_3190.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_18040.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_10241.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_930.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_9011.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_11871.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_8312.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_13871.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_3651.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_37161.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_24352.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/ILSVRC2012_val_00022252.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_11481.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_16220.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_9940.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_23321.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_4220.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_121.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_7982.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/ILSVRC2012_val_00026451.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_8522.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_48060.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_7790.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_29712.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_3241.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_16300.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_17551.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_30001.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_18981.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_6241.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_7841.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_12972.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_9642.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_3311.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_6992.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_11210.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_4900.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_20360.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_11091.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_30231.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_49041.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_8992.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_521.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_4352.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_20301.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_14910.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_28352.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_4511.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_10451.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_2511.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_5820.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_31592.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_1072.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_931.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_3281.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/ILSVRC2012_val_00025761.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_9311.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_12291.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_19162.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_15571.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_3532.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_15262.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/ILSVRC2012_val_00024560.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_30072.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_2390.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_8552.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_2122.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_672.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_5712.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_15441.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_24332.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_17460.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_14582.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_24552.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_1222.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_32350.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_5781.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_1132.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_4470.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_13600.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/ILSVRC2012_val_00030740.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_2782.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_16861.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_39880.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_41101.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_5641.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_8081.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_4131.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_27102.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_11081.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_8330.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_6622.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_15810.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_11871.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_782.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_28600.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_15701.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_35172.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_6421.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_7090.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_4980.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_29062.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_11701.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_3932.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_3582.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_8260.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_23510.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_71550.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_11401.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_29580.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_261.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_1002.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_15152.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_10230.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_10592.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_18622.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_9821.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_72470.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_8240.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_6882.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_4691.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_37571.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_7372.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_562.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_48881.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_26102.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_6180.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_51440.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_4260.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_9770.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_16321.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_12831.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_19842.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_11000.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_13480.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_13231.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_6391.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_911.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_20281.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_20061.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_10251.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_4162.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_3940.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_4020.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_4310.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_11452.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_2391.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_6752.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_16920.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_16051.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_29231.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_19501.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_29462.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_8542.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_1100.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_38841.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_16090.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_7292.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_230.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_11092.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_5362.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_10271.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_12632.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_6811.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_4492.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_4241.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_14351.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_5312.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_2102.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_38201.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_38680.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_10491.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_2730.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_3471.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_4320.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_2822.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_41871.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_10692.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_7772.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_19020.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_17192.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_39102.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_19472.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_17782.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_19542.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_102.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_3922.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_5231.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_27662.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_2342.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_18871.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_390.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_2290.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/ILSVRC2012_val_00038942.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_2270.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_46700.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_30141.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_5852.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/ILSVRC2012_val_00004301.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_13541.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_8610.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_7792.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_7311.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_5690.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_73490.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_14112.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_13281.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_192.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_23440.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_12802.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/ILSVRC2012_val_00035211.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_4411.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_3530.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_13250.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_15291.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_8891.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_4731.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_52232.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_1792.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_24941.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_2490.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/ILSVRC2012_val_00027110.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_9311.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_8681.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_532.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_28350.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_22681.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_22390.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_11011.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_10531.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_24391.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_28400.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_8552.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_5641.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_4852.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_42671.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_3061.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_2572.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_24681.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_7011.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_3780.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_2402.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_2321.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_6162.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_3792.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/ILSVRC2012_val_00008162.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_18152.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_3011.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_1630.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_17690.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_11470.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_1300.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_56022.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/ILSVRC2012_val_00023440.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_43251.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_9260.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_14682.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_5731.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_4972.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_7582.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_31961.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_5370.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_38560.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_16782.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_9300.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_16280.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/ILSVRC2012_val_00035160.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_16551.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_362.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_16370.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_1660.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_482.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_2340.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_501.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_1621.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_20500.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_320.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_19260.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_18590.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_12170.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_6160.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_11190.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_20572.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_5280.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_2170.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_6662.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_30671.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_31710.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_7092.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_10992.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_14870.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_6531.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_8661.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_17851.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_5120.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_26541.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/ILSVRC2012_val_00022172.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_67480.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_880.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_10141.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_20382.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_6251.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_35890.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_5090.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_3890.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_23911.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_21350.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_20742.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_15312.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_17001.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_3030.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_16072.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_1822.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_15820.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_18252.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_2382.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_23312.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_20052.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_13831.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_18430.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_3171.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_62551.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_13770.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_13542.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_7860.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_30881.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_14531.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_50380.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_10210.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_63471.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_3290.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_8320.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_41342.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_19060.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_10151.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_42422.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_21161.JPEG',\n",
       " '/mmfs1/home/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_8611.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/ILSVRC2012_val_00009651.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_3142.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_19390.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_8911.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_21730.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_18450.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_26260.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_6490.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_10121.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/ILSVRC2012_val_00021740.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_20620.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_73320.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_8161.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_21301.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_9811.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_16080.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_1980.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_9592.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_8801.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_4262.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_13582.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_8241.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_3242.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_1960.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_23571.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_15731.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_3722.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_26631.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_14181.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_38212.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_24542.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_6881.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_21032.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/ILSVRC2012_val_00017801.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_7160.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_2841.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_23971.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_19570.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_9220.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_16912.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_7620.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_28830.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_560.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_3062.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_14362.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_681.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_17272.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_18592.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_20121.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_22302.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_3441.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_1502.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_1350.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_18272.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_13702.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_32661.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_14992.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_1121.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_10790.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_19661.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_15130.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_6971.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_32422.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_22661.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_4341.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_7822.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_13672.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_1230.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_34492.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_49281.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_8112.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_4980.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_1292.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_4752.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_901.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_791.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_7002.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_5871.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_5501.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_17872.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_23272.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_15162.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_10230.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_3272.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_562.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_4382.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_17031.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_61581.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_1821.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_12122.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_931.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_33182.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_1401.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_7931.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_9662.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_6710.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_35320.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_1530.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_1262.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/ILSVRC2012_val_00047060.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_6031.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_36541.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_1100.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_10410.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_2170.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_72982.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_34632.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_29910.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_33221.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_27231.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_6081.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_7022.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_6042.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_60232.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_17862.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_3932.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_8220.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_1962.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_36380.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_72272.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_27252.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_18042.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_5680.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_621.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_35271.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_11642.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_761.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_8620.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_14502.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_34280.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_1770.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_1881.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_1842.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_7400.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_29410.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_19261.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_1300.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_731.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_19401.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_65922.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_19272.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/ILSVRC2012_val_00020502.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_1951.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_10762.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_1332.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_1850.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_58270.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_5311.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_10751.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_44580.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_9301.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_18581.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_3260.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_24502.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_14860.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_20751.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_7310.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_9440.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_27010.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_12301.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_12732.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_6882.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_2481.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_1790.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_8172.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_2930.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_1450.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_38320.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_3492.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_1271.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_13751.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_23421.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_76121.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_16081.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_4622.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_6400.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_26802.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_5592.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_910.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_11241.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/ILSVRC2012_val_00043731.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_22761.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_7160.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_11602.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_76721.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_2941.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_12370.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_11802.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_9441.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_13280.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_5402.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_40411.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_821.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_5222.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_1790.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_34132.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_16822.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_32980.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_20651.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_31181.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_1561.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_661.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_15511.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_7320.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_5861.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_11441.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_14900.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_20312.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_3972.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_9431.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_20572.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_5851.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_6552.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_1172.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_1372.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_6761.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_8052.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_731.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_13681.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_6201.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_26852.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_12140.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_12330.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_59361.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_5852.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_6770.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_33021.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_10612.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_10782.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_47472.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_5410.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_17922.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_410.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_7580.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_2241.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_13672.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_3601.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_16952.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_5890.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_10760.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_6912.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_1122.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_11120.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_652.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_17521.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_43260.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_38050.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_3200.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_31790.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_14002.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_8420.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_13601.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_5551.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_7960.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_12051.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_1781.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_27320.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_14600.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_422.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_3311.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_2221.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_1791.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_8831.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_8630.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_3540.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_8160.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_12401.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_12681.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_3161.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_39220.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_802.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_10342.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_1840.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_70.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_17060.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_108321.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_3321.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_2920.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_2162.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_3161.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_17280.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_3540.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_20232.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_7951.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_32580.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_151.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_142.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_16392.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_11561.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_1401.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_28352.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_12430.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_1631.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_6392.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_12861.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_6520.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_3371.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_5732.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_4342.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_10852.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_7581.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_4790.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_9981.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_8901.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_1022.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_7730.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_26270.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_11372.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_19282.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_630.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_7360.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_13442.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_400.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_6540.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_9481.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03394916/n03394916_48491.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_251.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_19580.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03417042/n03417042_1000.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_531.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_26892.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03445777/n03445777_1922.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02979186/n02979186_9740.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_11331.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03425413/n03425413_8641.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03028079/n03028079_9371.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03888257/n03888257_760.JPEG',\n",
       " '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n02102040/n02102040_350.JPEG']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74d5ceef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n03000684/n03000684_4351.JPEG']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49ff78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "/homes/gws/chanwkim/, /mmfs1/home/chanwkim/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2b8727f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|████████████████████████████████████████████████████████████████████████████████▌                                            | 644/1000 [00:06<00:03, 114.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|████████████████████████████████████████████████████████████████████████████████▌                                            | 644/1000 [00:19<00:03, 114.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       678   +    681   ->    682\n",
      "attention_rollout            678   +    680   ->    681\n",
      "attention_last               678   +    680   ->    681\n",
      "LRP                          678   +    680   ->    681\n",
      "gradcam                      678   +    680   ->    681\n",
      "gradcamgithub                678   +    680   ->    681\n",
      "vanillapixel                 678   +    680   ->    681\n",
      "vanillaembedding             678   +    680   ->    681\n",
      "sgpixel                      678   +    680   ->    681\n",
      "sgembedding                  678   +    680   ->    681\n",
      "vargradpixel                 678   +    680   ->    681\n",
      "vargradembedding             678   +    680   ->    681\n",
      "igpixel                      678   +    680   ->    681\n",
      "igembedding                  678   +    680   ->    681\n",
      "leaveoneoutclassifier        678   +    680   ->    681\n",
      "riseclassifier               678   +    680   ->    681\n",
      "ours                         678   +    680   ->    681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|████████████████████████████████████████████████████████████████████████████████▊                                           | 652/1000 [13:53<2:13:20, 22.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       683   +    684   ->    685\n",
      "attention_rollout            682   +    683   ->    684\n",
      "attention_last               682   +    683   ->    684\n",
      "LRP                          682   +    683   ->    684\n",
      "gradcam                      682   +    683   ->    684\n",
      "gradcamgithub                682   +    683   ->    684\n",
      "vanillapixel                 682   +    683   ->    684\n",
      "vanillaembedding             682   +    683   ->    684\n",
      "sgpixel                      682   +    683   ->    684\n",
      "sgembedding                  682   +    683   ->    684\n",
      "vargradpixel                 682   +    683   ->    684\n",
      "vargradembedding             682   +    683   ->    684\n",
      "igpixel                      682   +    683   ->    684\n",
      "igembedding                  682   +    683   ->    684\n",
      "leaveoneoutclassifier        682   +    683   ->    684\n",
      "riseclassifier               682   +    683   ->    684\n",
      "ours                         682   +    683   ->    684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 66%|█████████████████████████████████████████████████████████████████████████████████▎                                          | 656/1000 [27:44<4:36:50, 48.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       686   +    688   ->    689\n",
      "attention_rollout            685   +    687   ->    688\n",
      "attention_last               685   +    687   ->    688\n",
      "LRP                          685   +    687   ->    688\n",
      "gradcam                      685   +    687   ->    688\n",
      "gradcamgithub                685   +    687   ->    688\n",
      "vanillapixel                 685   +    687   ->    688\n",
      "vanillaembedding             685   +    687   ->    688\n",
      "sgpixel                      685   +    687   ->    688\n",
      "sgembedding                  685   +    687   ->    688\n",
      "vargradpixel                 685   +    687   ->    688\n",
      "vargradembedding             685   +    687   ->    688\n",
      "igpixel                      685   +    687   ->    688\n",
      "igembedding                  685   +    687   ->    688\n",
      "leaveoneoutclassifier        685   +    687   ->    688\n",
      "riseclassifier               685   +    687   ->    688\n",
      "ours                         685   +    687   ->    688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 66%|█████████████████████████████████████████████████████████████████████████████████▊                                          | 660/1000 [41:34<7:01:26, 74.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       690   +    692   ->    693\n",
      "attention_rollout            689   +    691   ->    692\n",
      "attention_last               689   +    691   ->    692\n",
      "LRP                          689   +    691   ->    692\n",
      "gradcam                      689   +    691   ->    692\n",
      "gradcamgithub                689   +    691   ->    692\n",
      "vanillapixel                 689   +    691   ->    692\n",
      "vanillaembedding             689   +    691   ->    692\n",
      "sgpixel                      689   +    691   ->    692\n",
      "sgembedding                  689   +    691   ->    692\n",
      "vargradpixel                 689   +    691   ->    692\n",
      "vargradembedding             689   +    691   ->    692\n",
      "igpixel                      689   +    691   ->    692\n",
      "igembedding                  689   +    691   ->    692\n",
      "leaveoneoutclassifier        689   +    691   ->    692\n",
      "riseclassifier               689   +    691   ->    692\n",
      "ours                         689   +    691   ->    692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 66%|██████████████████████████████████████████████████████████████████████████████████▎                                         | 664/1000 [55:25<9:18:05, 99.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       694   +    696   ->    697\n",
      "attention_rollout            693   +    695   ->    696\n",
      "attention_last               693   +    695   ->    696\n",
      "LRP                          693   +    695   ->    696\n",
      "gradcam                      693   +    695   ->    696\n",
      "gradcamgithub                693   +    695   ->    696\n",
      "vanillapixel                 693   +    695   ->    696\n",
      "vanillaembedding             693   +    695   ->    696\n",
      "sgpixel                      693   +    695   ->    696\n",
      "sgembedding                  693   +    695   ->    696\n",
      "vargradpixel                 693   +    695   ->    696\n",
      "vargradembedding             693   +    695   ->    696\n",
      "igpixel                      693   +    695   ->    696\n",
      "igembedding                  693   +    695   ->    696\n",
      "leaveoneoutclassifier        693   +    695   ->    696\n",
      "riseclassifier               693   +    695   ->    696\n",
      "ours                         693   +    695   ->    696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|████████████████████████████████████████████████████████████████████████████████▏                                       | 668/1000 [1:09:21<11:20:06, 122.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       698   +    699   ->    700\n",
      "attention_rollout            697   +    698   ->    699\n",
      "attention_last               697   +    698   ->    699\n",
      "LRP                          697   +    698   ->    699\n",
      "gradcam                      697   +    698   ->    699\n",
      "gradcamgithub                697   +    698   ->    699\n",
      "vanillapixel                 697   +    698   ->    699\n",
      "vanillaembedding             697   +    698   ->    699\n",
      "sgpixel                      697   +    698   ->    699\n",
      "sgembedding                  697   +    698   ->    699\n",
      "vargradpixel                 697   +    698   ->    699\n",
      "vargradembedding             697   +    698   ->    699\n",
      "igpixel                      697   +    698   ->    699\n",
      "igembedding                  697   +    698   ->    699\n",
      "leaveoneoutclassifier        697   +    698   ->    699\n",
      "riseclassifier               697   +    698   ->    699\n",
      "ours                         697   +    698   ->    699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|████████████████████████████████████████████████████████████████████████████████▋                                       | 672/1000 [1:23:13<13:00:36, 142.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       701   +    703   ->    704\n",
      "attention_rollout            700   +    702   ->    703\n",
      "attention_last               700   +    702   ->    703\n",
      "LRP                          700   +    702   ->    703\n",
      "gradcam                      700   +    702   ->    703\n",
      "gradcamgithub                700   +    702   ->    703\n",
      "vanillapixel                 700   +    702   ->    703\n",
      "vanillaembedding             700   +    702   ->    703\n",
      "sgpixel                      700   +    702   ->    703\n",
      "sgembedding                  700   +    702   ->    703\n",
      "vargradpixel                 700   +    702   ->    703\n",
      "vargradembedding             700   +    702   ->    703\n",
      "igpixel                      700   +    702   ->    703\n",
      "igembedding                  700   +    702   ->    703\n",
      "leaveoneoutclassifier        700   +    702   ->    703\n",
      "riseclassifier               700   +    702   ->    703\n",
      "ours                         700   +    702   ->    703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|█████████████████████████████████████████████████████████████████████████████████                                       | 676/1000 [1:37:05<14:18:54, 159.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       705   +    707   ->    708\n",
      "attention_rollout            704   +    706   ->    707\n",
      "attention_last               704   +    706   ->    707\n",
      "LRP                          704   +    706   ->    707\n",
      "gradcam                      704   +    706   ->    707\n",
      "gradcamgithub                704   +    706   ->    707\n",
      "vanillapixel                 704   +    706   ->    707\n",
      "vanillaembedding             704   +    706   ->    707\n",
      "sgpixel                      704   +    706   ->    707\n",
      "sgembedding                  704   +    706   ->    707\n",
      "vargradpixel                 704   +    706   ->    707\n",
      "vargradembedding             704   +    706   ->    707\n",
      "igpixel                      704   +    706   ->    707\n",
      "igembedding                  704   +    706   ->    707\n",
      "leaveoneoutclassifier        704   +    706   ->    707\n",
      "riseclassifier               704   +    706   ->    707\n",
      "ours                         704   +    706   ->    707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|█████████████████████████████████████████████████████████████████████████████████▌                                      | 680/1000 [1:50:56<15:16:46, 171.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       709   +    711   ->    712\n",
      "attention_rollout            708   +    710   ->    711\n",
      "attention_last               708   +    710   ->    711\n",
      "LRP                          708   +    710   ->    711\n",
      "gradcam                      708   +    710   ->    711\n",
      "gradcamgithub                708   +    710   ->    711\n",
      "vanillapixel                 708   +    710   ->    711\n",
      "vanillaembedding             708   +    710   ->    711\n",
      "sgpixel                      708   +    710   ->    711\n",
      "sgembedding                  708   +    710   ->    711\n",
      "vargradpixel                 708   +    710   ->    711\n",
      "vargradembedding             708   +    710   ->    711\n",
      "igpixel                      708   +    710   ->    711\n",
      "igembedding                  708   +    710   ->    711\n",
      "leaveoneoutclassifier        708   +    710   ->    711\n",
      "riseclassifier               708   +    710   ->    711\n",
      "ours                         708   +    710   ->    711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|██████████████████████████████████████████████████████████████████████████████████                                      | 684/1000 [2:04:49<15:57:40, 181.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       713   +    715   ->    716\n",
      "attention_rollout            712   +    714   ->    715\n",
      "attention_last               712   +    714   ->    715\n",
      "LRP                          712   +    714   ->    715\n",
      "gradcam                      712   +    714   ->    715\n",
      "gradcamgithub                712   +    714   ->    715\n",
      "vanillapixel                 712   +    714   ->    715\n",
      "vanillaembedding             712   +    714   ->    715\n",
      "sgpixel                      712   +    714   ->    715\n",
      "sgembedding                  712   +    714   ->    715\n",
      "vargradpixel                 712   +    714   ->    715\n",
      "vargradembedding             712   +    714   ->    715\n",
      "igpixel                      712   +    714   ->    715\n",
      "igembedding                  712   +    714   ->    715\n",
      "leaveoneoutclassifier        712   +    714   ->    715\n",
      "riseclassifier               712   +    714   ->    715\n",
      "ours                         712   +    714   ->    715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|██████████████████████████████████████████████████████████████████████████████████▌                                     | 688/1000 [2:18:41<16:23:42, 189.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       717   +    719   ->    720\n",
      "attention_rollout            716   +    718   ->    719\n",
      "attention_last               716   +    718   ->    719\n",
      "LRP                          716   +    718   ->    719\n",
      "gradcam                      716   +    718   ->    719\n",
      "gradcamgithub                716   +    718   ->    719\n",
      "vanillapixel                 716   +    718   ->    719\n",
      "vanillaembedding             716   +    718   ->    719\n",
      "sgpixel                      716   +    718   ->    719\n",
      "sgembedding                  716   +    718   ->    719\n",
      "vargradpixel                 716   +    718   ->    719\n",
      "vargradembedding             716   +    718   ->    719\n",
      "igpixel                      716   +    718   ->    719\n",
      "igembedding                  716   +    718   ->    719\n",
      "leaveoneoutclassifier        716   +    718   ->    719\n",
      "riseclassifier               716   +    718   ->    719\n",
      "ours                         716   +    718   ->    719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|███████████████████████████████████████████████████████████████████████████████████                                     | 692/1000 [2:32:33<16:38:48, 194.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       721   +    723   ->    724\n",
      "attention_rollout            720   +    722   ->    723\n",
      "attention_last               720   +    722   ->    723\n",
      "LRP                          720   +    722   ->    723\n",
      "gradcam                      720   +    722   ->    723\n",
      "gradcamgithub                720   +    722   ->    723\n",
      "vanillapixel                 720   +    722   ->    723\n",
      "vanillaembedding             720   +    722   ->    723\n",
      "sgpixel                      720   +    722   ->    723\n",
      "sgembedding                  720   +    722   ->    723\n",
      "vargradpixel                 720   +    722   ->    723\n",
      "vargradembedding             720   +    722   ->    723\n",
      "igpixel                      720   +    722   ->    723\n",
      "igembedding                  720   +    722   ->    723\n",
      "leaveoneoutclassifier        720   +    722   ->    723\n",
      "riseclassifier               720   +    722   ->    723\n",
      "ours                         720   +    722   ->    723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████████████████████████████████████████████████████████████████████████████████▌                                    | 696/1000 [2:46:24<16:45:15, 198.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       725   +    727   ->    728\n",
      "attention_rollout            724   +    726   ->    727\n",
      "attention_last               724   +    726   ->    727\n",
      "LRP                          724   +    726   ->    727\n",
      "gradcam                      724   +    726   ->    727\n",
      "gradcamgithub                724   +    726   ->    727\n",
      "vanillapixel                 724   +    726   ->    727\n",
      "vanillaembedding             724   +    726   ->    727\n",
      "sgpixel                      724   +    726   ->    727\n",
      "sgembedding                  724   +    726   ->    727\n",
      "vargradpixel                 724   +    726   ->    727\n",
      "vargradembedding             724   +    726   ->    727\n",
      "igpixel                      724   +    726   ->    727\n",
      "igembedding                  724   +    726   ->    727\n",
      "leaveoneoutclassifier        724   +    726   ->    727\n",
      "riseclassifier               724   +    726   ->    727\n",
      "ours                         724   +    726   ->    727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|████████████████████████████████████████████████████████████████████████████████████                                    | 700/1000 [3:00:16<16:45:52, 201.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       729   +    731   ->    732\n",
      "attention_rollout            728   +    730   ->    731\n",
      "attention_last               728   +    730   ->    731\n",
      "LRP                          728   +    730   ->    731\n",
      "gradcam                      728   +    730   ->    731\n",
      "gradcamgithub                728   +    730   ->    731\n",
      "vanillapixel                 728   +    730   ->    731\n",
      "vanillaembedding             728   +    730   ->    731\n",
      "sgpixel                      728   +    730   ->    731\n",
      "sgembedding                  728   +    730   ->    731\n",
      "vargradpixel                 728   +    730   ->    731\n",
      "vargradembedding             728   +    730   ->    731\n",
      "igpixel                      728   +    730   ->    731\n",
      "igembedding                  728   +    730   ->    731\n",
      "leaveoneoutclassifier        728   +    730   ->    731\n",
      "riseclassifier               728   +    730   ->    731\n",
      "ours                         728   +    730   ->    731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|████████████████████████████████████████████████████████████████████████████████████▍                                   | 704/1000 [3:14:08<16:42:30, 203.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       733   +    735   ->    736\n",
      "attention_rollout            732   +    734   ->    735\n",
      "attention_last               732   +    734   ->    735\n",
      "LRP                          732   +    734   ->    735\n",
      "gradcam                      732   +    734   ->    735\n",
      "gradcamgithub                732   +    734   ->    735\n",
      "vanillapixel                 732   +    734   ->    735\n",
      "vanillaembedding             732   +    734   ->    735\n",
      "sgpixel                      732   +    734   ->    735\n",
      "sgembedding                  732   +    734   ->    735\n",
      "vargradpixel                 732   +    734   ->    735\n",
      "vargradembedding             732   +    734   ->    735\n",
      "igpixel                      732   +    734   ->    735\n",
      "igembedding                  732   +    734   ->    735\n",
      "leaveoneoutclassifier        732   +    734   ->    735\n",
      "riseclassifier               732   +    734   ->    735\n",
      "ours                         732   +    734   ->    735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|████████████████████████████████████████████████████████████████████████████████████▉                                   | 708/1000 [3:28:01<16:36:05, 204.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       737   +    739   ->    740\n",
      "attention_rollout            736   +    738   ->    739\n",
      "attention_last               736   +    738   ->    739\n",
      "LRP                          736   +    738   ->    739\n",
      "gradcam                      736   +    738   ->    739\n",
      "gradcamgithub                736   +    738   ->    739\n",
      "vanillapixel                 736   +    738   ->    739\n",
      "vanillaembedding             736   +    738   ->    739\n",
      "sgpixel                      736   +    738   ->    739\n",
      "sgembedding                  736   +    738   ->    739\n",
      "vargradpixel                 736   +    738   ->    739\n",
      "vargradembedding             736   +    738   ->    739\n",
      "igpixel                      736   +    738   ->    739\n",
      "igembedding                  736   +    738   ->    739\n",
      "leaveoneoutclassifier        736   +    738   ->    739\n",
      "riseclassifier               736   +    738   ->    739\n",
      "ours                         736   +    738   ->    739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|█████████████████████████████████████████████████████████████████████████████████████▍                                  | 712/1000 [3:41:53<16:27:14, 205.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       741   +    743   ->    744\n",
      "attention_rollout            740   +    742   ->    743\n",
      "attention_last               740   +    742   ->    743\n",
      "LRP                          740   +    742   ->    743\n",
      "gradcam                      740   +    742   ->    743\n",
      "gradcamgithub                740   +    742   ->    743\n",
      "vanillapixel                 740   +    742   ->    743\n",
      "vanillaembedding             740   +    742   ->    743\n",
      "sgpixel                      740   +    742   ->    743\n",
      "sgembedding                  740   +    742   ->    743\n",
      "vargradpixel                 740   +    742   ->    743\n",
      "vargradembedding             740   +    742   ->    743\n",
      "igpixel                      740   +    742   ->    743\n",
      "igembedding                  740   +    742   ->    743\n",
      "leaveoneoutclassifier        740   +    742   ->    743\n",
      "riseclassifier               740   +    742   ->    743\n",
      "ours                         740   +    742   ->    743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|█████████████████████████████████████████████████████████████████████████████████████▉                                  | 716/1000 [3:55:47<16:17:33, 206.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       745   +    747   ->    748\n",
      "attention_rollout            744   +    746   ->    747\n",
      "attention_last               744   +    746   ->    747\n",
      "LRP                          744   +    746   ->    747\n",
      "gradcam                      744   +    746   ->    747\n",
      "gradcamgithub                744   +    746   ->    747\n",
      "vanillapixel                 744   +    746   ->    747\n",
      "vanillaembedding             744   +    746   ->    747\n",
      "sgpixel                      744   +    746   ->    747\n",
      "sgembedding                  744   +    746   ->    747\n",
      "vargradpixel                 744   +    746   ->    747\n",
      "vargradembedding             744   +    746   ->    747\n",
      "igpixel                      744   +    746   ->    747\n",
      "igembedding                  744   +    746   ->    747\n",
      "leaveoneoutclassifier        744   +    746   ->    747\n",
      "riseclassifier               744   +    746   ->    747\n",
      "ours                         744   +    746   ->    747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|██████████████████████████████████████████████████████████████████████████████████████▍                                 | 720/1000 [4:09:39<16:05:55, 206.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       749   +    751   ->    752\n",
      "attention_rollout            748   +    750   ->    751\n",
      "attention_last               748   +    750   ->    751\n",
      "LRP                          748   +    750   ->    751\n",
      "gradcam                      748   +    750   ->    751\n",
      "gradcamgithub                748   +    750   ->    751\n",
      "vanillapixel                 748   +    750   ->    751\n",
      "vanillaembedding             748   +    750   ->    751\n",
      "sgpixel                      748   +    750   ->    751\n",
      "sgembedding                  748   +    750   ->    751\n",
      "vargradpixel                 748   +    750   ->    751\n",
      "vargradembedding             748   +    750   ->    751\n",
      "igpixel                      748   +    750   ->    751\n",
      "igembedding                  748   +    750   ->    751\n",
      "leaveoneoutclassifier        748   +    750   ->    751\n",
      "riseclassifier               748   +    750   ->    751\n",
      "ours                         748   +    750   ->    751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|██████████████████████████████████████████████████████████████████████████████████████▉                                 | 724/1000 [4:23:32<15:53:42, 207.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       753   +    755   ->    756\n",
      "attention_rollout            752   +    754   ->    755\n",
      "attention_last               752   +    754   ->    755\n",
      "LRP                          752   +    754   ->    755\n",
      "gradcam                      752   +    754   ->    755\n",
      "gradcamgithub                752   +    754   ->    755\n",
      "vanillapixel                 752   +    754   ->    755\n",
      "vanillaembedding             752   +    754   ->    755\n",
      "sgpixel                      752   +    754   ->    755\n",
      "sgembedding                  752   +    754   ->    755\n",
      "vargradpixel                 752   +    754   ->    755\n",
      "vargradembedding             752   +    754   ->    755\n",
      "igpixel                      752   +    754   ->    755\n",
      "igembedding                  752   +    754   ->    755\n",
      "leaveoneoutclassifier        752   +    754   ->    755\n",
      "riseclassifier               752   +    754   ->    755\n",
      "ours                         752   +    754   ->    755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|███████████████████████████████████████████████████████████████████████████████████████▎                                | 728/1000 [4:37:22<15:40:26, 207.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       757   +    759   ->    760\n",
      "attention_rollout            756   +    758   ->    759\n",
      "attention_last               756   +    758   ->    759\n",
      "LRP                          756   +    758   ->    759\n",
      "gradcam                      756   +    758   ->    759\n",
      "gradcamgithub                756   +    758   ->    759\n",
      "vanillapixel                 756   +    758   ->    759\n",
      "vanillaembedding             756   +    758   ->    759\n",
      "sgpixel                      756   +    758   ->    759\n",
      "sgembedding                  756   +    758   ->    759\n",
      "vargradpixel                 756   +    758   ->    759\n",
      "vargradembedding             756   +    758   ->    759\n",
      "igpixel                      756   +    758   ->    759\n",
      "igembedding                  756   +    758   ->    759\n",
      "leaveoneoutclassifier        756   +    758   ->    759\n",
      "riseclassifier               756   +    758   ->    759\n",
      "ours                         756   +    758   ->    759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|███████████████████████████████████████████████████████████████████████████████████████▊                                | 732/1000 [4:51:13<15:26:45, 207.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       761   +    763   ->    764\n",
      "attention_rollout            760   +    762   ->    763\n",
      "attention_last               760   +    762   ->    763\n",
      "LRP                          760   +    762   ->    763\n",
      "gradcam                      760   +    762   ->    763\n",
      "gradcamgithub                760   +    762   ->    763\n",
      "vanillapixel                 760   +    762   ->    763\n",
      "vanillaembedding             760   +    762   ->    763\n",
      "sgpixel                      760   +    762   ->    763\n",
      "sgembedding                  760   +    762   ->    763\n",
      "vargradpixel                 760   +    762   ->    763\n",
      "vargradembedding             760   +    762   ->    763\n",
      "igpixel                      760   +    762   ->    763\n",
      "igembedding                  760   +    762   ->    763\n",
      "leaveoneoutclassifier        760   +    762   ->    763\n",
      "riseclassifier               760   +    762   ->    763\n",
      "ours                         760   +    762   ->    763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|████████████████████████████████████████████████████████████████████████████████████████▎                               | 736/1000 [5:05:05<15:13:41, 207.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       765   +    767   ->    768\n",
      "attention_rollout            764   +    766   ->    767\n",
      "attention_last               764   +    766   ->    767\n",
      "LRP                          764   +    766   ->    767\n",
      "gradcam                      764   +    766   ->    767\n",
      "gradcamgithub                764   +    766   ->    767\n",
      "vanillapixel                 764   +    766   ->    767\n",
      "vanillaembedding             764   +    766   ->    767\n",
      "sgpixel                      764   +    766   ->    767\n",
      "sgembedding                  764   +    766   ->    767\n",
      "vargradpixel                 764   +    766   ->    767\n",
      "vargradembedding             764   +    766   ->    767\n",
      "igpixel                      764   +    766   ->    767\n",
      "igembedding                  764   +    766   ->    767\n",
      "leaveoneoutclassifier        764   +    766   ->    767\n",
      "riseclassifier               764   +    766   ->    767\n",
      "ours                         764   +    766   ->    767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|████████████████████████████████████████████████████████████████████████████████████████▊                               | 740/1000 [5:18:57<15:00:09, 207.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       769   +    771   ->    772\n",
      "attention_rollout            768   +    770   ->    771\n",
      "attention_last               768   +    770   ->    771\n",
      "LRP                          768   +    770   ->    771\n",
      "gradcam                      768   +    770   ->    771\n",
      "gradcamgithub                768   +    770   ->    771\n",
      "vanillapixel                 768   +    770   ->    771\n",
      "vanillaembedding             768   +    770   ->    771\n",
      "sgpixel                      768   +    770   ->    771\n",
      "sgembedding                  768   +    770   ->    771\n",
      "vargradpixel                 768   +    770   ->    771\n",
      "vargradembedding             768   +    770   ->    771\n",
      "igpixel                      768   +    770   ->    771\n",
      "igembedding                  768   +    770   ->    771\n",
      "leaveoneoutclassifier        768   +    770   ->    771\n",
      "riseclassifier               768   +    770   ->    771\n",
      "ours                         768   +    770   ->    771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|█████████████████████████████████████████████████████████████████████████████████████████▎                              | 744/1000 [5:32:47<14:46:01, 207.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       773   +    775   ->    776\n",
      "attention_rollout            772   +    774   ->    775\n",
      "attention_last               772   +    774   ->    775\n",
      "LRP                          772   +    774   ->    775\n",
      "gradcam                      772   +    774   ->    775\n",
      "gradcamgithub                772   +    774   ->    775\n",
      "vanillapixel                 772   +    774   ->    775\n",
      "vanillaembedding             772   +    774   ->    775\n",
      "sgpixel                      772   +    774   ->    775\n",
      "sgembedding                  772   +    774   ->    775\n",
      "vargradpixel                 772   +    774   ->    775\n",
      "vargradembedding             772   +    774   ->    775\n",
      "igpixel                      772   +    774   ->    775\n",
      "igembedding                  772   +    774   ->    775\n",
      "leaveoneoutclassifier        772   +    774   ->    775\n",
      "riseclassifier               772   +    774   ->    775\n",
      "ours                         772   +    774   ->    775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|█████████████████████████████████████████████████████████████████████████████████████████▊                              | 748/1000 [5:46:37<14:32:09, 207.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       777   +    779   ->    780\n",
      "attention_rollout            776   +    778   ->    779\n",
      "attention_last               776   +    778   ->    779\n",
      "LRP                          776   +    778   ->    779\n",
      "gradcam                      776   +    778   ->    779\n",
      "gradcamgithub                776   +    778   ->    779\n",
      "vanillapixel                 776   +    778   ->    779\n",
      "vanillaembedding             776   +    778   ->    779\n",
      "sgpixel                      776   +    778   ->    779\n",
      "sgembedding                  776   +    778   ->    779\n",
      "vargradpixel                 776   +    778   ->    779\n",
      "vargradembedding             776   +    778   ->    779\n",
      "igpixel                      776   +    778   ->    779\n",
      "igembedding                  776   +    778   ->    779\n",
      "leaveoneoutclassifier        776   +    778   ->    779\n",
      "riseclassifier               776   +    778   ->    779\n",
      "ours                         776   +    778   ->    779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|██████████████████████████████████████████████████████████████████████████████████████████▏                             | 752/1000 [6:00:29<14:18:36, 207.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       781   +    783   ->    784\n",
      "attention_rollout            780   +    782   ->    783\n",
      "attention_last               780   +    782   ->    783\n",
      "LRP                          780   +    782   ->    783\n",
      "gradcam                      780   +    782   ->    783\n",
      "gradcamgithub                780   +    782   ->    783\n",
      "vanillapixel                 780   +    782   ->    783\n",
      "vanillaembedding             780   +    782   ->    783\n",
      "sgpixel                      780   +    782   ->    783\n",
      "sgembedding                  780   +    782   ->    783\n",
      "vargradpixel                 780   +    782   ->    783\n",
      "vargradembedding             780   +    782   ->    783\n",
      "igpixel                      780   +    782   ->    783\n",
      "igembedding                  780   +    782   ->    783\n",
      "leaveoneoutclassifier        780   +    782   ->    783\n",
      "riseclassifier               780   +    782   ->    783\n",
      "ours                         780   +    782   ->    783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|██████████████████████████████████████████████████████████████████████████████████████████▋                             | 756/1000 [6:14:20<14:04:57, 207.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       785   +    787   ->    788\n",
      "attention_rollout            784   +    786   ->    787\n",
      "attention_last               784   +    786   ->    787\n",
      "LRP                          784   +    786   ->    787\n",
      "gradcam                      784   +    786   ->    787\n",
      "gradcamgithub                784   +    786   ->    787\n",
      "vanillapixel                 784   +    786   ->    787\n",
      "vanillaembedding             784   +    786   ->    787\n",
      "sgpixel                      784   +    786   ->    787\n",
      "sgembedding                  784   +    786   ->    787\n",
      "vargradpixel                 784   +    786   ->    787\n",
      "vargradembedding             784   +    786   ->    787\n",
      "igpixel                      784   +    786   ->    787\n",
      "igembedding                  784   +    786   ->    787\n",
      "leaveoneoutclassifier        784   +    786   ->    787\n",
      "riseclassifier               784   +    786   ->    787\n",
      "ours                         784   +    786   ->    787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|███████████████████████████████████████████████████████████████████████████████████████████▏                            | 760/1000 [6:28:12<13:51:23, 207.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       789   +    791   ->    792\n",
      "attention_rollout            788   +    790   ->    791\n",
      "attention_last               788   +    790   ->    791\n",
      "LRP                          788   +    790   ->    791\n",
      "gradcam                      788   +    790   ->    791\n",
      "gradcamgithub                788   +    790   ->    791\n",
      "vanillapixel                 788   +    790   ->    791\n",
      "vanillaembedding             788   +    790   ->    791\n",
      "sgpixel                      788   +    790   ->    791\n",
      "sgembedding                  788   +    790   ->    791\n",
      "vargradpixel                 788   +    790   ->    791\n",
      "vargradembedding             788   +    790   ->    791\n",
      "igpixel                      788   +    790   ->    791\n",
      "igembedding                  788   +    790   ->    791\n",
      "leaveoneoutclassifier        788   +    790   ->    791\n",
      "riseclassifier               788   +    790   ->    791\n",
      "ours                         788   +    790   ->    791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|███████████████████████████████████████████████████████████████████████████████████████████▋                            | 764/1000 [6:42:07<13:38:35, 208.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       793   +    794   ->    795\n",
      "attention_rollout            792   +    793   ->    794\n",
      "attention_last               792   +    793   ->    794\n",
      "LRP                          792   +    793   ->    794\n",
      "gradcam                      792   +    793   ->    794\n",
      "gradcamgithub                792   +    793   ->    794\n",
      "vanillapixel                 792   +    793   ->    794\n",
      "vanillaembedding             792   +    793   ->    794\n",
      "sgpixel                      792   +    793   ->    794\n",
      "sgembedding                  792   +    793   ->    794\n",
      "vargradpixel                 792   +    793   ->    794\n",
      "vargradembedding             792   +    793   ->    794\n",
      "igpixel                      792   +    793   ->    794\n",
      "igembedding                  792   +    793   ->    794\n",
      "leaveoneoutclassifier        792   +    793   ->    794\n",
      "riseclassifier               792   +    793   ->    794\n",
      "ours                         792   +    793   ->    794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|████████████████████████████████████████████████████████████████████████████████████████████▏                           | 768/1000 [6:56:01<13:25:09, 208.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       796   +    798   ->    799\n",
      "attention_rollout            795   +    797   ->    798\n",
      "attention_last               795   +    797   ->    798\n",
      "LRP                          795   +    797   ->    798\n",
      "gradcam                      795   +    797   ->    798\n",
      "gradcamgithub                795   +    797   ->    798\n",
      "vanillapixel                 795   +    797   ->    798\n",
      "vanillaembedding             795   +    797   ->    798\n",
      "sgpixel                      795   +    797   ->    798\n",
      "sgembedding                  795   +    797   ->    798\n",
      "vargradpixel                 795   +    797   ->    798\n",
      "vargradembedding             795   +    797   ->    798\n",
      "igpixel                      795   +    797   ->    798\n",
      "igembedding                  795   +    797   ->    798\n",
      "leaveoneoutclassifier        795   +    797   ->    798\n",
      "riseclassifier               795   +    797   ->    798\n",
      "ours                         795   +    797   ->    798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|████████████████████████████████████████████████████████████████████████████████████████████▋                           | 772/1000 [7:09:53<13:10:55, 208.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       800   +    802   ->    803\n",
      "attention_rollout            799   +    801   ->    802\n",
      "attention_last               799   +    801   ->    802\n",
      "LRP                          799   +    801   ->    802\n",
      "gradcam                      799   +    801   ->    802\n",
      "gradcamgithub                799   +    801   ->    802\n",
      "vanillapixel                 799   +    801   ->    802\n",
      "vanillaembedding             799   +    801   ->    802\n",
      "sgpixel                      799   +    801   ->    802\n",
      "sgembedding                  799   +    801   ->    802\n",
      "vargradpixel                 799   +    801   ->    802\n",
      "vargradembedding             799   +    801   ->    802\n",
      "igpixel                      799   +    801   ->    802\n",
      "igembedding                  799   +    801   ->    802\n",
      "leaveoneoutclassifier        799   +    801   ->    802\n",
      "riseclassifier               799   +    801   ->    802\n",
      "ours                         799   +    801   ->    802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|█████████████████████████████████████████████████████████████████████████████████████████████                           | 776/1000 [7:23:44<12:56:41, 208.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       804   +    806   ->    807\n",
      "attention_rollout            803   +    805   ->    806\n",
      "attention_last               803   +    805   ->    806\n",
      "LRP                          803   +    805   ->    806\n",
      "gradcam                      803   +    805   ->    806\n",
      "gradcamgithub                803   +    805   ->    806\n",
      "vanillapixel                 803   +    805   ->    806\n",
      "vanillaembedding             803   +    805   ->    806\n",
      "sgpixel                      803   +    805   ->    806\n",
      "sgembedding                  803   +    805   ->    806\n",
      "vargradpixel                 803   +    805   ->    806\n",
      "vargradembedding             803   +    805   ->    806\n",
      "igpixel                      803   +    805   ->    806\n",
      "igembedding                  803   +    805   ->    806\n",
      "leaveoneoutclassifier        803   +    805   ->    806\n",
      "riseclassifier               803   +    805   ->    806\n",
      "ours                         803   +    805   ->    806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|█████████████████████████████████████████████████████████████████████████████████████████████▌                          | 780/1000 [7:37:38<12:43:10, 208.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       808   +    811   ->    812\n",
      "attention_rollout            807   +    810   ->    811\n",
      "attention_last               807   +    810   ->    811\n",
      "LRP                          807   +    810   ->    811\n",
      "gradcam                      807   +    810   ->    811\n",
      "gradcamgithub                807   +    810   ->    811\n",
      "vanillapixel                 807   +    810   ->    811\n",
      "vanillaembedding             807   +    810   ->    811\n",
      "sgpixel                      807   +    810   ->    811\n",
      "sgembedding                  807   +    810   ->    811\n",
      "vargradpixel                 807   +    810   ->    811\n",
      "vargradembedding             807   +    810   ->    811\n",
      "igpixel                      807   +    810   ->    811\n",
      "igembedding                  807   +    810   ->    811\n",
      "leaveoneoutclassifier        807   +    810   ->    811\n",
      "riseclassifier               807   +    810   ->    811\n",
      "ours                         807   +    810   ->    811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|██████████████████████████████████████████████████████████████████████████████████████████████                          | 784/1000 [7:51:29<12:29:04, 208.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       813   +    815   ->    816\n",
      "attention_rollout            812   +    814   ->    815\n",
      "attention_last               812   +    814   ->    815\n",
      "LRP                          812   +    814   ->    815\n",
      "gradcam                      812   +    814   ->    815\n",
      "gradcamgithub                812   +    814   ->    815\n",
      "vanillapixel                 812   +    814   ->    815\n",
      "vanillaembedding             812   +    814   ->    815\n",
      "sgpixel                      812   +    814   ->    815\n",
      "sgembedding                  812   +    814   ->    815\n",
      "vargradpixel                 812   +    814   ->    815\n",
      "vargradembedding             812   +    814   ->    815\n",
      "igpixel                      812   +    814   ->    815\n",
      "igembedding                  812   +    814   ->    815\n",
      "leaveoneoutclassifier        812   +    814   ->    815\n",
      "riseclassifier               812   +    814   ->    815\n",
      "ours                         812   +    814   ->    815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|██████████████████████████████████████████████████████████████████████████████████████████████▌                         | 788/1000 [8:05:20<12:14:48, 207.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       817   +    819   ->    820\n",
      "attention_rollout            816   +    818   ->    819\n",
      "attention_last               816   +    818   ->    819\n",
      "LRP                          816   +    818   ->    819\n",
      "gradcam                      816   +    818   ->    819\n",
      "gradcamgithub                816   +    818   ->    819\n",
      "vanillapixel                 816   +    818   ->    819\n",
      "vanillaembedding             816   +    818   ->    819\n",
      "sgpixel                      816   +    818   ->    819\n",
      "sgembedding                  816   +    818   ->    819\n",
      "vargradpixel                 816   +    818   ->    819\n",
      "vargradembedding             816   +    818   ->    819\n",
      "igpixel                      816   +    818   ->    819\n",
      "igembedding                  816   +    818   ->    819\n",
      "leaveoneoutclassifier        816   +    818   ->    819\n",
      "riseclassifier               816   +    818   ->    819\n",
      "ours                         816   +    818   ->    819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|███████████████████████████████████████████████████████████████████████████████████████████████                         | 792/1000 [8:19:11<12:00:44, 207.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       821   +    823   ->    824\n",
      "attention_rollout            820   +    822   ->    823\n",
      "attention_last               820   +    822   ->    823\n",
      "LRP                          820   +    822   ->    823\n",
      "gradcam                      820   +    822   ->    823\n",
      "gradcamgithub                820   +    822   ->    823\n",
      "vanillapixel                 820   +    822   ->    823\n",
      "vanillaembedding             820   +    822   ->    823\n",
      "sgpixel                      820   +    822   ->    823\n",
      "sgembedding                  820   +    822   ->    823\n",
      "vargradpixel                 820   +    822   ->    823\n",
      "vargradembedding             820   +    822   ->    823\n",
      "igpixel                      820   +    822   ->    823\n",
      "igembedding                  820   +    822   ->    823\n",
      "leaveoneoutclassifier        820   +    822   ->    823\n",
      "riseclassifier               820   +    822   ->    823\n",
      "ours                         820   +    822   ->    823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|███████████████████████████████████████████████████████████████████████████████████████████████▌                        | 796/1000 [8:33:02<11:46:35, 207.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       825   +    827   ->    828\n",
      "attention_rollout            824   +    826   ->    827\n",
      "attention_last               824   +    826   ->    827\n",
      "LRP                          824   +    826   ->    827\n",
      "gradcam                      824   +    826   ->    827\n",
      "gradcamgithub                824   +    826   ->    827\n",
      "vanillapixel                 824   +    826   ->    827\n",
      "vanillaembedding             824   +    826   ->    827\n",
      "sgpixel                      824   +    826   ->    827\n",
      "sgembedding                  824   +    826   ->    827\n",
      "vargradpixel                 824   +    826   ->    827\n",
      "vargradembedding             824   +    826   ->    827\n",
      "igpixel                      824   +    826   ->    827\n",
      "igembedding                  824   +    826   ->    827\n",
      "leaveoneoutclassifier        824   +    826   ->    827\n",
      "riseclassifier               824   +    826   ->    827\n",
      "ours                         824   +    826   ->    827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████████████████████████████████████████████████████████████████████████████████████████████                        | 800/1000 [8:46:55<11:33:16, 207.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       829   +    831   ->    832\n",
      "attention_rollout            828   +    830   ->    831\n",
      "attention_last               828   +    830   ->    831\n",
      "LRP                          828   +    830   ->    831\n",
      "gradcam                      828   +    830   ->    831\n",
      "gradcamgithub                828   +    830   ->    831\n",
      "vanillapixel                 828   +    830   ->    831\n",
      "vanillaembedding             828   +    830   ->    831\n",
      "sgpixel                      828   +    830   ->    831\n",
      "sgembedding                  828   +    830   ->    831\n",
      "vargradpixel                 828   +    830   ->    831\n",
      "vargradembedding             828   +    830   ->    831\n",
      "igpixel                      828   +    830   ->    831\n",
      "igembedding                  828   +    830   ->    831\n",
      "leaveoneoutclassifier        828   +    830   ->    831\n",
      "riseclassifier               828   +    830   ->    831\n",
      "ours                         828   +    830   ->    831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████████████████████████████████████████████████████████████████████████████████████████████▍                       | 804/1000 [9:00:47<11:19:19, 207.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       833   +    835   ->    836\n",
      "attention_rollout            832   +    834   ->    835\n",
      "attention_last               832   +    834   ->    835\n",
      "LRP                          832   +    834   ->    835\n",
      "gradcam                      832   +    834   ->    835\n",
      "gradcamgithub                832   +    834   ->    835\n",
      "vanillapixel                 832   +    834   ->    835\n",
      "vanillaembedding             832   +    834   ->    835\n",
      "sgpixel                      832   +    834   ->    835\n",
      "sgembedding                  832   +    834   ->    835\n",
      "vargradpixel                 832   +    834   ->    835\n",
      "vargradembedding             832   +    834   ->    835\n",
      "igpixel                      832   +    834   ->    835\n",
      "igembedding                  832   +    834   ->    835\n",
      "leaveoneoutclassifier        832   +    834   ->    835\n",
      "riseclassifier               832   +    834   ->    835\n",
      "ours                         832   +    834   ->    835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|████████████████████████████████████████████████████████████████████████████████████████████████▉                       | 808/1000 [9:14:38<11:05:18, 207.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       837   +    839   ->    840\n",
      "attention_rollout            836   +    838   ->    839\n",
      "attention_last               836   +    838   ->    839\n",
      "LRP                          836   +    838   ->    839\n",
      "gradcam                      836   +    838   ->    839\n",
      "gradcamgithub                836   +    838   ->    839\n",
      "vanillapixel                 836   +    838   ->    839\n",
      "vanillaembedding             836   +    838   ->    839\n",
      "sgpixel                      836   +    838   ->    839\n",
      "sgembedding                  836   +    838   ->    839\n",
      "vargradpixel                 836   +    838   ->    839\n",
      "vargradembedding             836   +    838   ->    839\n",
      "igpixel                      836   +    838   ->    839\n",
      "igembedding                  836   +    838   ->    839\n",
      "leaveoneoutclassifier        836   +    838   ->    839\n",
      "riseclassifier               836   +    838   ->    839\n",
      "ours                         836   +    838   ->    839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|█████████████████████████████████████████████████████████████████████████████████████████████████▍                      | 812/1000 [9:28:29<10:51:10, 207.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       841   +    843   ->    844\n",
      "attention_rollout            840   +    842   ->    843\n",
      "attention_last               840   +    842   ->    843\n",
      "LRP                          840   +    842   ->    843\n",
      "gradcam                      840   +    842   ->    843\n",
      "gradcamgithub                840   +    842   ->    843\n",
      "vanillapixel                 840   +    842   ->    843\n",
      "vanillaembedding             840   +    842   ->    843\n",
      "sgpixel                      840   +    842   ->    843\n",
      "sgembedding                  840   +    842   ->    843\n",
      "vargradpixel                 840   +    842   ->    843\n",
      "vargradembedding             840   +    842   ->    843\n",
      "igpixel                      840   +    842   ->    843\n",
      "igembedding                  840   +    842   ->    843\n",
      "leaveoneoutclassifier        840   +    842   ->    843\n",
      "riseclassifier               840   +    842   ->    843\n",
      "ours                         840   +    842   ->    843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|█████████████████████████████████████████████████████████████████████████████████████████████████▉                      | 816/1000 [9:42:19<10:37:13, 207.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       845   +    847   ->    848\n",
      "attention_rollout            844   +    846   ->    847\n",
      "attention_last               844   +    846   ->    847\n",
      "LRP                          844   +    846   ->    847\n",
      "gradcam                      844   +    846   ->    847\n",
      "gradcamgithub                844   +    846   ->    847\n",
      "vanillapixel                 844   +    846   ->    847\n",
      "vanillaembedding             844   +    846   ->    847\n",
      "sgpixel                      844   +    846   ->    847\n",
      "sgembedding                  844   +    846   ->    847\n",
      "vargradpixel                 844   +    846   ->    847\n",
      "vargradembedding             844   +    846   ->    847\n",
      "igpixel                      844   +    846   ->    847\n",
      "igembedding                  844   +    846   ->    847\n",
      "leaveoneoutclassifier        844   +    846   ->    847\n",
      "riseclassifier               844   +    846   ->    847\n",
      "ours                         844   +    846   ->    847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|██████████████████████████████████████████████████████████████████████████████████████████████████▍                     | 820/1000 [9:56:10<10:23:20, 207.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       849   +    851   ->    852\n",
      "attention_rollout            848   +    850   ->    851\n",
      "attention_last               848   +    850   ->    851\n",
      "LRP                          848   +    850   ->    851\n",
      "gradcam                      848   +    850   ->    851\n",
      "gradcamgithub                848   +    850   ->    851\n",
      "vanillapixel                 848   +    850   ->    851\n",
      "vanillaembedding             848   +    850   ->    851\n",
      "sgpixel                      848   +    850   ->    851\n",
      "sgembedding                  848   +    850   ->    851\n",
      "vargradpixel                 848   +    850   ->    851\n",
      "vargradembedding             848   +    850   ->    851\n",
      "igpixel                      848   +    850   ->    851\n",
      "igembedding                  848   +    850   ->    851\n",
      "leaveoneoutclassifier        848   +    850   ->    851\n",
      "riseclassifier               848   +    850   ->    851\n",
      "ours                         848   +    850   ->    851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|██████████████████████████████████████████████████████████████████████████████████████████████████                     | 824/1000 [10:10:04<10:09:57, 207.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       853   +    855   ->    856\n",
      "attention_rollout            852   +    854   ->    855\n",
      "attention_last               852   +    854   ->    855\n",
      "LRP                          852   +    854   ->    855\n",
      "gradcam                      852   +    854   ->    855\n",
      "gradcamgithub                852   +    854   ->    855\n",
      "vanillapixel                 852   +    854   ->    855\n",
      "vanillaembedding             852   +    854   ->    855\n",
      "sgpixel                      852   +    854   ->    855\n",
      "sgembedding                  852   +    854   ->    855\n",
      "vargradpixel                 852   +    854   ->    855\n",
      "vargradembedding             852   +    854   ->    855\n",
      "igpixel                      852   +    854   ->    855\n",
      "igembedding                  852   +    854   ->    855\n",
      "leaveoneoutclassifier        852   +    854   ->    855\n",
      "riseclassifier               852   +    854   ->    855\n",
      "ours                         852   +    854   ->    855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|███████████████████████████████████████████████████████████████████████████████████████████████████▎                    | 828/1000 [10:23:55<9:55:56, 207.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       857   +    859   ->    860\n",
      "attention_rollout            856   +    858   ->    859\n",
      "attention_last               856   +    858   ->    859\n",
      "LRP                          856   +    858   ->    859\n",
      "gradcam                      856   +    858   ->    859\n",
      "gradcamgithub                856   +    858   ->    859\n",
      "vanillapixel                 856   +    858   ->    859\n",
      "vanillaembedding             856   +    858   ->    859\n",
      "sgpixel                      856   +    858   ->    859\n",
      "sgembedding                  856   +    858   ->    859\n",
      "vargradpixel                 856   +    858   ->    859\n",
      "vargradembedding             856   +    858   ->    859\n",
      "igpixel                      856   +    858   ->    859\n",
      "igembedding                  856   +    858   ->    859\n",
      "leaveoneoutclassifier        856   +    858   ->    859\n",
      "riseclassifier               856   +    858   ->    859\n",
      "ours                         856   +    858   ->    859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|███████████████████████████████████████████████████████████████████████████████████████████████████▊                    | 832/1000 [10:37:47<9:42:08, 207.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       861   +    863   ->    864\n",
      "attention_rollout            860   +    862   ->    863\n",
      "attention_last               860   +    862   ->    863\n",
      "LRP                          860   +    862   ->    863\n",
      "gradcam                      860   +    862   ->    863\n",
      "gradcamgithub                860   +    862   ->    863\n",
      "vanillapixel                 860   +    862   ->    863\n",
      "vanillaembedding             860   +    862   ->    863\n",
      "sgpixel                      860   +    862   ->    863\n",
      "sgembedding                  860   +    862   ->    863\n",
      "vargradpixel                 860   +    862   ->    863\n",
      "vargradembedding             860   +    862   ->    863\n",
      "igpixel                      860   +    862   ->    863\n",
      "igembedding                  860   +    862   ->    863\n",
      "leaveoneoutclassifier        860   +    862   ->    863\n",
      "riseclassifier               860   +    862   ->    863\n",
      "ours                         860   +    862   ->    863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|████████████████████████████████████████████████████████████████████████████████████████████████████▎                   | 836/1000 [10:51:37<9:28:05, 207.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       865   +    867   ->    868\n",
      "attention_rollout            864   +    866   ->    867\n",
      "attention_last               864   +    866   ->    867\n",
      "LRP                          864   +    866   ->    867\n",
      "gradcam                      864   +    866   ->    867\n",
      "gradcamgithub                864   +    866   ->    867\n",
      "vanillapixel                 864   +    866   ->    867\n",
      "vanillaembedding             864   +    866   ->    867\n",
      "sgpixel                      864   +    866   ->    867\n",
      "sgembedding                  864   +    866   ->    867\n",
      "vargradpixel                 864   +    866   ->    867\n",
      "vargradembedding             864   +    866   ->    867\n",
      "igpixel                      864   +    866   ->    867\n",
      "igembedding                  864   +    866   ->    867\n",
      "leaveoneoutclassifier        864   +    866   ->    867\n",
      "riseclassifier               864   +    866   ->    867\n",
      "ours                         864   +    866   ->    867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|████████████████████████████████████████████████████████████████████████████████████████████████████▊                   | 840/1000 [11:05:28<9:14:07, 207.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       869   +    871   ->    872\n",
      "attention_rollout            868   +    870   ->    871\n",
      "attention_last               868   +    870   ->    871\n",
      "LRP                          868   +    870   ->    871\n",
      "gradcam                      868   +    870   ->    871\n",
      "gradcamgithub                868   +    870   ->    871\n",
      "vanillapixel                 868   +    870   ->    871\n",
      "vanillaembedding             868   +    870   ->    871\n",
      "sgpixel                      868   +    870   ->    871\n",
      "sgembedding                  868   +    870   ->    871\n",
      "vargradpixel                 868   +    870   ->    871\n",
      "vargradembedding             868   +    870   ->    871\n",
      "igpixel                      868   +    870   ->    871\n",
      "igembedding                  868   +    870   ->    871\n",
      "leaveoneoutclassifier        868   +    870   ->    871\n",
      "riseclassifier               868   +    870   ->    871\n",
      "ours                         868   +    870   ->    871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|█████████████████████████████████████████████████████████████████████████████████████████████████████▎                  | 844/1000 [11:19:22<9:00:44, 207.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       873   +    874   ->    875\n",
      "attention_rollout            872   +    873   ->    874\n",
      "attention_last               872   +    873   ->    874\n",
      "LRP                          872   +    873   ->    874\n",
      "gradcam                      872   +    873   ->    874\n",
      "gradcamgithub                872   +    873   ->    874\n",
      "vanillapixel                 872   +    873   ->    874\n",
      "vanillaembedding             872   +    873   ->    874\n",
      "sgpixel                      872   +    873   ->    874\n",
      "sgembedding                  872   +    873   ->    874\n",
      "vargradpixel                 872   +    873   ->    874\n",
      "vargradembedding             872   +    873   ->    874\n",
      "igpixel                      872   +    873   ->    874\n",
      "igembedding                  872   +    873   ->    874\n",
      "leaveoneoutclassifier        872   +    873   ->    874\n",
      "riseclassifier               872   +    873   ->    874\n",
      "ours                         872   +    873   ->    874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|█████████████████████████████████████████████████████████████████████████████████████████████████████▊                  | 848/1000 [11:33:13<8:46:48, 207.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       876   +    878   ->    879\n",
      "attention_rollout            875   +    877   ->    878\n",
      "attention_last               875   +    877   ->    878\n",
      "LRP                          875   +    877   ->    878\n",
      "gradcam                      875   +    877   ->    878\n",
      "gradcamgithub                875   +    877   ->    878\n",
      "vanillapixel                 875   +    877   ->    878\n",
      "vanillaembedding             875   +    877   ->    878\n",
      "sgpixel                      875   +    877   ->    878\n",
      "sgembedding                  875   +    877   ->    878\n",
      "vargradpixel                 875   +    877   ->    878\n",
      "vargradembedding             875   +    877   ->    878\n",
      "igpixel                      875   +    877   ->    878\n",
      "igembedding                  875   +    877   ->    878\n",
      "leaveoneoutclassifier        875   +    877   ->    878\n",
      "riseclassifier               875   +    877   ->    878\n",
      "ours                         875   +    877   ->    878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|██████████████████████████████████████████████████████████████████████████████████████████████████████▏                 | 852/1000 [11:47:04<8:32:42, 207.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       880   +    882   ->    883\n",
      "attention_rollout            879   +    881   ->    882\n",
      "attention_last               879   +    881   ->    882\n",
      "LRP                          879   +    881   ->    882\n",
      "gradcam                      879   +    881   ->    882\n",
      "gradcamgithub                879   +    881   ->    882\n",
      "vanillapixel                 879   +    881   ->    882\n",
      "vanillaembedding             879   +    881   ->    882\n",
      "sgpixel                      879   +    881   ->    882\n",
      "sgembedding                  879   +    881   ->    882\n",
      "vargradpixel                 879   +    881   ->    882\n",
      "vargradembedding             879   +    881   ->    882\n",
      "igpixel                      879   +    881   ->    882\n",
      "igembedding                  879   +    881   ->    882\n",
      "leaveoneoutclassifier        879   +    881   ->    882\n",
      "riseclassifier               879   +    881   ->    882\n",
      "ours                         879   +    881   ->    882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|██████████████████████████████████████████████████████████████████████████████████████████████████████▋                 | 856/1000 [12:00:56<8:18:57, 207.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       884   +    886   ->    887\n",
      "attention_rollout            883   +    885   ->    886\n",
      "attention_last               883   +    885   ->    886\n",
      "LRP                          883   +    885   ->    886\n",
      "gradcam                      883   +    885   ->    886\n",
      "gradcamgithub                883   +    885   ->    886\n",
      "vanillapixel                 883   +    885   ->    886\n",
      "vanillaembedding             883   +    885   ->    886\n",
      "sgpixel                      883   +    885   ->    886\n",
      "sgembedding                  883   +    885   ->    886\n",
      "vargradpixel                 883   +    885   ->    886\n",
      "vargradembedding             883   +    885   ->    886\n",
      "igpixel                      883   +    885   ->    886\n",
      "igembedding                  883   +    885   ->    886\n",
      "leaveoneoutclassifier        883   +    885   ->    886\n",
      "riseclassifier               883   +    885   ->    886\n",
      "ours                         883   +    885   ->    886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|███████████████████████████████████████████████████████████████████████████████████████████████████████▏                | 860/1000 [12:14:46<8:04:55, 207.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       888   +    890   ->    891\n",
      "attention_rollout            887   +    889   ->    890\n",
      "attention_last               887   +    889   ->    890\n",
      "LRP                          887   +    889   ->    890\n",
      "gradcam                      887   +    889   ->    890\n",
      "gradcamgithub                887   +    889   ->    890\n",
      "vanillapixel                 887   +    889   ->    890\n",
      "vanillaembedding             887   +    889   ->    890\n",
      "sgpixel                      887   +    889   ->    890\n",
      "sgembedding                  887   +    889   ->    890\n",
      "vargradpixel                 887   +    889   ->    890\n",
      "vargradembedding             887   +    889   ->    890\n",
      "igpixel                      887   +    889   ->    890\n",
      "igembedding                  887   +    889   ->    890\n",
      "leaveoneoutclassifier        887   +    889   ->    890\n",
      "riseclassifier               887   +    889   ->    890\n",
      "ours                         887   +    889   ->    890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|███████████████████████████████████████████████████████████████████████████████████████████████████████▋                | 864/1000 [12:28:38<7:51:05, 207.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       892   +    894   ->    895\n",
      "attention_rollout            891   +    893   ->    894\n",
      "attention_last               891   +    893   ->    894\n",
      "LRP                          891   +    893   ->    894\n",
      "gradcam                      891   +    893   ->    894\n",
      "gradcamgithub                891   +    893   ->    894\n",
      "vanillapixel                 891   +    893   ->    894\n",
      "vanillaembedding             891   +    893   ->    894\n",
      "sgpixel                      891   +    893   ->    894\n",
      "sgembedding                  891   +    893   ->    894\n",
      "vargradpixel                 891   +    893   ->    894\n",
      "vargradembedding             891   +    893   ->    894\n",
      "igpixel                      891   +    893   ->    894\n",
      "igembedding                  891   +    893   ->    894\n",
      "leaveoneoutclassifier        891   +    893   ->    894\n",
      "riseclassifier               891   +    893   ->    894\n",
      "ours                         891   +    893   ->    894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|████████████████████████████████████████████████████████████████████████████████████████████████████████▏               | 868/1000 [12:42:31<7:37:35, 207.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       896   +    898   ->    899\n",
      "attention_rollout            895   +    897   ->    898\n",
      "attention_last               895   +    897   ->    898\n",
      "LRP                          895   +    897   ->    898\n",
      "gradcam                      895   +    897   ->    898\n",
      "gradcamgithub                895   +    897   ->    898\n",
      "vanillapixel                 895   +    897   ->    898\n",
      "vanillaembedding             895   +    897   ->    898\n",
      "sgpixel                      895   +    897   ->    898\n",
      "sgembedding                  895   +    897   ->    898\n",
      "vargradpixel                 895   +    897   ->    898\n",
      "vargradembedding             895   +    897   ->    898\n",
      "igpixel                      895   +    897   ->    898\n",
      "igembedding                  895   +    897   ->    898\n",
      "leaveoneoutclassifier        895   +    897   ->    898\n",
      "riseclassifier               895   +    897   ->    898\n",
      "ours                         895   +    897   ->    898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|████████████████████████████████████████████████████████████████████████████████████████████████████████▋               | 872/1000 [12:56:22<7:23:35, 207.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       900   +    901   ->    902\n",
      "attention_rollout            899   +    900   ->    901\n",
      "attention_last               899   +    900   ->    901\n",
      "LRP                          899   +    900   ->    901\n",
      "gradcam                      899   +    900   ->    901\n",
      "gradcamgithub                899   +    900   ->    901\n",
      "vanillapixel                 899   +    900   ->    901\n",
      "vanillaembedding             899   +    900   ->    901\n",
      "sgpixel                      899   +    900   ->    901\n",
      "sgembedding                  899   +    900   ->    901\n",
      "vargradpixel                 899   +    900   ->    901\n",
      "vargradembedding             899   +    900   ->    901\n",
      "igpixel                      899   +    900   ->    901\n",
      "igembedding                  899   +    900   ->    901\n",
      "leaveoneoutclassifier        899   +    900   ->    901\n",
      "riseclassifier               899   +    900   ->    901\n",
      "ours                         899   +    900   ->    901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████               | 876/1000 [13:10:13<7:09:28, 207.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       903   +    903   ->    904\n",
      "attention_rollout            902   +    902   ->    903\n",
      "attention_last               902   +    902   ->    903\n",
      "LRP                          902   +    902   ->    903\n",
      "gradcam                      902   +    902   ->    903\n",
      "gradcamgithub                902   +    902   ->    903\n",
      "vanillapixel                 902   +    902   ->    903\n",
      "vanillaembedding             902   +    902   ->    903\n",
      "sgpixel                      902   +    902   ->    903\n",
      "sgembedding                  902   +    902   ->    903\n",
      "vargradpixel                 902   +    902   ->    903\n",
      "vargradembedding             902   +    902   ->    903\n",
      "igpixel                      902   +    902   ->    903\n",
      "igembedding                  902   +    902   ->    903\n",
      "leaveoneoutclassifier        902   +    902   ->    903\n",
      "riseclassifier               902   +    902   ->    903\n",
      "ours                         902   +    902   ->    903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▌              | 880/1000 [13:24:02<6:55:22, 207.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       905   +    905   ->    906\n",
      "attention_rollout            904   +    904   ->    905\n",
      "attention_last               904   +    904   ->    905\n",
      "LRP                          904   +    904   ->    905\n",
      "gradcam                      904   +    904   ->    905\n",
      "gradcamgithub                904   +    904   ->    905\n",
      "vanillapixel                 904   +    904   ->    905\n",
      "vanillaembedding             904   +    904   ->    905\n",
      "sgpixel                      904   +    904   ->    905\n",
      "sgembedding                  904   +    904   ->    905\n",
      "vargradpixel                 904   +    904   ->    905\n",
      "vargradembedding             904   +    904   ->    905\n",
      "igpixel                      904   +    904   ->    905\n",
      "igembedding                  904   +    904   ->    905\n",
      "leaveoneoutclassifier        904   +    904   ->    905\n",
      "riseclassifier               904   +    904   ->    905\n",
      "ours                         904   +    904   ->    905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|██████████████████████████████████████████████████████████████████████████████████████████████████████████              | 884/1000 [13:37:51<6:41:13, 207.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       907   +    907   ->    908\n",
      "attention_rollout            906   +    906   ->    907\n",
      "attention_last               906   +    906   ->    907\n",
      "LRP                          906   +    906   ->    907\n",
      "gradcam                      906   +    906   ->    907\n",
      "gradcamgithub                906   +    906   ->    907\n",
      "vanillapixel                 906   +    906   ->    907\n",
      "vanillaembedding             906   +    906   ->    907\n",
      "sgpixel                      906   +    906   ->    907\n",
      "sgembedding                  906   +    906   ->    907\n",
      "vargradpixel                 906   +    906   ->    907\n",
      "vargradembedding             906   +    906   ->    907\n",
      "igpixel                      906   +    906   ->    907\n",
      "igembedding                  906   +    906   ->    907\n",
      "leaveoneoutclassifier        906   +    906   ->    907\n",
      "riseclassifier               906   +    906   ->    907\n",
      "ours                         906   +    906   ->    907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▌             | 888/1000 [13:51:40<6:27:15, 207.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       909   +    909   ->    910\n",
      "attention_rollout            908   +    908   ->    909\n",
      "attention_last               908   +    908   ->    909\n",
      "LRP                          908   +    908   ->    909\n",
      "gradcam                      908   +    908   ->    909\n",
      "gradcamgithub                908   +    908   ->    909\n",
      "vanillapixel                 908   +    908   ->    909\n",
      "vanillaembedding             908   +    908   ->    909\n",
      "sgpixel                      908   +    908   ->    909\n",
      "sgembedding                  908   +    908   ->    909\n",
      "vargradpixel                 908   +    908   ->    909\n",
      "vargradembedding             908   +    908   ->    909\n",
      "igpixel                      908   +    908   ->    909\n",
      "igembedding                  908   +    908   ->    909\n",
      "leaveoneoutclassifier        908   +    908   ->    909\n",
      "riseclassifier               908   +    908   ->    909\n",
      "ours                         908   +    908   ->    909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|███████████████████████████████████████████████████████████████████████████████████████████████████████████             | 892/1000 [14:05:29<6:13:19, 207.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       911   +    911   ->    912\n",
      "attention_rollout            910   +    910   ->    911\n",
      "attention_last               910   +    910   ->    911\n",
      "LRP                          910   +    910   ->    911\n",
      "gradcam                      910   +    910   ->    911\n",
      "gradcamgithub                910   +    910   ->    911\n",
      "vanillapixel                 910   +    910   ->    911\n",
      "vanillaembedding             910   +    910   ->    911\n",
      "sgpixel                      910   +    910   ->    911\n",
      "sgembedding                  910   +    910   ->    911\n",
      "vargradpixel                 910   +    910   ->    911\n",
      "vargradembedding             910   +    910   ->    911\n",
      "igpixel                      910   +    910   ->    911\n",
      "igembedding                  910   +    910   ->    911\n",
      "leaveoneoutclassifier        910   +    910   ->    911\n",
      "riseclassifier               910   +    910   ->    911\n",
      "ours                         910   +    910   ->    911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▌            | 896/1000 [14:19:18<5:59:22, 207.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       913   +    915   ->    916\n",
      "attention_rollout            912   +    914   ->    915\n",
      "attention_last               912   +    913   ->    914\n",
      "LRP                          912   +    913   ->    914\n",
      "gradcam                      912   +    913   ->    914\n",
      "gradcamgithub                912   +    913   ->    914\n",
      "vanillapixel                 912   +    913   ->    914\n",
      "vanillaembedding             912   +    913   ->    914\n",
      "sgpixel                      912   +    913   ->    914\n",
      "sgembedding                  912   +    913   ->    914\n",
      "vargradpixel                 912   +    913   ->    914\n",
      "vargradembedding             912   +    913   ->    914\n",
      "igpixel                      912   +    913   ->    914\n",
      "igembedding                  912   +    913   ->    914\n",
      "leaveoneoutclassifier        912   +    913   ->    914\n",
      "riseclassifier               912   +    913   ->    914\n",
      "ours                         912   +    913   ->    914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████            | 900/1000 [14:33:06<5:45:29, 207.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       917   +    916   ->    917\n",
      "attention_rollout            916   +    915   ->    916\n",
      "attention_last               915   +    914   ->    915\n",
      "LRP                          915   +    914   ->    915\n",
      "gradcam                      915   +    914   ->    915\n",
      "gradcamgithub                915   +    914   ->    915\n",
      "vanillapixel                 915   +    914   ->    915\n",
      "vanillaembedding             915   +    914   ->    915\n",
      "sgpixel                      915   +    914   ->    915\n",
      "sgembedding                  915   +    914   ->    915\n",
      "vargradpixel                 915   +    914   ->    915\n",
      "vargradembedding             915   +    914   ->    915\n",
      "igpixel                      915   +    914   ->    915\n",
      "igembedding                  915   +    914   ->    915\n",
      "leaveoneoutclassifier        915   +    914   ->    915\n",
      "riseclassifier               915   +    914   ->    915\n",
      "ours                         915   +    914   ->    915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▍           | 904/1000 [14:46:53<5:31:19, 207.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n",
      "attention_rollout not exist\n",
      "attention_last not exist\n",
      "LRP not exist\n",
      "gradcam not exist\n",
      "gradcamgithub not exist\n",
      "vanillapixel not exist\n",
      "vanillaembedding not exist\n",
      "sgpixel not exist\n",
      "sgembedding not exist\n",
      "vargradpixel not exist\n",
      "vargradembedding not exist\n",
      "igpixel not exist\n",
      "igembedding not exist\n",
      "leaveoneoutclassifier not exist\n",
      "riseclassifier not exist\n",
      "ours not exist\n",
      "random                       918   +    917   ->    918\n",
      "attention_rollout            917   +    916   ->    917\n",
      "attention_last               916   +    915   ->    916\n",
      "LRP                          916   +    915   ->    916\n",
      "gradcam                      916   +    915   ->    916\n",
      "gradcamgithub                916   +    915   ->    916\n",
      "vanillapixel                 916   +    915   ->    916\n",
      "vanillaembedding             916   +    915   ->    916\n",
      "sgpixel                      916   +    915   ->    916\n",
      "sgembedding                  916   +    915   ->    916\n",
      "vargradpixel                 916   +    915   ->    916\n",
      "vargradembedding             916   +    915   ->    916\n",
      "igpixel                      916   +    915   ->    916\n",
      "igembedding                  916   +    915   ->    916\n",
      "leaveoneoutclassifier        916   +    915   ->    916\n",
      "riseclassifier               916   +    915   ->    916\n",
      "ours                         916   +    915   ->    916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▉           | 908/1000 [15:00:39<5:17:19, 206.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▏          | 911/1000 [15:01:33<1:28:04, 59.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_rollout not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_12131.JPEG'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m     explanations\u001b[38;5;241m=\u001b[39m[np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mRandomState(idx)\u001b[38;5;241m.\u001b[39muniform(low\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, high\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-40\u001b[39m, size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, num_players)) \u001b[38;5;28;01mfor\u001b[39;00m idx, path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(paths)]\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 37\u001b[0m     explanations\u001b[38;5;241m=\u001b[39m[explanation_save_dict[backbone_type][explanation_method][adapt_path(path, \u001b[38;5;28mlist\u001b[39m(explanation_save_dict[backbone_type][explanation_method]\u001b[38;5;241m.\u001b[39mkeys()))][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexplanation\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     38\u001b[0m                   \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m paths]\n\u001b[1;32m     40\u001b[0m insertdelete_dict\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minsertion\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeletion\u001b[39m\u001b[38;5;124m'\u001b[39m: []}\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image, explanation \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(images, explanations):\n",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     35\u001b[0m     explanations\u001b[38;5;241m=\u001b[39m[np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mRandomState(idx)\u001b[38;5;241m.\u001b[39muniform(low\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, high\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-40\u001b[39m, size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, num_players)) \u001b[38;5;28;01mfor\u001b[39;00m idx, path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(paths)]\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 37\u001b[0m     explanations\u001b[38;5;241m=\u001b[39m[\u001b[43mexplanation_save_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbackbone_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mexplanation_method\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43madapt_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexplanation_save_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbackbone_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mexplanation_method\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexplanation\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     38\u001b[0m                   \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m paths]\n\u001b[1;32m     40\u001b[0m insertdelete_dict\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minsertion\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeletion\u001b[39m\u001b[38;5;124m'\u001b[39m: []}\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image, explanation \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(images, explanations):\n",
      "\u001b[0;31mKeyError\u001b[0m: '/homes/gws/chanwkim/.fastai/data/imagenette2-160/val/n01440764/n01440764_12131.JPEG'"
     ]
    }
   ],
   "source": [
    "if evaluation_stage==\"4_insert_delete\":\n",
    "    num_players=196\n",
    "    for idx, batch in enumerate(tqdm(data_loader, total = int(1000/data_loader.batch_size+0.5) if dataset_split==\"test\" and (len(explanation_method_to_run)!=1 or explanation_method_to_run[0]!=\"kernelshap\") else None)):\n",
    "        if dataset_split==\"test\" and (len(explanation_method_to_run)!=1 or explanation_method_to_run[0]!=\"kernelshap\"):\n",
    "            if idx>int(1000/data_loader.batch_size+0.5):\n",
    "                break\n",
    "        if (idx%parallel_mode[1])!=parallel_mode[0]:\n",
    "            continue                                \n",
    "\n",
    "        images=batch['images']\n",
    "        labels=batch['labels']\n",
    "        paths=batch['path']\n",
    "\n",
    "        updated_signal_list=[]\n",
    "\n",
    "        for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "\n",
    "            for explanation_method in explanation_method_to_run:\n",
    "                data_keys=insertdelete_save_dict[backbone_type][explanation_method].keys()\n",
    "                data_keys=[adapt_path(data_keys_path, paths) for data_keys_path in data_keys]\n",
    "                if explanation_method=='kernelshap':\n",
    "                    if all([(path in data_keys) or (path not in estimationerror_sample_path_list) for path in paths]):\n",
    "                        continue\n",
    "                    else:\n",
    "                        print(explanation_method,'not exist')\n",
    "                        updated_signal_list.append(explanation_method)                \n",
    "                else:\n",
    "                    if all([path in data_keys for path in paths]):\n",
    "                        continue\n",
    "                    else:\n",
    "                        print(explanation_method,'not exist')\n",
    "                        updated_signal_list.append(explanation_method)                      \n",
    "                \n",
    "                if explanation_method==\"random\":\n",
    "                    explanations=[np.random.RandomState(idx).uniform(low=0, high=1e-40, size=(10, num_players)) for idx, path in enumerate(paths)]\n",
    "                else:\n",
    "                    explanations=[explanation_save_dict[backbone_type][explanation_method][adapt_path(path, list(explanation_save_dict[backbone_type][explanation_method].keys()))]['explanation']\n",
    "                                  for path in paths]\n",
    "                \n",
    "                insertdelete_dict={'insertion': [], 'deletion': []}\n",
    "                for image, explanation in zip(images, explanations):\n",
    "                    image_loaded=image[np.newaxis, :].repeat(num_players+1, 1, 1, 1).to(surrogate_dict[backbone_type][\"pre-softmax\"].device)\n",
    "                    if np.isnan(explanation).any():\n",
    "                        print(explanation_method, \"Null found\")\n",
    "                      \n",
    "                    if explanation_method==\"random\":\n",
    "                        for metric_mode in insertdelete_dict.keys():\n",
    "                            mask=explanation_to_mask(explanation=np.array([get_relative_value(explanation_) for explanation_ in explanation]), mode=metric_mode)\n",
    "                            prob_=[]\n",
    "                            for random_iter in range(mask.shape[0]):\n",
    "                                surrogate_dict[backbone_type][\"pre-softmax\"].eval()\n",
    "                                with torch.no_grad():\n",
    "                                    output = surrogate_dict[backbone_type][\"pre-softmax\"](image_loaded,\n",
    "                                                                                          masks=torch.Tensor(mask[random_iter]).to(surrogate_dict[backbone_type][\"pre-softmax\"].device))\n",
    "                                if _config[\"output_dim\"]==1:\n",
    "                                    prob=output['logits'].sigmoid().cpu().numpy()\n",
    "                                else:\n",
    "                                    prob=output['logits'].softmax(dim=-1).cpu().numpy()\n",
    "                                prob_.append(prob.T)\n",
    "                            prob=np.array(prob_) # (10, num_players+1, num_classes)\n",
    "                            insertdelete_dict[metric_mode].append(prob)\n",
    "                        \n",
    "                    elif len(explanation.shape)==1:\n",
    "                        for metric_mode in insertdelete_dict.keys():\n",
    "                            mask=explanation_to_mask(explanation=get_relative_value(explanation)[np.newaxis,:], mode=metric_mode)\n",
    "                            surrogate_dict[backbone_type][\"pre-softmax\"].eval()\n",
    "                            with torch.no_grad():\n",
    "                                output = surrogate_dict[backbone_type][\"pre-softmax\"](image_loaded,\n",
    "                                                                                      masks=torch.Tensor(mask[0]).to(surrogate_dict[backbone_type][\"pre-softmax\"].device))\n",
    "                            if _config[\"output_dim\"]==1:\n",
    "                                prob=output['logits'].sigmoid().cpu().numpy()\n",
    "                            else:\n",
    "                                prob=output['logits'].softmax(dim=-1).cpu().numpy()\n",
    "                            prob=prob.T # ( , num_players)\n",
    "                            insertdelete_dict[metric_mode].append(prob) \n",
    "\n",
    "                    elif len(explanation.shape)==2:\n",
    "                        for metric_mode in insertdelete_dict.keys():\n",
    "                            mask=explanation_to_mask(explanation=np.array([get_relative_value(explanation_) for explanation_ in explanation]), mode=metric_mode)\n",
    "                            prob_=[]\n",
    "                            for class_idx in range(mask.shape[0]):\n",
    "                                surrogate_dict[backbone_type][\"pre-softmax\"].eval()\n",
    "                                with torch.no_grad():\n",
    "                                    output = surrogate_dict[backbone_type][\"pre-softmax\"](image_loaded,\n",
    "                                                                                          masks=torch.Tensor(mask[class_idx]).to(surrogate_dict[backbone_type][\"pre-softmax\"].device))                                  \n",
    "                                if _config[\"output_dim\"]==1:\n",
    "                                    prob=output['logits'].sigmoid().cpu().numpy()\n",
    "                                else:\n",
    "                                    prob=output['logits'].softmax(dim=-1).cpu().numpy()\n",
    "                                prob_.append(prob[:, class_idx])\n",
    "                            prob=np.array(prob_)\n",
    "                            insertdelete_dict[metric_mode].append(prob)\n",
    "                    else:\n",
    "                        raise\n",
    "                            \n",
    "                if explanation_method==\"random\":\n",
    "                    insertdelete_save_dict_update(backbone_type, explanation_method,\n",
    "                                                  path_list=paths, \n",
    "                                                  insert_list=insertdelete_dict[\"insertion\"], \n",
    "                                                  delete_list=insertdelete_dict[\"deletion\"], \n",
    "                                                  shape=(10, _config[\"output_dim\"], num_players+1))                       \n",
    "                elif len(explanations[0].shape)==1:\n",
    "                    insertdelete_save_dict_update(backbone_type, explanation_method,\n",
    "                                                  path_list=paths, \n",
    "                                                  insert_list=insertdelete_dict[\"insertion\"], \n",
    "                                                  delete_list=insertdelete_dict[\"deletion\"], \n",
    "                                                  shape=(_config[\"output_dim\"], num_players+1))                    \n",
    "                elif len(explanations[0].shape)==2:\n",
    "                    insertdelete_save_dict_update(backbone_type, explanation_method,\n",
    "                                                  path_list=paths, \n",
    "                                                  insert_list=insertdelete_dict[\"insertion\"], \n",
    "                                                  delete_list=insertdelete_dict[\"deletion\"], \n",
    "                                                  shape=(_config[\"output_dim\"], num_players+1))\n",
    "                else:\n",
    "                    raise\n",
    "        \n",
    "        \n",
    "        for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "            for explanation_method, insertdelete_save_dict_backbone_method in insertdelete_save_dict[backbone_type].items():\n",
    "                \n",
    "                if explanation_method not in updated_signal_list:\n",
    "                    continue                \n",
    "                \n",
    "                insertdelete_save_dict_path=f'results/4_insert_delete/{_config[\"datasets\"]}/{backbone_type}_{explanation_method}_{dataset_split}.pickle'\n",
    "\n",
    "                if os.path.isfile(insertdelete_save_dict_path):\n",
    "                    with open(insertdelete_save_dict_path, 'rb') as f:\n",
    "                        insertdelete_save_dict_loaded=pickle.load(f)\n",
    "                else:\n",
    "                    insertdelete_save_dict_loaded={}\n",
    "\n",
    "                len_original=len(insertdelete_save_dict_backbone_method)            \n",
    "                len_loaded=len(insertdelete_save_dict_loaded)\n",
    "                insertdelete_save_dict_backbone_method.update(insertdelete_save_dict_loaded)\n",
    "                len_updated=len(insertdelete_save_dict_backbone_method)\n",
    "\n",
    "                print(f'{explanation_method:24}  {len_original:6}   + {len_loaded:6}   -> {len_updated:6}')\n",
    "\n",
    "\n",
    "                with open(insertdelete_save_dict_path, \"wb\") as f:\n",
    "                    pickle.dump(insertdelete_save_dict_backbone_method, f)           \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfa8ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluation_stage==\"4_insert_delete\":\n",
    "    for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "        for explanation_method, insertdelete_save_dict_backbone_method in insertdelete_save_dict[backbone_type].items():\n",
    "            insertdelete_save_dict_path=f'results/4_insert_delete/{_config[\"datasets\"]}/{backbone_type}_{explanation_method}_{dataset_split}.pickle'\n",
    "\n",
    "            if os.path.isfile(insertdelete_save_dict_path):\n",
    "                with open(insertdelete_save_dict_path, 'rb') as f:\n",
    "                    insertdelete_save_dict_loaded=pickle.load(f)\n",
    "            else:\n",
    "                insertdelete_save_dict_loaded={}\n",
    "\n",
    "            len_original=len(insertdelete_save_dict_backbone_method)            \n",
    "            len_loaded=len(insertdelete_save_dict_loaded)\n",
    "            insertdelete_save_dict_backbone_method.update(insertdelete_save_dict_loaded)\n",
    "            len_updated=len(insertdelete_save_dict_backbone_method)\n",
    "\n",
    "            print(f'{explanation_method:24}  {len_original:6}   + {len_loaded:6}   -> {len_updated:6}')\n",
    "\n",
    "\n",
    "            with open(insertdelete_save_dict_path, \"wb\") as f:\n",
    "                pickle.dump(insertdelete_save_dict_backbone_method, f)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19f433b",
   "metadata": {},
   "source": [
    "# 5_sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415b792b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(num_players: int, num_mask_samples: int or None = None, paired_mask_samples: bool = True,\n",
    "                  mode: str = 'uniform', random_state: np.random.RandomState or None = None) -> np.array:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        num_players: the number of players in the coalitional game\n",
    "        num_mask_samples: the number of masks to generate\n",
    "        paired_mask_samples: if True, the generated masks are pairs of x and 1-x.\n",
    "        mode: the distribution that the number of masked features follows. ('uniform' or 'shapley')\n",
    "        random_state: random generator\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor of shape\n",
    "        (num_masks, num_players) if num_masks is int\n",
    "        (num_players) if num_masks is None\n",
    "\n",
    "    \"\"\"\n",
    "    random_state = random_state or np.random\n",
    "\n",
    "    num_samples_ = num_mask_samples or 1\n",
    "\n",
    "    if paired_mask_samples:\n",
    "        assert num_samples_ % 2 == 0, \"'num_samples' must be a multiple of 2 if 'paired' is True\"\n",
    "        num_samples_ = num_samples_ // 2\n",
    "    else:\n",
    "        num_samples_ = num_samples_\n",
    "\n",
    "    if mode == 'uniform':\n",
    "        masks = (random_state.rand(num_samples_, num_players) > random_state.rand(num_samples_, 1)).astype('int')\n",
    "    elif mode == 'shapley':\n",
    "        probs = 1 / (np.arange(1, num_players) * (num_players - np.arange(1, num_players)))\n",
    "        probs = probs / probs.sum()\n",
    "        masks = (random_state.rand(num_samples_, num_players) > 1 / num_players * random_state.choice(\n",
    "            np.arange(num_players - 1), p=probs, size=[num_samples_, 1])).astype('int')\n",
    "    else:\n",
    "        raise ValueError(\"'mode' must be 'random' or 'shapley'\")\n",
    "\n",
    "    if paired_mask_samples:\n",
    "        masks = np.stack([masks, 1 - masks], axis=1).reshape(num_samples_ * 2, num_players)\n",
    "\n",
    "    if num_mask_samples is None:\n",
    "        masks = masks.squeeze(0)\n",
    "        return masks  # (num_masks)\n",
    "    else:\n",
    "        return masks  # (num_samples, num_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a4bf39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if evaluation_stage==\"5_sensitivity\":\n",
    "    num_players=196\n",
    "    for idx, batch in enumerate(tqdm(data_loader, total = int(1000/data_loader.batch_size+0.5) if dataset_split==\"test\" else None)):   \n",
    "        if dataset_split==\"test\":\n",
    "            if idx>int(1000/data_loader.batch_size+0.5):\n",
    "                break\n",
    "        if (idx%parallel_mode[1])!=parallel_mode[0]:\n",
    "            continue                                \n",
    "\n",
    "        images=batch['images']\n",
    "        labels=batch['labels']\n",
    "        paths=batch['path']\n",
    "\n",
    "        for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):            \n",
    "            for image, path in zip(images, paths):\n",
    "                #path=path.replace('l0.cs.washington.edu','l2lambda.cs.washington.edu')\n",
    "                for num_included_players in [\"all\"] + list(range(14, 196, 14)):\n",
    "                    if num_included_players==\"all\":\n",
    "                        prob_all=[]\n",
    "                        mask_all=[]\n",
    "                        for random_iter in range(20):\n",
    "                            image_loaded=image[np.newaxis, :].repeat(50, 1, 1, 1).to(surrogate_dict[backbone_type][\"pre-softmax\"].device)\n",
    "                            mask=generate_mask(num_players=num_players,\n",
    "                                               num_mask_samples=50,\n",
    "                                               paired_mask_samples=False,\n",
    "                                               mode=\"uniform\",\n",
    "                                               random_state=np.random.RandomState(random_iter))\n",
    "\n",
    "                            surrogate_dict[backbone_type][\"pre-softmax\"].eval()\n",
    "                            with torch.no_grad():\n",
    "                                output = surrogate_dict[backbone_type][\"pre-softmax\"](image_loaded,\n",
    "                                                                                      masks=torch.Tensor(mask).to(surrogate_dict[backbone_type][\"pre-softmax\"].device))\n",
    "\n",
    "                            if _config[\"output_dim\"]==1:\n",
    "                                prob=output['logits'].sigmoid().cpu().numpy()\n",
    "                            else:\n",
    "                                prob=output['logits'].softmax(dim=-1).cpu().numpy()   \n",
    "                            prob_all.append(prob)\n",
    "                            mask_all.append(mask)\n",
    "                        prob_all=np.concatenate(prob_all, axis=0)\n",
    "                        mask_all=np.concatenate(mask_all, axis=0)\n",
    "                    else:\n",
    "                        prob_all=[]\n",
    "                        mask_all=[]\n",
    "                        for random_iter in range(20):\n",
    "                            image_loaded=image[np.newaxis, :].repeat(50, 1, 1, 1).to(surrogate_dict[backbone_type][\"pre-softmax\"].device)                                                        \n",
    "                            mask=np.zeros((50, num_players))\n",
    "                            mask[:, :num_included_players]=1\n",
    "                            for i in range(len(mask)):\n",
    "                                mask[i]=np.random.RandomState(42+10*random_iter+i).permutation(mask[i])\n",
    "\n",
    "                            surrogate_dict[backbone_type][\"pre-softmax\"].eval()\n",
    "                            with torch.no_grad():\n",
    "                                output = surrogate_dict[backbone_type][\"pre-softmax\"](image_loaded,\n",
    "                                                                                      masks=torch.Tensor(mask).to(surrogate_dict[backbone_type][\"pre-softmax\"].device))\n",
    "\n",
    "                            if _config[\"output_dim\"]==1:\n",
    "                                prob=output['logits'].sigmoid().cpu().numpy()\n",
    "                            else:\n",
    "                                prob=output['logits'].softmax(dim=-1).cpu().numpy()   \n",
    "                            prob_all.append(prob)\n",
    "                            mask_all.append(mask)\n",
    "                        prob_all=np.concatenate(prob_all, axis=0)\n",
    "                        mask_all=np.concatenate(mask_all, axis=0)\n",
    "                    \n",
    "                    for explanation_method in explanation_method_to_run:                \n",
    "                        if explanation_method==\"random\":\n",
    "                            continue\n",
    "                        explanation=explanation_save_dict[backbone_type][explanation_method][adapt_path(path, list(explanation_save_dict[backbone_type][explanation_method].keys())[0])]['explanation']\n",
    "                        explanation=explanation+np.random.RandomState(42).uniform(low=0, high=1e-40, size=explanation.shape)\n",
    "                        explanation_mask=explanation@(mask_all.T)\n",
    "\n",
    "                        if len(explanation.shape)==1:\n",
    "                            correlation = np.array([stats.spearmanr(explanation_mask, prob_all[:, i]).correlation for i in range(prob_all.shape[1])])\n",
    "                            assert correlation.shape==(_config[\"output_dim\"],)\n",
    "                            \n",
    "                        elif len(explanation.shape)==2:\n",
    "                            #correlation=stats.spearmanr(np.concatenate([explanation_mask, prob_all.T], axis=0), axis=1).correlation\n",
    "                            correlation = np.array([stats.spearmanr(explanation_mask[i], prob_all[:, i]).correlation for i in range(prob_all.shape[1])])\n",
    "                            assert correlation.shape==(_config[\"output_dim\"],)\n",
    "                        else:\n",
    "                            raise\n",
    "                        sensitivity_save_dit_update(backbone_type, explanation_method,\n",
    "                                                     num_included_players=num_included_players,\n",
    "                                                     path_list=[path], sensitivity_list=[correlation],\n",
    "                                                     shape=(_config[\"output_dim\"],))     \n",
    "                    \n",
    "                    \n",
    "                \n",
    "        for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "            for explanation_method, sensitivity_save_dit_backbone_method in sensitivity_save_dit[backbone_type].items():\n",
    "                sensitivity_save_dit_path=f'results/5_sensitivity/{_config[\"datasets\"]}/{backbone_type}_{explanation_method}_{dataset_split}.pickle'\n",
    "\n",
    "                if os.path.isfile(sensitivity_save_dit_path):\n",
    "                    with open(sensitivity_save_dit_path, 'rb') as f:\n",
    "                        sensitivity_save_dit_loaded=pickle.load(f)\n",
    "                else:\n",
    "                    sensitivity_save_dit_loaded={}\n",
    "\n",
    "                len_original=len(sensitivity_save_dit_backbone_method)            \n",
    "                len_loaded=len(sensitivity_save_dit_loaded)\n",
    "                sensitivity_save_dit_backbone_method.update(sensitivity_save_dit_loaded)\n",
    "                len_updated=len(sensitivity_save_dit_backbone_method)\n",
    "\n",
    "                print(f'{explanation_method:24}  {len_original:6}   + {len_loaded:6}   -> {len_updated:6}')\n",
    "\n",
    "\n",
    "                with open(sensitivity_save_dit_path, \"wb\") as f:\n",
    "                    pickle.dump(sensitivity_save_dit_backbone_method, f)        \n",
    "                    \n",
    "             \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89881f09",
   "metadata": {},
   "source": [
    "# 6_noretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca77a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluation_stage==\"6_noretraining\":\n",
    "    num_players=196\n",
    "    for idx, batch in enumerate(tqdm(data_loader, total = int(1000/data_loader.batch_size+0.5) if dataset_split==\"test\" else None)):   \n",
    "        if dataset_split==\"test\":\n",
    "            if idx>int(1000/data_loader.batch_size+0.5):\n",
    "                break\n",
    "        if (idx%parallel_mode[1])!=parallel_mode[0]:\n",
    "            continue                                \n",
    "\n",
    "        images=batch['images']\n",
    "        labels=batch['labels']\n",
    "        paths=batch['path']\n",
    "        updated_signal_list=[]\n",
    "\n",
    "        for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "\n",
    "            for explanation_method in [\"random\"]+explanation_method_to_run:\n",
    "                data_keys=noretraining_save_dict[backbone_type][explanation_method].keys()\n",
    "                data_keys=[adapt_path(data_keys_path, paths) for data_keys_path in data_keys]\n",
    "                if all([path in data_keys for path in paths]):\n",
    "                    continue\n",
    "                else:\n",
    "                    print(explanation_method,'not exist')\n",
    "                    updated_signal_list.append(explanation_method)                \n",
    "                \n",
    "                if explanation_method==\"random\":\n",
    "                    explanations=[np.random.RandomState(idx).uniform(low=0, high=1e-40, size=(10, num_players)) for idx, path in enumerate(paths)]\n",
    "                else:\n",
    "                    explanations=[explanation_save_dict[backbone_type][explanation_method][adapt_path(path, list(explanation_save_dict[backbone_type][explanation_method].keys()))]['explanation']\n",
    "                                  for path in paths]\n",
    "                noretraining_dict={'insertion': [], 'deletion': []}\n",
    "                for image, explanation, label in zip(images, explanations, labels):\n",
    "                    image_loaded=image[np.newaxis, :].repeat(num_players+1, 1, 1, 1).to(surrogate_dict[backbone_type][\"pre-softmax\"].device)\n",
    "                    if np.isnan(explanation).any():\n",
    "                        print(explanation_method, \"Null found\")\n",
    "                      \n",
    "                    if explanation_method==\"random\":\n",
    "                        for metric_mode in noretraining_dict.keys():\n",
    "                            mask=explanation_to_mask(explanation=np.array([get_relative_value(explanation_) for explanation_ in explanation]), mode=metric_mode)\n",
    "                            prob_=[]\n",
    "                            for random_iter in range(mask.shape[0]):\n",
    "                                surrogate_dict[backbone_type][\"pre-softmax\"].eval()\n",
    "                                with torch.no_grad():\n",
    "                                    output = surrogate_dict[backbone_type][\"pre-softmax\"](image_loaded,\n",
    "                                                                                          masks=torch.Tensor(mask[random_iter]).to(surrogate_dict[backbone_type][\"pre-softmax\"].device))\n",
    "                                if _config[\"output_dim\"]==1:\n",
    "                                    prob=output['logits'].sigmoid().cpu().numpy()\n",
    "                                else:\n",
    "                                    prob=output['logits'].softmax(dim=-1).cpu().numpy()\n",
    "                                prob_.append(prob.T)\n",
    "                            prob=np.array(prob_) # (10, num_classes, num_players+1)\n",
    "                            noretraining_dict[metric_mode].append(prob)#(prob.argmax(axis=1)==label.item()).astype(float))\n",
    "                        \n",
    "                    elif len(explanation.shape)==1:\n",
    "                        for metric_mode in noretraining_dict.keys():\n",
    "                            mask=explanation_to_mask(explanation=get_relative_value(explanation)[np.newaxis,:], mode=metric_mode)\n",
    "                            surrogate_dict[backbone_type][\"pre-softmax\"].eval()\n",
    "                            with torch.no_grad():\n",
    "                                output = surrogate_dict[backbone_type][\"pre-softmax\"](image_loaded,\n",
    "                                                                                      masks=torch.Tensor(mask[0]).to(surrogate_dict[backbone_type][\"pre-softmax\"].device))\n",
    "                            if _config[\"output_dim\"]==1:\n",
    "                                prob=output['logits'].sigmoid().cpu().numpy()\n",
    "                            else:\n",
    "                                prob=output['logits'].softmax(dim=-1).cpu().numpy()\n",
    "                            prob=prob.T # (num_classes, num_players+1)                            \n",
    "                            #noretraining_dict[metric_mode].append((prob.argmax(axis=0)==label.item()).astype(float)) \n",
    "                            noretraining_dict[metric_mode].append(prob) \n",
    "\n",
    "                    elif len(explanation.shape)==2:\n",
    "                        for metric_mode in noretraining_dict.keys():\n",
    "                            #mask=explanation_to_mask(explanation=explanation[label.item()], mode=metric_mode)\n",
    "                            if len(explanation)==1:\n",
    "                                mask=explanation_to_mask(explanation=get_relative_value(explanation[0])[np.newaxis,:], mode=metric_mode)\n",
    "                            else:\n",
    "                                mask=explanation_to_mask(explanation=get_relative_value(explanation[label.item()])[np.newaxis,:], mode=metric_mode)\n",
    "                            \n",
    "                            surrogate_dict[backbone_type][\"pre-softmax\"].eval()\n",
    "                            with torch.no_grad():\n",
    "                                output = surrogate_dict[backbone_type][\"pre-softmax\"](image_loaded,\n",
    "                                                                                      masks=torch.Tensor(mask[0]).to(surrogate_dict[backbone_type][\"pre-softmax\"].device))\n",
    "                            if _config[\"output_dim\"]==1:\n",
    "                                prob=output['logits'].sigmoid().cpu().numpy()\n",
    "                            else:\n",
    "                                prob=output['logits'].softmax(dim=-1).cpu().numpy()\n",
    "                            prob=prob.T\n",
    "                            #noretraining_dict[metric_mode].append((prob.argmax(axis=0)==label.item()).astype(float))\n",
    "                            noretraining_dict[metric_mode].append(prob)\n",
    "                    else:\n",
    "                        raise\n",
    "                          \n",
    "                if explanation_method==\"random\":\n",
    "                    noretraining_save_dict_update(backbone_type, explanation_method,\n",
    "                                                  path_list=paths, \n",
    "                                                  insert_list=noretraining_dict[\"insertion\"], \n",
    "                                                  delete_list=noretraining_dict[\"deletion\"], \n",
    "                                                  shape=(10, _config[\"output_dim\"], num_players+1))                       \n",
    "                elif len(explanations[0].shape)==1:\n",
    "                    noretraining_save_dict_update(backbone_type, explanation_method,\n",
    "                                                  path_list=paths, \n",
    "                                                  insert_list=noretraining_dict[\"insertion\"], \n",
    "                                                  delete_list=noretraining_dict[\"deletion\"], \n",
    "                                                  shape=(_config[\"output_dim\"], num_players+1))                    \n",
    "                elif len(explanations[0].shape)==2:\n",
    "                    noretraining_save_dict_update(backbone_type, explanation_method,\n",
    "                                                  path_list=paths, \n",
    "                                                  insert_list=noretraining_dict[\"insertion\"], \n",
    "                                                  delete_list=noretraining_dict[\"deletion\"], \n",
    "                                                  shape=(_config[\"output_dim\"], num_players+1))\n",
    "                else:\n",
    "                    raise\n",
    "        \n",
    "        try:        \n",
    "            for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "                for explanation_method, noretraining_save_dict_backbone_method in noretraining_save_dict[backbone_type].items():\n",
    "\n",
    "                    if explanation_method not in updated_signal_list:\n",
    "                        continue                \n",
    "\n",
    "                    noretraining_save_dict_path=f'results/6_noretraining/{_config[\"datasets\"]}/{backbone_type}_{explanation_method}_{dataset_split}.pickle'\n",
    "\n",
    "                    if os.path.isfile(noretraining_save_dict_path):\n",
    "                        try:\n",
    "                            with open(noretraining_save_dict_path, 'rb') as f:\n",
    "                                noretraining_save_dict_loaded=pickle.load(f)\n",
    "                        except:\n",
    "                            noretraining_save_dict_loaded={}\n",
    "                    else:\n",
    "                        noretraining_save_dict_loaded={}\n",
    "\n",
    "                    len_original=len(noretraining_save_dict_backbone_method)            \n",
    "                    len_loaded=len(noretraining_save_dict_loaded)\n",
    "                    noretraining_save_dict_backbone_method.update(noretraining_save_dict_loaded)\n",
    "                    len_updated=len(noretraining_save_dict_backbone_method)\n",
    "\n",
    "                    print(f'{explanation_method:24}  {len_original:6}   + {len_loaded:6}   -> {len_updated:6}')\n",
    "\n",
    "\n",
    "                    with open(noretraining_save_dict_path, \"wb\") as f:\n",
    "                        pickle.dump(noretraining_save_dict_backbone_method, f)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "                            \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8f74b5",
   "metadata": {},
   "source": [
    "# 7_classifiermasked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ff594c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluation_stage==\"7_classifiermasked\":\n",
    "    num_players=196\n",
    "    for idx, batch in enumerate(tqdm(data_loader, total = int(1000/data_loader.batch_size+0.5) if dataset_split==\"test\" else None)):   \n",
    "        if dataset_split==\"test\":\n",
    "            if idx>int(1000/data_loader.batch_size+0.5):\n",
    "                break\n",
    "        if (idx%parallel_mode[1])!=parallel_mode[0]:\n",
    "            continue                                \n",
    "\n",
    "        images=batch['images']\n",
    "        labels=batch['labels']\n",
    "        paths=batch['path']\n",
    "        updated_signal_list=[]\n",
    "\n",
    "        for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "\n",
    "            for explanation_method in [\"random\"]+explanation_method_to_run:\n",
    "                data_keys=classifiermasked_save_dict[backbone_type][explanation_method].keys()\n",
    "                data_keys=[adapt_path(data_keys_path, paths) for data_keys_path in data_keys]\n",
    "                if all([path in data_keys for path in paths]):\n",
    "                    continue\n",
    "                else:\n",
    "                    print(explanation_method,'not exist')\n",
    "                    updated_signal_list.append(explanation_method)                \n",
    "                \n",
    "                if explanation_method==\"random\":\n",
    "                    explanations=[np.random.RandomState(idx).uniform(low=0, high=1e-40, size=(10, num_players)) for idx, path in enumerate(paths)]\n",
    "                else:\n",
    "                    explanations=[explanation_save_dict[backbone_type][explanation_method][adapt_path(path, list(explanation_save_dict[backbone_type][explanation_method].keys()))]['explanation']\n",
    "                                  for path in paths]\n",
    "                classifiermasked_dict={'insertion': [], 'deletion': []}\n",
    "                for image, explanation, label in zip(images, explanations, labels):\n",
    "                    image_loaded=image[np.newaxis, :].repeat(num_players+1, 1, 1, 1).to(classifier_masked_dict[backbone_type].device)\n",
    "                    if np.isnan(explanation).any():\n",
    "                        print(explanation_method, \"Null found\")\n",
    "                      \n",
    "                    if explanation_method==\"random\":\n",
    "                        for metric_mode in classifiermasked_dict.keys():\n",
    "                            mask=explanation_to_mask(explanation=np.array([get_relative_value(explanation_) for explanation_ in explanation]), mode=metric_mode)\n",
    "                            prob_=[]\n",
    "                            for random_iter in range(mask.shape[0]):\n",
    "                                classifier_masked_dict[backbone_type].eval()\n",
    "                                with torch.no_grad():\n",
    "                                    output = classifier_masked_dict[backbone_type](image_loaded,\n",
    "                                                                                          masks=torch.Tensor(mask[random_iter]).to(classifier_masked_dict[backbone_type].device))\n",
    "                                if _config[\"output_dim\"]==1:\n",
    "                                    prob=output['logits'].sigmoid().cpu().numpy()\n",
    "                                else:\n",
    "                                    prob=output['logits'].softmax(dim=-1).cpu().numpy()\n",
    "                                prob_.append(prob.T)\n",
    "                            prob=np.array(prob_) # (10, num_classes, num_players+1)\n",
    "                            classifiermasked_dict[metric_mode].append(prob)#(prob.argmax(axis=1)==label.item()).astype(float))\n",
    "                        \n",
    "                    elif len(explanation.shape)==1:\n",
    "                        for metric_mode in classifiermasked_dict.keys():\n",
    "                            mask=explanation_to_mask(explanation=get_relative_value(explanation)[np.newaxis,:], mode=metric_mode)\n",
    "                            classifier_masked_dict[backbone_type].eval()\n",
    "                            with torch.no_grad():\n",
    "                                output = classifier_masked_dict[backbone_type](image_loaded,\n",
    "                                                                                      masks=torch.Tensor(mask[0]).to(classifier_masked_dict[backbone_type].device))\n",
    "                            if _config[\"output_dim\"]==1:\n",
    "                                prob=output['logits'].sigmoid().cpu().numpy()\n",
    "                            else:\n",
    "                                prob=output['logits'].softmax(dim=-1).cpu().numpy()\n",
    "                            prob=prob.T # (num_classes, num_players+1)                            \n",
    "                            #classifiermasked_dict[metric_mode].append((prob.argmax(axis=0)==label.item()).astype(float)) \n",
    "                            classifiermasked_dict[metric_mode].append(prob) \n",
    "\n",
    "                    elif len(explanation.shape)==2:\n",
    "                        for metric_mode in classifiermasked_dict.keys():\n",
    "                            #mask=explanation_to_mask(explanation=explanation[label.item()], mode=metric_mode)\n",
    "                            if len(explanation)==1:\n",
    "                                mask=explanation_to_mask(explanation=get_relative_value(explanation[0])[np.newaxis,:], mode=metric_mode)\n",
    "                            else:\n",
    "                                mask=explanation_to_mask(explanation=get_relative_value(explanation[label.item()])[np.newaxis,:], mode=metric_mode)\n",
    "                            \n",
    "                            classifier_masked_dict[backbone_type].eval()\n",
    "                            with torch.no_grad():\n",
    "                                output = classifier_masked_dict[backbone_type](image_loaded,\n",
    "                                                                                      masks=torch.Tensor(mask[0]).to(classifier_masked_dict[backbone_type].device))\n",
    "                            if _config[\"output_dim\"]==1:\n",
    "                                prob=output['logits'].sigmoid().cpu().numpy()\n",
    "                            else:\n",
    "                                prob=output['logits'].softmax(dim=-1).cpu().numpy()\n",
    "                            prob=prob.T\n",
    "                            #classifiermasked_dict[metric_mode].append((prob.argmax(axis=0)==label.item()).astype(float))\n",
    "                            classifiermasked_dict[metric_mode].append(prob)\n",
    "                    else:\n",
    "                        raise\n",
    "                          \n",
    "                if explanation_method==\"random\":\n",
    "                    classifiermasked_save_dict_update(backbone_type, explanation_method,\n",
    "                                                  path_list=paths, \n",
    "                                                  insert_list=classifiermasked_dict[\"insertion\"], \n",
    "                                                  delete_list=classifiermasked_dict[\"deletion\"], \n",
    "                                                  shape=(10, _config[\"output_dim\"], num_players+1))                       \n",
    "                elif len(explanations[0].shape)==1:\n",
    "                    classifiermasked_save_dict_update(backbone_type, explanation_method,\n",
    "                                                  path_list=paths, \n",
    "                                                  insert_list=classifiermasked_dict[\"insertion\"], \n",
    "                                                  delete_list=classifiermasked_dict[\"deletion\"], \n",
    "                                                  shape=(_config[\"output_dim\"], num_players+1))                    \n",
    "                elif len(explanations[0].shape)==2:\n",
    "                    classifiermasked_save_dict_update(backbone_type, explanation_method,\n",
    "                                                  path_list=paths, \n",
    "                                                  insert_list=classifiermasked_dict[\"insertion\"], \n",
    "                                                  delete_list=classifiermasked_dict[\"deletion\"], \n",
    "                                                  shape=(_config[\"output_dim\"], num_players+1))\n",
    "                else:\n",
    "                    raise\n",
    "        \n",
    "        try:        \n",
    "            for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "                for explanation_method, classifiermasked_save_dict_backbone_method in classifiermasked_save_dict[backbone_type].items():\n",
    "\n",
    "                    if explanation_method not in updated_signal_list:\n",
    "                        continue                \n",
    "\n",
    "                    classifiermasked_save_dict_path=f'results/7_classifiermasked/{_config[\"datasets\"]}/{backbone_type}_{explanation_method}_{dataset_split}.pickle'\n",
    "\n",
    "                    if os.path.isfile(classifiermasked_save_dict_path):\n",
    "                        try:\n",
    "                            with open(classifiermasked_save_dict_path, 'rb') as f:\n",
    "                                classifiermasked_save_dict_loaded=pickle.load(f)\n",
    "                        except:\n",
    "                            classifiermasked_save_dict_loaded={}\n",
    "                    else:\n",
    "                        classifiermasked_save_dict_loaded={}\n",
    "\n",
    "                    len_original=len(classifiermasked_save_dict_backbone_method)            \n",
    "                    len_loaded=len(classifiermasked_save_dict_loaded)\n",
    "                    classifiermasked_save_dict_backbone_method.update(classifiermasked_save_dict_loaded)\n",
    "                    len_updated=len(classifiermasked_save_dict_backbone_method)\n",
    "\n",
    "                    print(f'{explanation_method:24}  {len_original:6}   + {len_loaded:6}   -> {len_updated:6}')\n",
    "\n",
    "\n",
    "                    with open(classifiermasked_save_dict_path, \"wb\") as f:\n",
    "                        pickle.dump(classifiermasked_save_dict_backbone_method, f)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "                            \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedcf947",
   "metadata": {},
   "source": [
    "# 8_elapsedtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5e1873",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_random_explanation(num_players, num_samples=None):\n",
    "    if num_samples is None:\n",
    "        return np.random.RandomState(42).uniform(low=0, high=1e-40, size=(num_players,))\n",
    "    else:\n",
    "        return np.random.RandomState(42).uniform(low=0, high=1e-40, size=(num_samples, num_players))\n",
    "\n",
    "\n",
    "if evaluation_stage==\"8_elapsedtime\":\n",
    "    for idx, batch in enumerate(tqdm(data_loader)):#, total = int(1000/data_loader.batch_size+0.5) if dataset_split==\"test\" else None)):\n",
    "        if dataset_split==\"test\":\n",
    "            if idx>int(1000/data_loader.batch_size+0.5):\n",
    "                break\n",
    "        if (idx%parallel_mode[1])!=parallel_mode[0]:\n",
    "            continue\n",
    "            \n",
    "        images=batch['images']\n",
    "        labels=batch['labels']\n",
    "        paths=batch['path']\n",
    "        updated_signal_list=[]\n",
    "        \n",
    "        for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "            # Get classifier output\n",
    "            classifier_dict[backbone_type].eval()\n",
    "            with torch.no_grad():\n",
    "                classifier_output=classifier_dict[backbone_type](images.to(classifier_dict[backbone_type].device),\n",
    "                                                                 output_attentions=True)        \n",
    "            for explanation_method in explanation_method_to_run:\n",
    "                data_keys=elapsedtime_save_dict[backbone_type][explanation_method].keys()\n",
    "                data_keys=[adapt_path(data_keys_path, paths) for data_keys_path in data_keys]\n",
    "                if all([path in data_keys for path in paths]):\n",
    "                    continue\n",
    "                else:\n",
    "                    print(explanation_method,'not exist')\n",
    "                    updated_signal_list.append(explanation_method)\n",
    "                    \n",
    "                if explanation_method==\"random\":\n",
    "                    start_time=time.time()\n",
    "                    explanation_random_list=[get_random_explanation(num_players=196) for path in paths]\n",
    "                    elapsed_time=time.time()-start_time\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'random', path_list=paths, elapsed_time_list=[elapsed_time/len(paths) for i in range(len(paths))])\n",
    "                \n",
    "                elif explanation_method==\"attention_rollout\":\n",
    "                    attentions = np.asarray([att.cpu().detach().numpy() for att in classifier_output['self_attentions']]).transpose(1,0,2,3,4)\n",
    "                    start_time=time.time()\n",
    "                    explanation_attention_rollout_list=attentions_to_explanation(attentions, mode='rollout')\n",
    "                    elapsed_time=time.time()-start_time\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'attention_rollout', path_list=paths, elapsed_time_list=[elapsed_time/len(paths) for i in range(len(paths))])\n",
    "\n",
    "                elif explanation_method==\"attention_last\":\n",
    "                    attentions = np.asarray([att.cpu().detach().numpy() for att in classifier_output['self_attentions']]).transpose(1,0,2,3,4)\n",
    "                    start_time=time.time()\n",
    "                    explanation_attention_last_list=attentions_to_explanation(attentions, mode=-1)\n",
    "                    elapsed_time=time.time()-start_time\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'attention_last', path_list=paths, elapsed_time_list=[elapsed_time/len(paths) for i in range(len(paths))])            \n",
    "\n",
    "                elif explanation_method==\"LRP\":\n",
    "                    explanation_lrp_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        explanation_lrp_list.append(np.concatenate([get_lrp_module_explanation(backbone_type=backbone_type,\n",
    "                                                                              original_image=image.squeeze(0),\n",
    "                                                                              class_index=i,\n",
    "                                                                              mode='transformer_attribution').cpu().numpy() for i in range(_config[\"output_dim\"])], axis=0))\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'LRP', path_list=paths, elapsed_time_list=elapsed_time_list)\n",
    "\n",
    "                elif explanation_method==\"gradcam\":\n",
    "                    explanation_gradcam_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        explanation_gradcam = np.concatenate([get_lrp_module_explanation(backbone_type=backbone_type,\n",
    "                                                                                              original_image=image.squeeze(0),\n",
    "                                                                                              class_index=i,\n",
    "                                                                                              mode='attn_gradcam').cpu().numpy() for i in range(_config[\"output_dim\"])], axis=0)\n",
    "                        explanation_gradcam = np.nan_to_num(explanation_gradcam,nan=0)+np.random.uniform(low=0, high=1e-20, size=explanation_gradcam.shape)                    \n",
    "                        explanation_gradcam_list.append(explanation_gradcam)\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'gradcam', path_list=paths, elapsed_time_list=elapsed_time_list)\n",
    "\n",
    "\n",
    "                elif explanation_method==\"gradcamgithub\":\n",
    "                    explanation_gradcamgithub_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        explanation_gradcamgithub = np.concatenate([cam_dict[backbone_type](input_tensor=image.unsqueeze(0).to(next(cam_dict[backbone_type].model.parameters()).device),\n",
    "                                                                                            targets=[ClassifierOutputTarget(i)], resize=False).flatten()[np.newaxis,:] for i in range(_config[\"output_dim\"])], axis=0)\n",
    "                        explanation_gradcamgithub_list.append(explanation_gradcamgithub)\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'gradcamgithub', path_list=paths, elapsed_time_list=elapsed_time_list)\n",
    "\n",
    "                elif explanation_method==\"vanillapixel\":\n",
    "                    explanation_vanillapixel_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        grad=get_vanilla(image, saliency_pixel=saliency_pixel_dict[backbone_type])\n",
    "                        explanation_vanillapixel_list.append(grad[\"attributions_pixel_patchabssum\"].reshape(_config[\"output_dim\"],196).numpy())\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'vanillapixel', path_list=paths, elapsed_time_list=elapsed_time_list)\n",
    "\n",
    "                elif explanation_method==\"vanillaembedding\":\n",
    "                    explanation_vanillaembedding_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:\n",
    "                        start_time=time.time()\n",
    "                        grad=get_vanilla(image, saliency_embedding=saliency_embedding_dict[backbone_type])\n",
    "                        explanation_vanillaembedding_list.append(grad[\"attributions_embedding_abssum\"].reshape(_config[\"output_dim\"],196).numpy())\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'vanillaembedding', path_list=paths, elapsed_time_list=elapsed_time_list)\n",
    "\n",
    "                elif explanation_method==\"sgpixel\":\n",
    "                    explanation_sgpixel_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        grad=get_sg(image, noisetunnel_pixel=noisetunnel_pixel_dict[backbone_type])\n",
    "                        explanation_sgpixel_list.append(grad[\"attributions_pixel_patchabssum\"].reshape(_config[\"output_dim\"],196).numpy())\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'sgpixel', path_list=paths, elapsed_time_list=elapsed_time_list)\n",
    "\n",
    "\n",
    "                elif explanation_method==\"sgembedding\":\n",
    "                    explanation_sgembedding_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        grad=get_sg(image, noisetunnel_embedding=noisetunnel_embedding_dict[backbone_type])\n",
    "                        explanation_sgembedding_list.append(grad[\"attributions_embedding_abssum\"].reshape(_config[\"output_dim\"],196).numpy())\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'sgembedding', path_list=paths, elapsed_time_list=elapsed_time_list)                                \n",
    "\n",
    "                elif explanation_method==\"vargradpixel\":\n",
    "                    explanation_vargradpixel_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        grad=get_vargrad(image, noisetunnel_pixel=noisetunnel_pixel_dict[backbone_type])\n",
    "                        explanation_vargradpixel_list.append(grad[\"attributions_pixel_patchabssum\"].reshape(_config[\"output_dim\"],196).numpy())\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'vargradpixel', path_list=paths, elapsed_time_list=elapsed_time_list)\n",
    "\n",
    "                elif explanation_method==\"vargradembedding\":\n",
    "                    explanation_vargradembedding_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        grad=get_vargrad(image, noisetunnel_embedding=noisetunnel_embedding_dict[backbone_type])\n",
    "                        explanation_vargradembedding_list.append(grad[\"attributions_embedding_abssum\"].reshape(_config[\"output_dim\"],196).numpy())\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'vargradembedding', path_list=paths, elapsed_time_list=elapsed_time_list)                                \n",
    "\n",
    "                elif explanation_method==\"igpixel\":\n",
    "                    explanation_igpixel_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        grad=get_ig(image, ig_pixel=ig_pixel_dict[backbone_type])\n",
    "                        explanation_igpixel_list.append(grad[\"attributions_pixel_patchsum\"].reshape(_config[\"output_dim\"],196).numpy())\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'igpixel', path_list=paths, elapsed_time_list=elapsed_time_list)\n",
    "\n",
    "                elif explanation_method==\"igembedding\":\n",
    "                    explanation_igembedding_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        grad=get_ig(image, ig_embedding=ig_embedding_dict[backbone_type])\n",
    "                        explanation_igembedding_list.append(grad[\"attributions_embedding_sum\"].reshape(_config[\"output_dim\"],196).numpy())\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'igembedding', path_list=paths, elapsed_time_list=elapsed_time_list)                \n",
    "\n",
    "                elif explanation_method==\"leaveoneoutclassifier\":\n",
    "                    explanation_leaveoneoutclassifier_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        explanation_leaveoneoutclassifier = leave_one_out(classifier=classifier_dict_[backbone_type], image=image).reshape(_config[\"output_dim\"], 196)\n",
    "                        explanation_leaveoneoutclassifier_list.append(explanation_leaveoneoutclassifier)\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'leaveoneoutclassifier', path_list=paths, elapsed_time_list=elapsed_time_list)\n",
    "\n",
    "                elif explanation_method==\"leaveoneoutsurrogate\":\n",
    "                    explanation_leaveoneoutsurrogate_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        explanation_leaveoneoutsurrogate = leave_one_out(surrogate=surrogate_dict[backbone_type][\"pre-softmax\"], image=image).reshape(_config[\"output_dim\"], 196)\n",
    "                        explanation_leaveoneoutsurrogate_list.append(explanation_leaveoneoutsurrogate)\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'leaveoneoutsurrogate', path_list=paths, elapsed_time_list=elapsed_time_list)                \n",
    "\n",
    "                elif explanation_method==\"riseclassifier\":\n",
    "                    explanation_riseclassifier_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        explanation_riseclassifier = rise(classifier=classifier_dict_[backbone_type], image=image, N=2000, include_prob=0.5).reshape(_config[\"output_dim\"], 196)\n",
    "                        explanation_riseclassifier_list.append(explanation_riseclassifier)\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'riseclassifier', path_list=paths, elapsed_time_list=elapsed_time_list)\n",
    "\n",
    "                elif explanation_method==\"risesurrogate\":\n",
    "                    explanation_risesurrogate_list=[]\n",
    "                    elapsed_time_list=[]\n",
    "                    for image in images:               \n",
    "                        start_time=time.time()\n",
    "                        explanation_risesurrogate = rise(surrogate=surrogate_dict[backbone_type][\"pre-softmax\"], image=image, N=2000, include_prob=0.5).reshape(_config[\"output_dim\"], 196)\n",
    "                        explanation_risesurrogate_list.append(explanation_risesurrogate)\n",
    "                        elapsed_time_list.append(time.time()-start_time)\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'risesurrogate', path_list=paths, elapsed_time_list=elapsed_time_list)                \n",
    "                    \n",
    "                elif explanation_method==\"ours\":\n",
    "                    start_time=time.time()\n",
    "                    explainer_dict[backbone_type].eval()\n",
    "                    with torch.no_grad():\n",
    "                        explanation_ours = explainer_dict[backbone_type](images.to(explainer_dict[backbone_type].device))[0].detach().cpu().numpy().transpose(0, 2, 1)\n",
    "                    elapsed_time=time.time()-start_time\n",
    "                    elapsedtime_save_dict_update(backbone_type, 'ours', path_list=paths, elapsed_time_list=[elapsed_time/len(paths) for i in range(len(paths))])                                \n",
    "                else:\n",
    "                    raise\n",
    "\n",
    "        try:\n",
    "            for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "                for explanation_method, elapsedtime_save_dict_backbone_method in elapsedtime_save_dict[backbone_type].items():\n",
    "                    if explanation_method not in updated_signal_list:\n",
    "                        continue\n",
    "                    elapsedtime_save_dict_path=f'results/8_elapsedtime/{_config[\"datasets\"]}/{backbone_type}_{explanation_method}_{dataset_split}.pickle'\n",
    "\n",
    "                    if os.path.isfile(elapsedtime_save_dict_path):\n",
    "                        try:\n",
    "                            with open(elapsedtime_save_dict_path, 'rb') as f:\n",
    "                                elapsedtime_save_dict_loaded=pickle.load(f)\n",
    "                        except:\n",
    "                            elapsedtime_save_dict_loaded={}\n",
    "                    else:\n",
    "                        elapsedtime_save_dict_loaded={}\n",
    "\n",
    "                    len_original=len(elapsedtime_save_dict_backbone_method)            \n",
    "                    len_loaded=len(elapsedtime_save_dict_loaded)\n",
    "                    elapsedtime_save_dict_backbone_method.update(elapsedtime_save_dict_loaded)\n",
    "                    len_updated=len(elapsedtime_save_dict_backbone_method)\n",
    "\n",
    "                    print(f'{explanation_method:24}  {len_original:6}   + {len_loaded:6}   -> {len_updated:6}')\n",
    "\n",
    "                    with open(elapsedtime_save_dict_path, \"wb\") as f:\n",
    "                        pickle.dump(elapsedtime_save_dict_backbone_method, f)\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "            \n",
    "            \n",
    "    #         # Get ours\n",
    "    #         start_time=time.time();explainer_dict[backbone_type].eval()\n",
    "    #         with torch.no_grad():\n",
    "    #             values, _, _=explainer_dict[backbone_type](torch.Tensor(image).unsqueeze(0).to(explainer_dict[backbone_type].device))\n",
    "    #             values=values.cpu().numpy().transpose(0,2,1).squeeze(0)\n",
    "    #         values_ours=(values, time.time()-start_time)                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fe2ef4",
   "metadata": {},
   "source": [
    "# 9_estimationerror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3afb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimationerror_sample_path_list=pd.DataFrame(data_loader.dataset.data).groupby(\"label\").apply(lambda x: x.sample(n=10, random_state=42))[\"img_path\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9d02b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if evaluation_stage==\"9_estimationerror\":\n",
    "    for idx, batch in enumerate(tqdm(data_loader)):#, total = int(1000/data_loader.batch_size+0.5) if dataset_split==\"test\" else None)):\n",
    "#         if dataset_split==\"test\":\n",
    "#             if idx>int(1000/data_loader.batch_size+0.5):\n",
    "#                 break\n",
    "        if (idx%parallel_mode[1])!=parallel_mode[0]:\n",
    "            continue\n",
    "            \n",
    "        images=batch['images']\n",
    "        labels=batch['labels']\n",
    "        paths=batch['path']\n",
    "        updated_signal_list=[]\n",
    "        \n",
    "        for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "            # Get classifier output\n",
    "            classifier_dict[backbone_type].eval()\n",
    "            with torch.no_grad():\n",
    "                classifier_output=classifier_dict[backbone_type](images.to(classifier_dict[backbone_type].device),\n",
    "                                                                 output_attentions=True)        \n",
    "            for explanation_method in [\"kernelshapnopair\"]:\n",
    "                data_keys=estimationerror_save_dict[backbone_type][explanation_method].keys()\n",
    "                data_keys=[adapt_path(data_keys_path, paths) for data_keys_path in data_keys]\n",
    "                if all([(path in data_keys) or (path not in estimationerror_sample_path_list) for path in paths]):\n",
    "                    continue\n",
    "                else:\n",
    "                    print(explanation_method,'not exist')\n",
    "                    updated_signal_list.append(explanation_method)\n",
    "                    \n",
    "                if explanation_method==\"ours\":\n",
    "                    start_time=time.time()\n",
    "                    explainer_dict[backbone_type].eval()\n",
    "                    with torch.no_grad():\n",
    "                        explanation_ours = explainer_dict[backbone_type](images.to(explainer_dict[backbone_type].device))[0].detach().cpu().numpy().transpose(0, 2, 1)\n",
    "                    elapsed_time=time.time()-start_time\n",
    "                    estimationerror_save_dict_update(backbone_type, \n",
    "                                                     'ours', \n",
    "                                                     path_list=[path for path, paths in zip(paths, paths) if path in estimationerror_sample_path_list],\n",
    "                                                     estimation_list=[estimation for path, estimation in zip(paths, explanation_ours) if path in estimationerror_sample_path_list],\n",
    "                                                     label_list=[label for path, label in zip(paths, labels.cpu().numpy().tolist()) if path in estimationerror_sample_path_list],\n",
    "                                                     shape=(_config[\"output_dim\"], 196))\n",
    "                    \n",
    "                elif explanation_method==\"kernelshap\":\n",
    "                    path_list=[]\n",
    "                    explanation_kernelshap_list=[]\n",
    "                    label_list=[]\n",
    "                    \n",
    "                    surrogate_SHAP_wrapped_dict[backbone_type].eval()\n",
    "                    for path, image, label in zip(paths, images, labels.cpu().numpy().tolist()):\n",
    "                        if path in estimationerror_sample_path_list:\n",
    "                            start_time=time.time()      \n",
    "                            game = games.PredictionGame_torchimagetensor(surrogate_SHAP_wrapped_dict[backbone_type],\n",
    "                                                                         image)\n",
    "                            \n",
    "                            explanation_kernelshap = shapley.ShapleyRegression(game, \n",
    "                                                                    batch_size=64, \n",
    "                                                                    thresh=0.1,\n",
    "                                                                    variance_batches=60,\n",
    "                                                                    return_all=True)                              \n",
    "                            \n",
    "                            path_list.append(path)\n",
    "                            explanation_kernelshap_list.append(explanation_kernelshap)\n",
    "                            label_list.append(label)\n",
    "                            \n",
    "                    estimationerror_save_dict_update(backbone_type, \n",
    "                                                     'kernelshap', \n",
    "                                                     path_list=path_list, \n",
    "                                                     estimation_list=explanation_kernelshap_list, \n",
    "                                                     label_list=label_list,\n",
    "                                                     shape=None)  \n",
    "                    \n",
    "                elif explanation_method==\"kernelshapnopair\":\n",
    "                    path_list=[]\n",
    "                    explanation_kernelshap_list=[]\n",
    "                    label_list=[]\n",
    "                    \n",
    "                    surrogate_SHAP_wrapped_dict[backbone_type].eval()\n",
    "                    for path, image, label in zip(paths, images, labels.cpu().numpy().tolist()):\n",
    "                        if path in estimationerror_sample_path_list:\n",
    "                            start_time=time.time()      \n",
    "                            game = games.PredictionGame_torchimagetensor(surrogate_SHAP_wrapped_dict[backbone_type],\n",
    "                                                                         image)\n",
    "                            \n",
    "                            explanation_kernelshap = shapley.ShapleyRegression(game, \n",
    "                                                                    batch_size=128, \n",
    "                                                                    detect_convergence=False,\n",
    "                                                                    paired_sampling=False,\n",
    "                                                                    n_samples=200000,\n",
    "                                                                    variance_batches=60,\n",
    "                                                                    return_all=True)                              \n",
    "                            \n",
    "                            path_list.append(path)\n",
    "                            explanation_kernelshap_list.append(explanation_kernelshap)\n",
    "                            label_list.append(label)\n",
    "                            \n",
    "                    estimationerror_save_dict_update(backbone_type, \n",
    "                                                     'kernelshapnopair', \n",
    "                                                     path_list=path_list, \n",
    "                                                     estimation_list=explanation_kernelshap_list, \n",
    "                                                     label_list=label_list,\n",
    "                                                     shape=None)  \n",
    "                    \n",
    "                else:\n",
    "                    raise\n",
    "        \n",
    "        try:\n",
    "            for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "                for explanation_method, estimationerror_save_dict_backbone_method in estimationerror_save_dict[backbone_type].items():\n",
    "\n",
    "                    if explanation_method not in updated_signal_list:\n",
    "                        continue\n",
    "\n",
    "                    estimationerror_save_dict_path=f'results/9_estimationerror/{_config[\"datasets\"]}/{backbone_type}_{explanation_method}_{dataset_split}.pickle'\n",
    "\n",
    "                    if os.path.isfile(estimationerror_save_dict_path):\n",
    "                        try:\n",
    "                            with open(estimationerror_save_dict_path, 'rb') as f:\n",
    "                                estimationerror_save_dict_loaded=pickle.load(f)\n",
    "                        except:\n",
    "                            estimationerror_save_dict_loaded={}\n",
    "                    else:\n",
    "                        estimationerror_save_dict_loaded={}\n",
    "\n",
    "                    len_original=len(estimationerror_save_dict_backbone_method)            \n",
    "                    len_loaded=len(estimationerror_save_dict_loaded)\n",
    "                    estimationerror_save_dict_backbone_method.update(estimationerror_save_dict_loaded)\n",
    "                    len_updated=len(estimationerror_save_dict_backbone_method)\n",
    "\n",
    "                    print(f'{explanation_method:24}  {len_original:6}   + {len_loaded:6}   -> {len_updated:6}')\n",
    "                    \n",
    "                    with open(estimationerror_save_dict_path, \"wb\") as f:\n",
    "                        pickle.dump(estimationerror_save_dict_backbone_method, f)\n",
    "        except:\n",
    "            pass                    \n",
    "            \n",
    "    #         # Get ours\n",
    "    #         start_time=time.time();explainer_dict[backbone_type].eval()\n",
    "    #         with torch.no_grad():\n",
    "    #             values, _, _=explainer_dict[backbone_type](torch.Tensor(image).unsqueeze(0).to(explainer_dict[backbone_type].device))\n",
    "    #             values=values.cpu().numpy().transpose(0,2,1).squeeze(0)\n",
    "    #         values_ours=(values, time.time()-start_time)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c14574",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    for path, data in estimationerror_save_dict[backbone_type][\"kernelshap\"].items():\n",
    "        explanation_save_dict[backbone_type]['kernelshap'][path]={\"explanation\": data['estimation'][0].values.T,\n",
    "                                                                  \"elapsed_time\": np.nan}\n",
    "        print(path, data['estimation'][0].values.T.shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f1edf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    for explanation_method, estimationerror_save_dict_backbone_method in estimationerror_save_dict[backbone_type].items():\n",
    "        print(backbone_type, explanation_method, len(estimationerror_save_dict_backbone_method))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1639263",
   "metadata": {},
   "outputs": [],
   "source": [
    "[elapsed_time/len(paths) for i in range(len(paths))][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4297a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsedtime_save_dict_update??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da58468",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_masked_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529f6928",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86f61e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c839705",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.session.delete();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51e3403",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4b2d9d0",
   "metadata": {},
   "source": [
    "# sensitivity-n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d6d44f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_players=196\n",
    "for idx, batch in enumerate(tqdm(data_loader, total = int(1000/data_loader.batch_size+0.5) if dataset_split==\"test\" else None)):   \n",
    "    if dataset_split==\"test\":\n",
    "        if idx>int(1000/data_loader.batch_size+0.5):\n",
    "            break\n",
    "    if (idx%parallel_mode[1])!=parallel_mode[0]:\n",
    "        continue                                \n",
    "\n",
    "    images=batch['images']\n",
    "    labels=batch['labels']\n",
    "    paths=batch['path']\n",
    "\n",
    "    for _, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):            \n",
    "        for image, path, label in zip(images, paths, labels):\n",
    "            #path=path.replace('l0.cs.washington.edu','l2lambda.cs.washington.edu')\n",
    "            for num_included_players in [\"all\"]:\n",
    "                print(num_included_players)                \n",
    "                if num_included_players==\"all\":\n",
    "                    prob_all=[]\n",
    "                    mask_all=[]\n",
    "                    for random_iter in range(20):\n",
    "                        image_loaded=image[np.newaxis, :].repeat(50, 1, 1, 1).to(surrogate_dict[backbone_type][\"pre-softmax\"].device)\n",
    "                        mask=generate_mask(num_players=num_players,\n",
    "                                           num_mask_samples=50,\n",
    "                                           paired_mask_samples=False,\n",
    "                                           mode=\"uniform\",\n",
    "                                           random_state=np.random.RandomState(random_iter))\n",
    "\n",
    "                        surrogate_dict[backbone_type][\"pre-softmax\"].eval()\n",
    "                        with torch.no_grad():\n",
    "                            output = surrogate_dict[backbone_type][\"pre-softmax\"](image_loaded,\n",
    "                                                                                  masks=torch.Tensor(mask).to(surrogate_dict[backbone_type][\"pre-softmax\"].device))\n",
    "\n",
    "                        if _config[\"output_dim\"]==1:\n",
    "                            prob=output['logits'].sigmoid().cpu().numpy()\n",
    "                        else:\n",
    "                            prob=output['logits'].softmax(dim=-1).cpu().numpy()   \n",
    "                        prob_all.append(prob)\n",
    "                        mask_all.append(mask)\n",
    "                    prob_all=np.concatenate(prob_all, axis=0)\n",
    "                    mask_all=np.concatenate(mask_all, axis=0)\n",
    "\n",
    "                for explanation_method in explanation_method_to_run:                \n",
    "                    print(explanation_method)\n",
    "                    explanation=explanation_save_dict[backbone_type][explanation_method][adapt_path(path, list(explanation_save_dict[backbone_type][explanation_method].keys())[0])]['explanation']\n",
    "                    explanation=explanation+np.random.RandomState(42).uniform(low=0, high=1e-40, size=explanation.shape)\n",
    "                    explanation_mask=explanation@(mask_all.T)\n",
    "\n",
    "                    if len(explanation.shape)==1:\n",
    "                        correlation = np.array([stats.spearmanr(explanation_mask, prob_all[:, i]).correlation for i in range(prob_all.shape[1])])\n",
    "                        assert correlation.shape==(_config[\"output_dim\"],)\n",
    "\n",
    "                    elif len(explanation.shape)==2:\n",
    "                        #correlation=stats.spearmanr(np.concatenate([explanation_mask, prob_all.T], axis=0), axis=1).correlation\n",
    "                        correlation = np.array([stats.spearmanr(explanation_mask[i], prob_all[:, i]).correlation for i in range(prob_all.shape[1])])\n",
    "                        assert correlation.shape==(_config[\"output_dim\"],)\n",
    "                        \n",
    "                        fig=plt.figure(figsize=(20,5))\n",
    "                        ax=fig.add_subplot(121)\n",
    "                        ax.scatter(explanation_mask[label], prob_all[:,label])#\n",
    "                        ax.set_xlabel(\"sum_explanation\")\n",
    "                        ax.set_ylabel(\"model output\")\n",
    "                        ax.set_title(explanation_method)\n",
    "                        \n",
    "                        ax=fig.add_subplot(122)\n",
    "                        for i in range(prob_all.shape[1]):\n",
    "                            if i>3:\n",
    "                                break\n",
    "                            if i!=label:\n",
    "                                ax.scatter(explanation_mask[i], prob_all[:,i])#, s=4)#\n",
    "                        ax.set_xlabel(\"sum_explanation\")\n",
    "                        ax.set_ylabel(\"model output\")\n",
    "                        ax.set_title(explanation_method)                        \n",
    "                        \n",
    "                        plt.show()\n",
    "                        \n",
    "                    else:\n",
    "                        raise\n",
    "#                     sensitivity_save_dit_update(backbone_type, explanation_method,\n",
    "#                                                  num_included_players=num_included_players,\n",
    "#                                                  path_list=[path], sensitivity_list=[correlation],\n",
    "#                                                  shape=(_config[\"output_dim\"],))     \n",
    "            break\n",
    "\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b35df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure()\n",
    "ax=fig.add_subplot()\n",
    "ax.scatter(explanation_mask[label], prob_all[:,label])#\n",
    "ax.set_xlabel(\"sum_explanation\")\n",
    "ax.set_ylabel(\"model output\")\n",
    "ax.set_title(explanation_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcd97c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb3e1c7e",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a068a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "fig = plt.figure()\n",
    "ax_temp=fig.add_subplot()\n",
    "plt.clf()\n",
    "\n",
    "def visualize_result(x, values, pred=None, vmin_vmax='separate',\n",
    "                     image_labels=['normal','abnormal']*5,\n",
    "                     class_labels=['normal','abnormal']*5):\n",
    "    # colormap\n",
    "    from matplotlib import cm\n",
    "    from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "\n",
    "    color_num = 1000\n",
    "    img_mean = np.array([0.4914, 0.4822, 0.4465])[:, np.newaxis, np.newaxis]\n",
    "    img_std = np.array([0.2023, 0.1994, 0.2010])[:, np.newaxis, np.newaxis]    \n",
    "\n",
    "    if isinstance(vmin_vmax,tuple):\n",
    "        vmin, vmax= vmin_vmax\n",
    "        assert vmin < vmax\n",
    "        if vmin * vmax < 0:\n",
    "            ratio=vmax/(-vmin+vmax)\n",
    "            seismic = cm.get_cmap('seismic', color_num)\n",
    "            newcolors = seismic(np.linspace(0, 1, color_num))\n",
    "            newcolors[:int(color_num*(1-ratio))] = seismic(np.linspace(0, 0.5, int(color_num*(1-ratio))))\n",
    "            newcolors[-int(color_num*ratio)-1:] = seismic(np.linspace(0.5, 1, int(color_num*ratio)+1))\n",
    "            newcmp = ListedColormap(newcolors)\n",
    "        elif vmin > 0:\n",
    "            seismic = cm.get_cmap('seismic', color_num)\n",
    "            newcolors = seismic(np.linspace(0.5, 1, color_num))\n",
    "            newcmp = ListedColormap(newcolors)\n",
    "        else:\n",
    "            raise\n",
    "    elif vmin_vmax==\"separate\":\n",
    "        #seismic = cm.get_cmap('seismic', color_num)\n",
    "        #newcolors = seismic(np.linspace(0.5, 1, color_num))\n",
    "        #newcmp = ListedColormap(newcolors)        \n",
    "        if values.min()>0:\n",
    "            seismic = cm.get_cmap('seismic', color_num)\n",
    "            newcolors = seismic(np.linspace(0.5, 1, color_num))\n",
    "            newcmp = ListedColormap(newcolors)            \n",
    "        else:\n",
    "            seismic = cm.get_cmap('seismic', color_num)\n",
    "            newcolors = seismic(np.linspace(0, 1, color_num))\n",
    "            newcmp = ListedColormap(newcolors)\n",
    "\n",
    "\n",
    "    fig, axes = plt.subplots(values.shape[0], 1+values.shape[1], figsize=(2*(1+values.shape[1]), 2*(values.shape[0]+1)))\n",
    "\n",
    "    assert len(image_labels)==values.shape[0]==(len(image_labels) if pred is None else pred.shape[0])\n",
    "    assert len(class_labels)==values.shape[1]==(len(class_labels) if pred is None else pred.shape[1])\n",
    "    \n",
    "    for row in range(axes.shape[0]):\n",
    "        for col in range(axes.shape[1]):\n",
    "            if col==0: # Image\n",
    "                im = x[row].numpy() * img_std + img_mean # (C, H, W)\n",
    "                im = im.transpose(1, 2, 0).astype(float) # (H, W, C)\n",
    "                im = np.clip(im, a_min=0, a_max=1)\n",
    "\n",
    "                axes[row, 0].imshow(im, vmin=0, vmax=1)\n",
    "                axes[row, 0].set_ylabel('{}'.format(image_labels[row]), fontsize=12)\n",
    "            else: # Explanation\n",
    "                values_select=values[row, col-1]\n",
    "                values_select_min, values_select_max=values_select.min(),values_select.max()\n",
    "\n",
    "                if vmin_vmax==\"separate\":\n",
    "                    if values.min()>0:\n",
    "                        axes[row, col].imshow(values_select, cmap=newcmp,\n",
    "                                              vmin=values_select_min,\n",
    "                                              vmax=values_select_max)\n",
    "                    else:\n",
    "                        axes[row, col].imshow(values_select, cmap=newcmp, \n",
    "                                              vmin=-max([abs(values_select_min),abs(values_select_max)]), \n",
    "                                              vmax=max([abs(values_select_min),abs(values_select_max)]))\n",
    "                else:\n",
    "                    axes[row, col].imshow(values_select, cmap=newcmp, vmin=vmin, vmax=vmax)\n",
    "\n",
    "                if pred is None:\n",
    "                    axes[row, col].set_xlabel('{:.2f}/{:.2f}'.format(values_select_min, values_select_max), fontsize=12)\n",
    "                else:\n",
    "                    axes[row, col].set_xlabel('{:.2f} {:.2f}/{:.2f}'.format(pred[row, col-1], values_select_min, values_select_max), fontsize=12)            \n",
    "\n",
    "                # Class labels\n",
    "                if row == 0:\n",
    "                    axes[row, col].set_title('{}'.format(class_labels[col-1]), fontsize=12)  \n",
    "\n",
    "            axes[row, col].set_xticks([])\n",
    "            axes[row, col].set_yticks([])                           \n",
    "\n",
    "    if vmin_vmax!=\"separate\":\n",
    "        fig = plt.figure(figsize=(5, 0.5))\n",
    "        ax=fig.add_subplot()        \n",
    "        \n",
    "        sns.heatmap([[0,0],[0,0]],\n",
    "                    ax=ax_temp,\n",
    "                    cmap=newcmp,\n",
    "                    vmin=vmin,\n",
    "                    vmax=vmax,\n",
    "                    xticklabels=True,\n",
    "                    linewidths=1,\n",
    "                    linecolor=np.array([220,220,220,256])/256,\n",
    "                    cbar_ax=ax,\n",
    "                    cbar_kws={'fraction':0.1, \"ticks\":np.linspace(vmin, vmax, 5), \"orientation\": \"horizontal\"},\n",
    "                    cbar=True,\n",
    "                    alpha=1,edgecolor='black')#,legend=None)\n",
    "\n",
    "        #plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        fig=plt.figure()\n",
    "        ax=fig.add_subplot()\n",
    "        ax.hist(values.flatten(),bins=np.linspace(vmin, vmax, 20))\n",
    "        ax.set_yscale('log')\n",
    "        print(values.flatten().min(), values.flatten().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b53e07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prob_all_all=[]\n",
    "mask_all_all=[]\n",
    "for idx, image in enumerate(x):\n",
    "    print(idx)\n",
    "    prob_all=[]\n",
    "    mask_all=[]\n",
    "    for random_iter in range(20):\n",
    "        image_loaded=image[np.newaxis, :].repeat(50, 1, 1, 1).to(surrogate_dict[backbone_type][\"pre-softmax\"].device)\n",
    "        mask=generate_mask(num_players=num_players,\n",
    "                           num_mask_samples=50,\n",
    "                           paired_mask_samples=False,\n",
    "                           mode=\"uniform\",\n",
    "                           random_state=np.random.RandomState(random_iter))\n",
    "\n",
    "        surrogate_dict[backbone_type][\"pre-softmax\"].eval()\n",
    "        with torch.no_grad():\n",
    "            output = surrogate_dict[backbone_type][\"pre-softmax\"](image_loaded,\n",
    "                                                                  masks=torch.Tensor(mask).to(surrogate_dict[backbone_type][\"pre-softmax\"].device))\n",
    "\n",
    "        if _config[\"output_dim\"]==1:\n",
    "            prob=output['logits'].sigmoid().cpu().numpy()\n",
    "        else:\n",
    "            prob=output['logits'].softmax(dim=-1).cpu().numpy()   \n",
    "        prob_all.append(prob)\n",
    "        mask_all.append(mask)\n",
    "    prob_all=np.concatenate(prob_all, axis=0)\n",
    "    mask_all=np.concatenate(mask_all, axis=0)\n",
    "    #print(prob_all.shape, mask_all.shape)\n",
    "    prob_all_all.append(prob_all)\n",
    "    mask_all_all.append(mask_all)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0270ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "values.reshape(values.shape[0], values.shape[1], 196)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd15b98b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for idx, (backbone_type, backbone_type_config) in enumerate(backbone_type_config_dict.items()):\n",
    "    #get classifier\n",
    "    classifier_dict[backbone_type].eval()\n",
    "    with torch.no_grad():\n",
    "        output = classifier_dict[backbone_type](x.to(next(classifier_dict[backbone_type].parameters()).device), output_attentions=False)\n",
    "        if _config[\"output_dim\"]==1:\n",
    "            pred=output['logits'].detach().sigmoid().cpu().data.numpy()\n",
    "        else:\n",
    "            pred=output['logits'].detach().softmax().cpu().data.numpy()\n",
    "    del output    \n",
    "    \n",
    "    # Get explanation (modified_LRP)\n",
    "    values=np.concatenate([np.concatenate([get_lrp_module_explanation(backbone_type, image.squeeze(0), class_index=i, mode='transformer_attribution').cpu() for i in range(_config[\"output_dim\"])], axis=0)[np.newaxis,:] for image in x])\n",
    "    values=values.reshape(values.shape[0], values.shape[1], 14, 14)\n",
    "    \n",
    "    visualize_result(x, pred=pred, values=values,\n",
    "                     class_labels=y_labels[1:2] if _config[\"output_dim\"]==1 else y_labels,\n",
    "                     image_labels=y_labels,\n",
    "                     vmin_vmax=\"separate\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    values_lrp=values.reshape(values.shape[0], values.shape[1], 196)\n",
    "    \n",
    "    \n",
    "    explainer_dict[backbone_type].eval()\n",
    "    with torch.no_grad():\n",
    "        values=explainer_dict[backbone_type](x.to(next(explainer_dict[backbone_type].parameters()).device))    \n",
    "        values=values[0].reshape(-1, _config[\"output_dim\"], 14, 14).cpu().numpy()\n",
    "        \n",
    "    visualize_result(x, pred=pred, values=values,\n",
    "                     class_labels=y_labels[1:2] if _config[\"output_dim\"]==1 else y_labels,\n",
    "                     image_labels=y_labels,\n",
    "                     vmin_vmax=(-0.2, 0.2))    \n",
    "    \n",
    "    values_ours=values.reshape(values.shape[0], values.shape[1], 196)\n",
    "    \n",
    "    #visualize_result(x, pred=pred.repeat(10, axis=1), values=values.repeat(10, axis=1), separate=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vitshapley",
   "language": "python",
   "name": "vitshapley"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
